{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir('..')\n",
    "from inference.utils import *\n",
    "from core.utils import load_or_fail\n",
    "from train import WurstCoreB\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2854e1fc-0851-47f8-9ab0-6f61c8cc66d9",
   "metadata": {},
   "source": [
    "### Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5168bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP STAGE B & A\n",
    "config_file_b = 'configs/inference/stage_b_3b.yaml'\n",
    "with open(config_file_b, \"r\", encoding=\"utf-8\") as file:\n",
    "    config_file_b = yaml.safe_load(file)\n",
    "    \n",
    "core = WurstCoreB(config_dict=config_file_b, device=device, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7054ba5c-98f3-414c-8fcb-4cc4b81a635f",
   "metadata": {},
   "source": [
    "### Load Extras, Data & Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6888a-fff2-4ddd-8f6c-3bd98c0fdebb",
   "metadata": {},
   "source": [
    "Download a sample dataset of just a few ImageNet images to use for image reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab379476-b6fd-4d1c-853d-373bd1bcaeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/dome272/stable-cascade/resolve/main/imagenet_1024.tar -P inference -q --show-progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac0a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "extras = core.setup_extras_pre()\n",
    "data = core.setup_data(extras)\n",
    "models = core.setup_models(extras)\n",
    "models.generator.bfloat16()\n",
    "print(\"STAGE B READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef02091-891a-45de-b7c3-b08502356268",
   "metadata": {},
   "source": [
    "### Preview Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(data.iterator)\n",
    "print(\"ORIG SIZE:\", batch['images'].shape)\n",
    "\n",
    "show_images(batch['images'])\n",
    "\n",
    "print(batch['captions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6b9fc8-51d9-4ab4-81a0-1a03ff0ccde0",
   "metadata": {},
   "source": [
    "## Step-by-Step Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c7dc8-0c3c-45b1-b626-62cb5b76533c",
   "metadata": {},
   "source": [
    "A quick reminder, StableCascade uses three Stages (A, B & C). Stage C is responsible for the text-conditional generation in the highly compressed latent space. On the other hand, Stage A & B are used for achieving this high compression, thus encoding and decoding images to the latent space. Specific details can be found in the [paper](https://openreview.net/forum?id=gU58d5QeGv), but you only need to know that Stage A is just a VAE, providing a small compression factor of 4. (`4 x 3 x 1024 x 1024 -> 4 x 4 x 256 x 256`). Then Stage B is learnt on top of that compressed latent space, to compress images even further. This cascading would not be possible when just using another VAE. Therefore, a more powerful approach is needed: a diffusion model.\n",
    "\n",
    "Stage B iteratively reconstructs images into the latent space of the VAE (Stage A), from where it can be decoded into the pixel-space.\n",
    "\n",
    "Let's set the sampling parameters for Stage B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a668ed4f-5c9a-4ace-bcc8-dc7cb0e5084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extras.sampling_configs['cfg'] = 1.1\n",
    "extras.sampling_configs['shift'] = 1\n",
    "extras.sampling_configs['timesteps'] = 10\n",
    "extras.sampling_configs['t_start'] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa668e3-a9c3-4ac2-97d8-4f815cd0eea4",
   "metadata": {},
   "source": [
    "Next we encode the images. By default, the encoder (an EfficientNet architecture) yields feature representations with a compression factor of 32. This would mean that we encode our images like this: `4 x 3 x 1024 x 1024 -> 4 x 16 x 32 x 32`. To increase this even further, during training we additionally downscale images before encoding them. There might be other ways of achieving encoding images even further (e.g. adding more layers), but this works as well. \n",
    "\n",
    "During training we use a downscaling factor randomly between 1.0 and 0.5, yielding compression factors between 32 and 64. That means Stage B, the diffusion model, can compress images of size `4 x 3 x 1024 x 1024` to latent dimensions between `4 x 16 x 32 x 32` and `4 x 16 x 16 x 16`. So up to a spatial compression factor of 64! Of course, the more you compress, the more details you lose in the reconstructions. We found that a downscaling factor of 0.75 works very well and preserves a lot of details, resulting in a spatial compression factor of `42`.\n",
    "\n",
    "In the code below, you can test different downscaling factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75eeb89-5e7c-40d0-bfd6-8c53a122fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Size:\", batch['images'].shape)\n",
    "factor = 3/4\n",
    "scaled_image = downscale_images(batch['images'], factor)\n",
    "print(\"[Optional] Downscaled Size:\", scaled_image.shape)\n",
    "\n",
    "effnet_latents = models.effnet(extras.effnet_preprocess(scaled_image.to(device)))\n",
    "print(\"Encoded Size:\", effnet_latents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fd3ebd-a76e-4899-9c07-9598ab142dfe",
   "metadata": {},
   "source": [
    "Now, we set the conditions for the diffusion model (Stage B). We condition the model on text as well, however the effect of it is tiny, especially when the `effnet_latents` are given as well, because they are just so powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc99f4-2a8e-4dae-9275-6339f1e27ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = core.get_conditions(batch, models, extras, is_eval=True, is_unconditional=False)\n",
    "unconditions = core.get_conditions(batch, models, extras, is_eval=True, is_unconditional=True)    \n",
    "conditions['effnet'] = effnet_latents\n",
    "unconditions['effnet'] = torch.zeros_like(effnet_latents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d7eee-d38f-47ab-afbb-ed2e4c184801",
   "metadata": {},
   "source": [
    "Here comes the sampling loop for reconstructing the images them into the latent space of Stage A and afterwards using Stage A to decode those latents into the pixel-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ca921-a288-468e-8cae-b83074b897a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "    sampling_b = extras.gdf.sample(\n",
    "        models.generator, conditions, (batch['images'].size(0), 4, batch['images'].size(-2)//4, batch['images'].size(-1)//4),\n",
    "        unconditions, device=device, **extras.sampling_configs\n",
    "    )\n",
    "    for (sampled_b, _, _) in tqdm(sampling_b, total=extras.sampling_configs['timesteps']):\n",
    "        sampled_b = sampled_b\n",
    "    sampled = models.stage_a.decode(sampled_b).float()\n",
    "    print(\"Decoded Size:\", sampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8d6127-1bb2-4460-bceb-87025704e0d8",
   "metadata": {},
   "source": [
    "Let's visualise both the original images and the reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db775485-946c-474b-9f52-314ff0dad884",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(batch['images'])\n",
    "show_images(sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d4c86-79bd-4b32-8410-f6a9285c5fe2",
   "metadata": {},
   "source": [
    "## Full Reconstruction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857f0fd-9e33-4d3f-95a3-2f8549a515f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage B Parameters\n",
    "extras.sampling_configs['cfg'] = 1.1\n",
    "extras.sampling_configs['shift'] = 1\n",
    "extras.sampling_configs['timesteps'] = 10\n",
    "extras.sampling_configs['t_start'] = 1.0\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "    # torch.manual_seed(42)\n",
    "\n",
    "    print(\"Original Size:\", batch['images'].shape)\n",
    "    factor = 3/4\n",
    "    scaled_image = downscale_images(batch['images'], factor)\n",
    "    print(\"[Optional] Downscaled Size:\", scaled_image.shape)\n",
    "    \n",
    "    effnet_latents = models.effnet(extras.effnet_preprocess(scaled_image.to(device)))\n",
    "    print(\"Encoded Size:\", effnet_latents.shape)\n",
    "    \n",
    "    conditions = core.get_conditions(batch, models, extras, is_eval=True, is_unconditional=False)\n",
    "    unconditions = core.get_conditions(batch, models, extras, is_eval=True, is_unconditional=True)    \n",
    "    conditions['effnet'] = effnet_latents\n",
    "    unconditions['effnet'] = torch.zeros_like(effnet_latents)\n",
    "\n",
    "    sampling_b = extras.gdf.sample(\n",
    "        models.generator, conditions, (batch['images'].size(0), 4, batch['images'].size(-2)//4, batch['images'].size(-1)//4),\n",
    "        unconditions, device=device, **extras.sampling_configs\n",
    "    )\n",
    "    for (sampled_b, _, _) in tqdm(sampling_b, total=extras.sampling_configs['timesteps']):\n",
    "        sampled_b = sampled_b\n",
    "    sampled = models.stage_a.decode(sampled_b).float()\n",
    "    print(\"Decoded Size:\", sampled.shape)\n",
    "        \n",
    "show_images(batch['images'])\n",
    "show_images(sampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
