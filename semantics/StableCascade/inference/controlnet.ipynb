{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce520e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir('..')\n",
    "from inference.utils import *\n",
    "from core.utils import load_or_fail\n",
    "from train import ControlNetCore, WurstCoreB\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ef2fab",
   "metadata": {},
   "source": [
    "### Choose your ControlNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dfbb3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Inpainting / Outpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32461c3f-350b-4504-a0a9-589b62223e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/stabilityai/stable-cascade/resolve/main/controlnet/inpainting.safetensors -P models -q --show-progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f77ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'configs/inference/controlnet_c_3b_inpainting.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d215547",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Face Identity (Not available yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257948df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_file = 'configs/inference/controlnet_c_3b_identity.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23450130",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636eecb7-295c-4cf1-9638-c619ce5be082",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/stabilityai/stable-cascade/resolve/main/controlnet/canny.safetensors -P models -q --show-progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bde3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'configs/inference/controlnet_c_3b_canny.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340cd59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Super Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc89dfa-c1c1-41b2-ae18-9d6a21ef2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/stabilityai/stable-cascade/resolve/main/controlnet/super_resolution.safetensors -P models -q --show-progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'configs/inference/controlnet_c_3b_sr.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f4f2c",
   "metadata": {},
   "source": [
    "## Load Config & Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ebc56",
   "metadata": {},
   "source": [
    "### Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP STAGE C\n",
    "with open(config_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    loaded_config = yaml.safe_load(file)\n",
    "\n",
    "core = ControlNetCore(config_dict=loaded_config, device=device, training=False)\n",
    "\n",
    "# SETUP STAGE B\n",
    "config_file_b = 'configs/inference/stage_b_3b.yaml'\n",
    "with open(config_file_b, \"r\", encoding=\"utf-8\") as file:\n",
    "    config_file_b = yaml.safe_load(file)\n",
    "    \n",
    "core_b = WurstCoreB(config_dict=config_file_b, device=device, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ed50b0",
   "metadata": {},
   "source": [
    "### Load Extras & Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extras = core.setup_extras_pre()\n",
    "models = core.setup_models(extras)\n",
    "models.generator.eval().requires_grad_(False)\n",
    "print(\"CONTROLNET READY\")\n",
    "\n",
    "extras_b = core_b.setup_extras_pre()\n",
    "models_b = core_b.setup_models(extras_b, skip_clip=True)\n",
    "models_b = WurstCoreB.Models(\n",
    "   **{**models_b.to_dict(), 'tokenizer': models.tokenizer, 'text_model': models.text_model}\n",
    ")\n",
    "models_b.generator.eval().requires_grad_(False)\n",
    "print(\"STAGE B READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f80062",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Inpainting / Outpainting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347aa162",
   "metadata": {},
   "source": [
    "**Note**: You are able to define your own masks with the `mask` parameter or for demonstration purposes, use what we do during training to generate masks: use a tiny saliency model to predict the area of \"interesting content\", like an animal, a person, an object etc. This results in masks that closely mimic how humans actually inpaint, can be calculated extremely fast and with just a few lines of code. You have two parameters to control the masks `threshold` and `outpaint`. The former determines how much area will be masked and `outpaint` would just flip the predicted mask. Just play around with the parameters and you will get a feeling for it (`theshold` should be between 0.0 and 0.4). If you do wish, to load your own masks, just uncomment the `mask` parameter and replace it with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49161a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "url = \"https://cdn.discordapp.com/attachments/1121232062708457508/1204787053892603914/cat_dog.png?ex=65d60061&is=65c38b61&hm=37c3d179a39b1eca4b8894e3c239930cedcbb965da00ae2209cca45f883f86f4&\"\n",
    "images = resize_image(download_image(url)).unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "\n",
    "batch = {'images': images}\n",
    "\n",
    "mask = None\n",
    "# mask = torch.ones(batch_size, 1, images.size(2), images.size(3)).bool()\n",
    "\n",
    "outpaint = False\n",
    "threshold = 0.2\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "    cnet, cnet_input = core.get_cnet(batch, models, extras, mask=mask, outpaint=outpaint, threshold=threshold)\n",
    "    cnet_uncond = cnet\n",
    "    \n",
    "show_images(batch['images'])\n",
    "show_images(cnet_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1a2ea6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Face Identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3885a87",
   "metadata": {},
   "source": [
    "**Note**: This ControlNet lets you generate images based on faces in a given image. Simply load an image or enter the `url`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ada1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "url = \"https://cdn.discordapp.com/attachments/1039261364935462942/1200109692978999317/three_people.png?ex=65c4fc3f&is=65b2873f&hm=064a8cebea5560b74e7088be9d1399a5fe48863d1581e65ea9d6734725f4c8d3&\"\n",
    "images = resize_image(download_image(url)).unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "\n",
    "batch = {'images': images}\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "    cnet, cnet_input = core.get_cnet(batch, models, extras)\n",
    "    cnet_uncond = core.get_cnet({'images': torch.zeros_like(batch['images'])}, models, extras)[0]\n",
    "    \n",
    "show_images(batch['images'])\n",
    "show_images(cnet_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dffa23b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Canny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b1498",
   "metadata": {},
   "source": [
    "**Note**: This here is a typical ControlNet for Canny Edge Detection. You can also use it for doing *sketch-to-image*. You can enable that, by setting `sketch = True` and providing a sketch as the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "url = \"https://media.discordapp.net/attachments/1177378292765036716/1205484279405219861/image.png?ex=65d889b9&is=65c614b9&hm=0722ab9707b48d677316c0b4de5e51702b43eac1e27b76c268a069ec67ff6d15&=&format=webp&quality=lossless&width=861&height=859\"\n",
    "images = resize_image(download_image(url)).unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "sketch = False\n",
    "\n",
    "batch = {'images': images}\n",
    "\n",
    "if sketch:\n",
    "    cnet_input = 1-images.mean(dim=1, keepdim=True)\n",
    "else:\n",
    "    cnet_input = None\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "    cnet, cnet_input = core.get_cnet(batch, models, extras, cnet_input=cnet_input)\n",
    "    cnet_uncond = cnet\n",
    "    \n",
    "show_images(batch['images'])\n",
    "show_images(cnet_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7e5ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Super Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66544b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "cnet_override = None\n",
    "# url = \"https://media.discordapp.net/attachments/1121232062708457508/1205134173053132810/image.png?ex=65d743a9&is=65c4cea9&hm=48dc4901514caada29271f48d76431f3a648940f2fda9e643a6bb693c906cc09&=&format=webp&quality=lossless&width=862&height=857\"\n",
    "# url = \"https://cdn.discordapp.com/attachments/1121232062708457508/1204787053892603914/cat_dog.png?ex=65d60061&is=65c38b61&hm=37c3d179a39b1eca4b8894e3c239930cedcbb965da00ae2209cca45f883f86f4&\"\n",
    "url = \"https://cdn.discordapp.com/attachments/1121232062708457508/1205110687538479145/A_photograph_of_a_sunflower_with_sunglasses_on_in__3.jpg?ex=65d72dc9&is=65c4b8c9&hm=72172e774ce6cda618503b3778b844de05cd1208b61e185d8418db512fb2858a&\"\n",
    "images = resize_image(download_image(url)).unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "\n",
    "batch = {'images': images}\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "    effnet_latents = core.encode_latents(batch, models, extras)\n",
    "    effnet_latents_up = torch.nn.functional.interpolate(effnet_latents, scale_factor=2, mode=\"nearest\")\n",
    "    cnet = models.controlnet(effnet_latents_up)\n",
    "    cnet_uncond = cnet\n",
    "    cnet_input = torch.nn.functional.interpolate(images, scale_factor=2, mode=\"nearest\")\n",
    "    # cnet, cnet_input = core.get_cnet(batch, models, extras)\n",
    "    # cnet_uncond = cnet\n",
    "    \n",
    "show_images(batch['images'])\n",
    "show_images(cnet_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dadbf4-ccfa-459b-82f4-483f73e08939",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Optional: Compile Stage C and Stage B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1f975a-d012-4cfb-a3c0-3542a09e3604",
   "metadata": {},
   "source": [
    "**Note**: This will increase speed inference by about 2x, but will initially take a few minutes to compile. Moreover, currently using `torch.compile` only works for a single image resolution, e.g. 1024 x 1024. If you use a different size, it will recompile. See more [here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aec11b-3ba4-4f7f-98bb-16eda5d7cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ControlNetCore.Models(\n",
    "   **{**models.to_dict(), 'generator': torch.compile(models.generator, mode=\"reduce-overhead\", fullgraph=True)}\n",
    ")\n",
    "\n",
    "models_b = WurstCoreB.Models(\n",
    "   **{**models_b.to_dict(), 'generator': torch.compile(models_b.generator, mode=\"reduce-overhead\", fullgraph=True)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892b863",
   "metadata": {},
   "source": [
    "## ControlNet Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae79908",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = \"An oil painting\"\n",
    "cnet_multiplier = 1.0 # 0.8 # 0.3\n",
    "\n",
    "if \"controlnet_c_3b_sr\" in config_file:\n",
    "    height, width = int(cnet[0].size(-2)*32*4/3), int(cnet[0].size(-1)*32*4/3)\n",
    "else:\n",
    "    height, width = 1024, 1024\n",
    "stage_c_latent_shape, stage_b_latent_shape = calculate_latent_sizes(height, width, batch_size=batch_size)\n",
    "\n",
    "# Stage C Parameters\n",
    "extras.sampling_configs['cfg'] = 1\n",
    "extras.sampling_configs['shift'] = 2\n",
    "extras.sampling_configs['timesteps'] = 20\n",
    "extras.sampling_configs['t_start'] = 1.0\n",
    "\n",
    "# Stage B Parameters\n",
    "extras_b.sampling_configs['cfg'] = 1.1\n",
    "extras_b.sampling_configs['shift'] = 1\n",
    "extras_b.sampling_configs['timesteps'] = 10\n",
    "extras_b.sampling_configs['t_start'] = 1.0\n",
    "\n",
    "# PREPARE CONDITIONS\n",
    "batch['captions'] = [caption] * batch_size\n",
    "conditions = core.get_conditions(batch, models, extras, is_eval=True, is_unconditional=False, eval_image_embeds=False)\n",
    "unconditions = core.get_conditions(batch, models, extras, is_eval=True, is_unconditional=True, eval_image_embeds=False)    \n",
    "conditions['cnet'] = [c.clone() * cnet_multiplier if c is not None else c for c in cnet]\n",
    "unconditions['cnet'] = [c.clone() * cnet_multiplier if c is not None else c for c in cnet_uncond]\n",
    "conditions_b = core_b.get_conditions(batch, models_b, extras_b, is_eval=True, is_unconditional=False)\n",
    "unconditions_b = core_b.get_conditions(batch, models_b, extras_b, is_eval=True, is_unconditional=True)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "    # torch.manual_seed(42)\n",
    "\n",
    "    sampling_c = extras.gdf.sample(\n",
    "        models.generator, conditions, stage_c_latent_shape,\n",
    "        unconditions, device=device, **extras.sampling_configs,\n",
    "    )\n",
    "    for (sampled_c, _, _) in tqdm(sampling_c, total=extras.sampling_configs['timesteps']):\n",
    "        sampled_c = sampled_c\n",
    "        \n",
    "    # preview_c = models.previewer(sampled_c).float()\n",
    "    # show_images(preview_c)\n",
    "\n",
    "    conditions_b['effnet'] = sampled_c\n",
    "    unconditions_b['effnet'] = torch.zeros_like(sampled_c)\n",
    "\n",
    "    sampling_b = extras_b.gdf.sample(\n",
    "        models_b.generator, conditions_b, stage_b_latent_shape,\n",
    "        unconditions_b, device=device, **extras_b.sampling_configs\n",
    "    )\n",
    "    for (sampled_b, _, _) in tqdm(sampling_b, total=extras_b.sampling_configs['timesteps']):\n",
    "        sampled_b = sampled_b\n",
    "    sampled = models_b.stage_a.decode(sampled_b).float()\n",
    "\n",
    "show_images(cnet_input)\n",
    "show_images(sampled)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
