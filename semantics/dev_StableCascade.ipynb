{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir('StableCascade')\n",
    "from inference.utils import *\n",
    "from core.utils import load_or_fail\n",
    "from train import WurstCoreC, WurstCoreB\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_version', 'effnet_checkpoint_path', 'previewer_checkpoint_path']\n",
      "['model_version', 'stage_a_checkpoint_path', 'effnet_checkpoint_path']\n",
      "['transforms', 'clip_preprocess', 'gdf', 'sampling_configs', 'effnet_preprocess']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84368664277452aa4c06108b40d86f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenizer', 'text_model', 'generator', 'effnet', 'previewer']\n",
      "STAGE C READY\n",
      "['transforms', 'clip_preprocess', 'gdf', 'sampling_configs', 'effnet_preprocess']\n",
      "['tokenizer', 'text_model', 'generator', 'effnet', 'stage_a']\n",
      "['tokenizer', 'text_model', 'generator', 'effnet', 'stage_a']\n",
      "STAGE B READY\n",
      "['tokenizer', 'text_model', 'generator', 'effnet', 'previewer']\n",
      "['tokenizer', 'text_model', 'generator', 'effnet', 'stage_a']\n",
      "CPU times: user 1min 57s, sys: 1min 1s, total: 2min 59s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Load config\n",
    "\n",
    "# SETUP STAGE C\n",
    "config_file = 'configs/inference/stage_c_3b.yaml'\n",
    "with open(config_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    loaded_config = yaml.safe_load(file)\n",
    "\n",
    "core = WurstCoreC(config_dict=loaded_config, device=device, training=False)\n",
    "\n",
    "# SETUP STAGE B\n",
    "config_file_b = 'configs/inference/stage_b_3b.yaml'\n",
    "with open(config_file_b, \"r\", encoding=\"utf-8\") as file:\n",
    "    config_file_b = yaml.safe_load(file)\n",
    "    \n",
    "core_b = WurstCoreB(config_dict=config_file_b, device=device, training=False)\n",
    "\n",
    "# SETUP MODELS & DATA\n",
    "extras = core.setup_extras_pre()\n",
    "models = core.setup_models(extras)\n",
    "models.generator.eval().requires_grad_(False)\n",
    "print(\"STAGE C READY\")\n",
    "\n",
    "extras_b = core_b.setup_extras_pre()\n",
    "models_b = core_b.setup_models(extras_b, skip_clip=True)\n",
    "models_b = WurstCoreB.Models(\n",
    "   **{**models_b.to_dict(), 'tokenizer': models.tokenizer, 'text_model': models.text_model}\n",
    ")\n",
    "models_b.generator.bfloat16().eval().requires_grad_(False)\n",
    "print(\"STAGE B READY\")\n",
    "\n",
    "# COMPILE\n",
    "models = WurstCoreC.Models(\n",
    "   **{**models.to_dict(), 'generator': torch.compile(models.generator, mode=\"reduce-overhead\", fullgraph=True)}\n",
    ")\n",
    "\n",
    "models_b = WurstCoreB.Models(\n",
    "   **{**models_b.to_dict(), 'generator': torch.compile(models_b.generator, mode=\"reduce-overhead\", fullgraph=True)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 20/20 [02:50<00:00,  8.52s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "# caption = \"Cinematic photo of an anthropomorphic nerdy rodent sitting in a cafe reading a book\"\n",
    "caption = \"Cinematic photo of an anthropomorphic penguin sitting in a cafe reading a book and having a coffee\"\n",
    "height, width = 256, 256\n",
    "stage_c_latent_shape, stage_b_latent_shape = calculate_latent_sizes(height, width, batch_size=batch_size)\n",
    "\n",
    "# Stage C Parameters\n",
    "extras.sampling_configs['cfg'] = 4\n",
    "extras.sampling_configs['shift'] = 2\n",
    "extras.sampling_configs['timesteps'] = 20\n",
    "extras.sampling_configs['t_start'] = 1.0\n",
    "\n",
    "# Stage B Parameters\n",
    "extras_b.sampling_configs['cfg'] = 1.1\n",
    "extras_b.sampling_configs['shift'] = 1\n",
    "extras_b.sampling_configs['timesteps'] = 10\n",
    "extras_b.sampling_configs['t_start'] = 1.0\n",
    "\n",
    "# PREPARE CONDITIONS\n",
    "batch = {'captions': [caption] * batch_size}\n",
    "conditions = core.get_conditions(batch, models, extras, is_eval=True, is_unconditional=False, eval_image_embeds=False)\n",
    "unconditions = core.get_conditions(batch, models, extras, is_eval=True, is_unconditional=True, eval_image_embeds=False)    \n",
    "conditions_b = core_b.get_conditions(batch, models_b, extras_b, is_eval=True, is_unconditional=False)\n",
    "unconditions_b = core_b.get_conditions(batch, models_b, extras_b, is_eval=True, is_unconditional=True)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "    # torch.manual_seed(42)\n",
    "\n",
    "    sampling_c = extras.gdf.sample(\n",
    "        models.generator, conditions, stage_c_latent_shape,\n",
    "        unconditions, device=device, **extras.sampling_configs,\n",
    "    )\n",
    "    for (sampled_c, _, _) in tqdm(sampling_c, total=extras.sampling_configs['timesteps']):\n",
    "        sampled_c = sampled_c\n",
    "        \n",
    "    # preview_c = models.previewer(sampled_c).float()\n",
    "    # show_images(preview_c)\n",
    "\n",
    "    conditions_b['effnet'] = sampled_c\n",
    "    unconditions_b['effnet'] = torch.zeros_like(sampled_c)\n",
    "\n",
    "    sampling_b = extras_b.gdf.sample(\n",
    "        models_b.generator, conditions_b, stage_b_latent_shape,\n",
    "        unconditions_b, device=device, **extras_b.sampling_configs\n",
    "    )\n",
    "    for (sampled_b, _, _) in tqdm(sampling_b, total=extras_b.sampling_configs['timesteps']):\n",
    "        sampled_b = sampled_b\n",
    "    sampled = models_b.stage_a.decode(sampled_b).float()\n",
    "\n",
    "show_images(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.1.1+py3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
