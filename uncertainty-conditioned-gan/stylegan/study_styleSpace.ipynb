{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from stylegan2.training.dataset import ImageFolderDataset\n",
    "import stylegan2.dnnlib\n",
    "import stylegan2.legacy\n",
    "from models import CNN_MNIST\n",
    "\n",
    "for p in [\n",
    "    Path('/d/alecoz/projects'), # DeepLab\n",
    "    Path(os.path.expandvars('$WORK')), # Jean Zay\n",
    "    Path('w:/')]: # local\n",
    "    if os.path.exists(p):\n",
    "        path_main = p\n",
    "\n",
    "path_results = path_main / 'domain-images/results'\n",
    "path_data = path_main / 'DATA'\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists(path_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_selected = 9\n",
    "# class_selected= 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CLEAN DATA\n",
    "# # path_model = path_results/ 'stylegan2-training-runs' / '00000-mnist_stylegan2-auto2'\n",
    "# # path_model = path_results/ 'stylegan2-training-runs' / '00001-mnist_stylegan2-cond-cifar' # best FID\n",
    "# path_model = path_results/ 'stylegan2-training-runs' / '00002-mnist_stylegan2-cond-auto2-classifCond' # classifCond\n",
    "# path_classifier = path_results / 'classifiers' / 'CNN_mnist_clean_20230303_1618.pth'\n",
    "# dataset_train = 'mnist_stylegan2'\n",
    "# dataset_test = 'mnistTest_stylegan2'\n",
    "\n",
    "# DATA CORRUPTED (TOO MUCH)\n",
    "# path_model = path_results / 'stylegan2-training-runs' / '00004-mnist_stylegan2_blur_noise_maxSeverity5_proba100-cond-cifar-classifCond'\n",
    "path_model = path_results / 'stylegan2-training-runs' / '00003-mnist_stylegan2_blur_noise_maxSeverity5_proba100-cond-cifar'\n",
    "path_classifier = path_results / 'classifiers' / 'CNN_mnist_stylegan2_blur_noise_maxSeverity5_proba100_20230307_1741.pth'\n",
    "dataset_train = 'mnist_stylegan2_blur_noise_maxSeverity5_proba100'\n",
    "dataset_test = 'mnistTest_stylegan2_blur_noise_maxSeverity5_proba100'\n",
    "\n",
    "# # DATA CORRUPTED\n",
    "# path_model = path_results / 'stylegan2-training-runs' / '00006-mnist_stylegan2_blur_noise_maxSeverity5_proba50-cond-cifar-classifCond'\n",
    "# path_classifier = path_results / 'classifiers' / 'CNN_mnist_stylegan2_blur_noise_maxSeverity5_proba50_20230307_1800.pth'\n",
    "# dataset_train = 'mnist_stylegan2_blur_noise_maxSeverity5_proba50'\n",
    "# dataset_test = 'mnistTest_stylegan2_blur_noise_maxSeverity5_proba50'\n",
    "\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_images(images):\n",
    "    assert images.dim() == 4, \"Expected 4D (B x C x H x W) image tensor, got {}D\".format(images.dim())\n",
    "    images = ((images + 1) / 2).clamp(0, 1) # scale\n",
    "    images = images[:, :, 2:30, 2:30] # remove padding\n",
    "    return images\n",
    "\n",
    "def plot_images_grid(images, title=''):\n",
    "    images = images * 255\n",
    "    images = images.to(torch.uint8)\n",
    "    plt.figure()\n",
    "    plt.imshow(vutils.make_grid(images.cpu(), pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.grid(False)\n",
    "    plt.title(title)\n",
    "    \n",
    "def generate_random_images(n_images=5):\n",
    "    z = torch.randn((n_images, G.z_dim), device=device)\n",
    "    labels = torch.randint(0, n_classes, (n_images,), device=device)\n",
    "    c = F.one_hot(labels, n_classes)\n",
    "    ws = G.mapping(z, c, truncation_psi=1)\n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "    img = postprocess_images(img)\n",
    "    return img\n",
    "\n",
    "def get_classifier_MSP(logits):\n",
    "    max_softmax_proba = torch.max(torch.softmax(logits, axis=1), axis=1).values\n",
    "    return max_softmax_proba\n",
    "\n",
    "def get_classifier_TCP(logits, labels):\n",
    "    y_as_idx = F.one_hot(labels.long(), num_classes=10).bool()\n",
    "    probas = torch.softmax(logits, dim=1)\n",
    "    true_class_proba = probas[y_as_idx]\n",
    "    return true_class_proba\n",
    "\n",
    "def mahalanobis(U, v):\n",
    "    ''' Compute the Mahalanobis distance between each row of U and v'''\n",
    "    cov_inv = torch.linalg.inv(torch.cov(U.T))\n",
    "    m = torch.zeros(U.shape[0])\n",
    "    for i in range(U.shape[0]):\n",
    "        delta = U[i, :] - v\n",
    "        m[i] = torch.sqrt(torch.matmul(torch.matmul(delta, cov_inv), delta.T))\n",
    "    return m\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_images(images, title=''):\n",
    "    images = images * 255\n",
    "    images = images.to(torch.uint8)\n",
    "    plt.figure()\n",
    "    plt.imshow(vutils.make_grid(images.cpu(), pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.grid(False)\n",
    "    plt.title(title)\n",
    "\n",
    "\n",
    "def generate_from_z(z):\n",
    "    for i in np.arange(0, z.shape[0], batch_size):\n",
    "        img = G(z[i:i+batch_size], c=None, noise_mode='const', force_fp32=True)\n",
    "        if i == 0: \n",
    "            imgs = img\n",
    "        else:\n",
    "            imgs = torch.cat((imgs, img))\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def plot_random_images(imgs):\n",
    "    # from generate.py: img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "    # imgs = (imgs * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "    imgs = postprocess_images(imgs)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(vutils.make_grid(imgs[torch.randint(0, imgs.shape[0], (100,))].cpu(), pad_value=255, nrow=10).permute(1,2,0))\n",
    "\n",
    "def plot_images_from_s(s):\n",
    "    imgs = generate_img_from_s(s)\n",
    "    imgs = postprocess_images(imgs)\n",
    "    plot_images(imgs)\n",
    "    \n",
    "def truncate(x, x_avg, psi):\n",
    "    # psi=0 means we get average value, \n",
    "    # psi=1 we get original value, \n",
    "    # 0<psi<1 we get interpolation between mean and original\n",
    "    return x_avg.lerp(x, psi)\n",
    "\n",
    "\n",
    "def styleSpace_dict2vec(styleSpace_dict):\n",
    "    styleSpace_vec = []\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            key = f'b{res}.{layer}'\n",
    "            values = styleSpace_dict[key]\n",
    "            if values.dim() == 1: values = values.unsqueeze(0)\n",
    "            styleSpace_vec.append(values)\n",
    "    styleSpace_vec = torch.cat(styleSpace_vec, dim=1)\n",
    "    return styleSpace_vec\n",
    "\n",
    "\n",
    "def styleSpace_vec2dict(styleSpace_vec):\n",
    "    if styleSpace_vec.dim() == 1:\n",
    "        styleSpace_vec = styleSpace_vec.unsqueeze(0)\n",
    "    styleSpace_dict = {}\n",
    "    dim_base = 0\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        block = getattr(G.synthesis, f'b{res}')\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            block_layer = getattr(block, layer)\n",
    "            dim_size = block_layer.affine.weight.shape[1]\n",
    "            key = f'b{res}.{layer}'\n",
    "            styleSpace_dict[key] = styleSpace_vec[:, dim_base:dim_base+dim_size]#.squeeze()\n",
    "            dim_base += dim_size\n",
    "    assert dim_base == styleSpace_vec.shape[1]\n",
    "    return styleSpace_dict\n",
    "\n",
    "\n",
    "def compute_styleSpace_vec_idx2coord():\n",
    "    vec_idx2coord = {}\n",
    "    idx = 0\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        block = getattr(G.synthesis, f'b{res}')\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            block_layer = getattr(block, layer)\n",
    "            dim_size = block_layer.affine.weight.shape[1]\n",
    "            for dim in range(dim_size):\n",
    "                vec_idx2coord[idx] = (f'b{res}.{layer}', dim)\n",
    "                idx += 1\n",
    "    return vec_idx2coord\n",
    "\n",
    "\n",
    "# function to generate image from S\n",
    "def generate_img_from_s(s):\n",
    "    def set_style(values):\n",
    "        def hook(module, input, output):\n",
    "            output = values\n",
    "            return output\n",
    "        return hook\n",
    "    \n",
    "    if type(s) != dict: s = styleSpace_vec2dict(s)\n",
    "    assert s['b4.conv1'].dim() == 2, 'Should be of 2 dimensions: batch_size x s_dim'\n",
    "    batch_size = s['b4.conv1'].shape[0]\n",
    "    \n",
    "    handles = []\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        block = getattr(G.synthesis, f'b{res}')\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            block_layer = getattr(block, layer)\n",
    "            values = s[f'b{res}.{layer}']\n",
    "            handles.append(block_layer.affine.register_forward_hook(set_style(values)))\n",
    "    \n",
    "    dummy_ws = torch.zeros((batch_size, G.num_ws, G.w_dim), device=device)\n",
    "    img = G.synthesis(dummy_ws, noise_mode='const', force_fp32=True)\n",
    "\n",
    "    for h in handles: h.remove()\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD GENERATOR\n",
    "if not str(path_model).endswith('pkl'):\n",
    "    # find best model in folder\n",
    "    with open(path_model / 'metric-fid50k_full.jsonl', 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    best_fid = 1e6\n",
    "    for json_str in json_list:\n",
    "        json_line = json.loads(json_str)\n",
    "        if json_line['results']['fid50k_full'] < best_fid:\n",
    "            best_fid = json_line['results']['fid50k_full']\n",
    "            best_model = json_line['snapshot_pkl']\n",
    "    print('Best FID: {:.2f} ; best model : {}'.format(best_fid, best_model))\n",
    "    path_model = path_model / best_model\n",
    "    with open(path_model, 'rb') as f:\n",
    "        G = pickle.load(f)['G_ema'].eval().to(device)  # torch.nn.Module\n",
    "else:\n",
    "    with dnnlib.util.open_url(path_model) as f:\n",
    "        G = legacy.load_network_pkl(f)['G_ema'].eval().requires_grad_(False).to(device)\n",
    "\n",
    "# registor hooks to save intermediate values (images and style space)\n",
    "intermediate_images_torgb = {}\n",
    "def get_torgb(name):\n",
    "    def hook(module, input, output):\n",
    "        intermediate_images_torgb[name] = output.detach()\n",
    "    return hook\n",
    "intermediate_images_block = {}\n",
    "def get_block_img(name):\n",
    "    def hook(module, input, output):\n",
    "        intermediate_images_block[name] = output[1].detach()\n",
    "    return hook\n",
    "styleSpace_values = {}\n",
    "def get_styleSpace_values(name):\n",
    "    def hook(module, input, output):\n",
    "        styleSpace_values[name] = output.detach()\n",
    "    return hook\n",
    "for res in G.synthesis.block_resolutions:\n",
    "    block = getattr(G.synthesis, f'b{res}')\n",
    "    block.torgb.register_forward_hook(get_torgb(res))\n",
    "    block.register_forward_hook(get_block_img(res))\n",
    "    for layer in ['conv0', 'conv1', 'torgb']:\n",
    "        if res == 4 and layer == 'conv0': continue\n",
    "        block_layer = getattr(block, layer)\n",
    "        block_layer.affine.register_forward_hook(get_styleSpace_values(name=f'b{res}.{layer}'))\n",
    "\n",
    "        \n",
    "# backward hooks to get gradients relative to styleSpace\n",
    "styleSpace_grads = {}\n",
    "def get_styleSpace_grads(name):\n",
    "    def hook(self, grad_input, grad_output):\n",
    "        styleSpace_grads[name] = grad_output[0].detach()\n",
    "    return hook\n",
    "\n",
    "for res in G.synthesis.block_resolutions:\n",
    "    block = getattr(G.synthesis, f'b{res}')\n",
    "    block.torgb.register_forward_hook(get_torgb(res))\n",
    "    block.register_forward_hook(get_block_img(res))\n",
    "    for layer in ['conv0', 'conv1', 'torgb']:\n",
    "        if res == 4 and layer == 'conv0': continue\n",
    "        block_layer = getattr(block, layer)\n",
    "        block_layer.affine.register_full_backward_hook(get_styleSpace_grads(name=f'b{res}.{layer}'))\n",
    "\n",
    "        \n",
    "# dict to convert index to coordinate for stylespace vectors\n",
    "styleSpace_vec_idx2coord = compute_styleSpace_vec_idx2coord()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CLASSIFIER\n",
    "classifier = CNN_MNIST()\n",
    "classifier.load_state_dict(torch.load(path_classifier, map_location=device))\n",
    "classifier = classifier.eval().requires_grad_(False).to(device)\n",
    "\n",
    "\n",
    "# LOAD DATASET\n",
    "path_dataset = path_data / 'MNIST' / f'{dataset_train}.zip'\n",
    "train_data = ImageFolderDataset(path_dataset, use_labels=True)\n",
    "train_dataloader = DataLoader(train_data, batch_size=128)\n",
    "\n",
    "path_dataset = path_data / 'MNIST' / f'{dataset_test}.zip'\n",
    "test_data = ImageFolderDataset(path_dataset, use_labels=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MSP_TCP(dataloader, classifier, device):\n",
    "\n",
    "    classifier.eval()\n",
    "    msp = torch.zeros((len(dataloader.dataset)))\n",
    "    tcp = torch.zeros((len(dataloader.dataset)))\n",
    "    idx = 0\n",
    "    for X, y in dataloader:\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        X = (X / 255)[:, :, 2:30, 2:30]\n",
    "        y = y.argmax(1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = classifier(X)\n",
    "        msp[idx:idx+batch_size] = get_classifier_MSP(logits)\n",
    "        tcp[idx:idx+batch_size] = get_classifier_TCP(logits, y)\n",
    "        idx += batch_size\n",
    "\n",
    "    return msp, tcp\n",
    "\n",
    "\n",
    "\n",
    "msp_train, tcp_train = get_MSP_TCP(train_dataloader, classifier, device)\n",
    "msp_test, tcp_test = get_MSP_TCP(test_dataloader, classifier, device)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.set_xlabel('train')\n",
    "ax1.hist(msp_train, alpha=0.5, bins=50, density=True, label='MSP', log=True)\n",
    "ax1.hist(tcp_train, alpha=0.5, bins=50, density=True, label='TCP', log=True)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_xlabel('test')\n",
    "ax2.hist(msp_test, alpha=0.5, bins=50, density=True, label='MSP', log=True)\n",
    "ax2.hist(tcp_test, alpha=0.5, bins=50, density=True, label='TCP', log=True)\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if G.c_dim == 11:\n",
    "    n_images = 8\n",
    "\n",
    "    z = torch.randn((n_images, G.z_dim), device=device)\n",
    "    if class_selected == 'all':\n",
    "        labels = torch.randint(0, n_classes, (n_images,), device=device)\n",
    "    else:\n",
    "        labels = class_selected * torch.ones((n_images,), dtype=torch.int64, device=device)\n",
    "    c_labels = F.one_hot(labels, n_classes)\n",
    "    c_classif = torch.ones((n_images, 1), device=device)\n",
    "    c = torch.cat((c_labels, c_classif), dim=1)\n",
    "\n",
    "    ws = G.mapping(z, c, truncation_psi=1)\n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "    img = postprocess_images(img)\n",
    "    plot_images_grid(img, title='Generated images, confidence=1')\n",
    "    plt.show()\n",
    "    print('confidences:', F.softmax(classifier(img), dim=1).max(dim=1).values)\n",
    "\n",
    "    # change only classif condition\n",
    "    proba = 0.7\n",
    "    c_classif = proba * torch.ones((n_images, 1), device=device)\n",
    "    c = torch.cat((c_labels, c_classif), dim=1)\n",
    "\n",
    "    ws = G.mapping(z, c, truncation_psi=1)\n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "    img = postprocess_images(img)\n",
    "    plot_images_grid(img, title=f'Generated images, confidence={proba}')\n",
    "    plt.show()\n",
    "    print('confidences:', F.softmax(classifier(img), dim=1).max(dim=1).values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSP direction in S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if G.c_dim == 11:\n",
    "    n_images = 10000\n",
    "    confidence_max = 1\n",
    "    confidence_min = 0.9\n",
    "\n",
    "    z = torch.randn((n_images, G.z_dim), device=device)\n",
    "    if class_selected == 'all':\n",
    "        labels = torch.randint(0, n_classes, (n_images,), device=device)\n",
    "    else:\n",
    "        labels = class_selected * torch.ones((n_images,), dtype=torch.int64, device=device)\n",
    "    c_labels = F.one_hot(labels, n_classes)\n",
    "\n",
    "    c_classif = confidence_max*torch.ones((n_images, 1), device=device)\n",
    "    c = torch.cat((c_labels, c_classif), dim=1)\n",
    "    s_max = torch.empty((n_images, 5632), device=device)\n",
    "    idx = 0\n",
    "    for z_i, c_i in zip(DataLoader(z, batch_size=128), DataLoader(c, batch_size=128)):\n",
    "        batch_size = c_i.shape[0]\n",
    "        with torch.no_grad():\n",
    "            ws = G.mapping(z_i, c_i, truncation_psi=1)\n",
    "            img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "        s_vec = styleSpace_dict2vec(styleSpace_values)\n",
    "        s_max[idx:idx+batch_size] = s_vec\n",
    "        idx += batch_size\n",
    "        \n",
    "        \n",
    "        \n",
    "    # WITH NEW Z\n",
    "    # z = torch.randn((n_images, G.z_dim), device=device)\n",
    "    # labels = torch.randint(0, n_classes, (n_images,), device=device)\n",
    "    # c_labels = F.one_hot(labels, n_classes)\n",
    "    # c_classif = confidence_min*torch.ones((n_images, 1), device=device)\n",
    "    # c = torch.cat((c_labels, c_classif), dim=1)\n",
    "    # w_min = G.mapping(z, c, truncation_psi=1)[:,0,:].mean(dim=0)\n",
    "\n",
    "    # WITHOUT NEW Z\n",
    "    c_classif = confidence_min*torch.ones((n_images, 1), device=device)\n",
    "    c = torch.cat((c_labels, c_classif), dim=1)\n",
    "    s_min = torch.empty((n_images, 5632), device=device)\n",
    "    idx = 0\n",
    "    for z_i, c_i in zip(DataLoader(z, batch_size=128), DataLoader(c, batch_size=128)):\n",
    "        batch_size = c_i.shape[0]\n",
    "        with torch.no_grad():\n",
    "            ws = G.mapping(z_i, c_i, truncation_psi=1)\n",
    "            img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "        s_vec = styleSpace_dict2vec(styleSpace_values)\n",
    "        s_min[idx:idx+batch_size] = s_vec\n",
    "        idx += batch_size\n",
    "\n",
    "\n",
    "    s_max = s_max.mean(0)\n",
    "    s_min = s_min.mean(0)\n",
    "    s_dir_msp = (s_max - s_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if G.c_dim == 11:\n",
    "    n_images = 8\n",
    "    lambda_ = 2\n",
    "\n",
    "    z = torch.randn((n_images, G.z_dim), device=device)\n",
    "    if class_selected == 'all':\n",
    "        labels = torch.randint(0, n_classes, (n_images,), device=device)\n",
    "    else:\n",
    "        labels = class_selected * torch.ones((n_images,), dtype=torch.int64, device=device)\n",
    "    c_labels = F.one_hot(labels, n_classes)\n",
    "    c_classif = torch.ones((n_images, 1), device=device)\n",
    "    c = torch.cat((c_labels, c_classif), dim=1)\n",
    "\n",
    "    ws = G.mapping(z, c, truncation_psi=1)\n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "    img = postprocess_images(img)\n",
    "    s_vec = styleSpace_dict2vec(styleSpace_values)\n",
    "    plot_images_grid(img, title='Generated images, confidence=1')\n",
    "    plt.show()\n",
    "    print('confidences:', F.softmax(classifier(img), dim=1).max(dim=1).values)\n",
    "\n",
    "    # move along w_dir_msp\n",
    "\n",
    "    s_vec_moved = s_vec - lambda_ * s_dir_msp\n",
    "    img = generate_img_from_s(s_vec_moved)\n",
    "    img = postprocess_images(img)\n",
    "    plot_images_grid(img, title=f'Generated images, edited along s_dir_msp')\n",
    "    plt.show()\n",
    "    print('confidences:', F.softmax(classifier(img), dim=1).max(dim=1).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_from_ws(ws, labels=None):\n",
    "    dataloader = DataLoader(TensorDataset(ws, labels), 128)\n",
    "    s = torch.empty((ws.shape[0], 5632), device=device)\n",
    "    msp = torch.empty(ws.shape[0])\n",
    "    tcp = torch.empty(ws.shape[0])\n",
    "    idx_batch = 0\n",
    "    for ws_, y in dataloader:\n",
    "        batch_size = ws_.shape[0]\n",
    "        img = G.synthesis(ws_, noise_mode='const')\n",
    "        img = postprocess_images(img)\n",
    "        logits = classifier(img)\n",
    "        s[idx_batch:idx_batch+batch_size] = styleSpace_dict2vec(styleSpace_values)\n",
    "        msp[idx_batch:idx_batch+batch_size] = get_classifier_MSP(logits).cpu()\n",
    "        tcp[idx_batch:idx_batch+batch_size] = get_classifier_TCP(logits, y).cpu()\n",
    "        idx_batch += batch_size\n",
    "    well_classified = msp == tcp\n",
    "    return s, msp, tcp, well_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "if class_selected == 'all':\n",
    "    n_images_train = 60000\n",
    "    labels_train = torch.randint(0, n_classes, (n_images_train,), device=device)\n",
    "else:\n",
    "    n_images_train = 60000\n",
    "    labels_train = class_selected * torch.ones((n_images_train,), dtype=torch.int64, device=device)\n",
    "z_train = torch.randn((n_images_train, G.z_dim), device=device)\n",
    "c_labels_train = F.one_hot(labels_train, n_classes)\n",
    "if G.c_dim == 11:\n",
    "    c_classif_train = 0.8 + 0.2 * torch.rand((n_images_train, 1), device=device)\n",
    "    c_train = torch.cat((c_labels_train, c_classif_train), dim=1)\n",
    "elif G.c_dim == 10:\n",
    "    c_train = c_labels_train\n",
    "\n",
    "ws_train = G.mapping(z_train, c_train, truncation_psi=1)\n",
    "w_train = ws_train[:, 0, :]\n",
    "s_train, msp_train, tcp_train, well_classified_train = classify_from_ws(ws_train, labels_train)\n",
    "\n",
    "# VAL\n",
    "if class_selected == 'all':\n",
    "    n_images_val = 10000\n",
    "    labels_val = torch.randint(0, n_classes, (n_images_val,), device=device)\n",
    "else:\n",
    "    n_images_val = 10000\n",
    "    labels_val = class_selected * torch.ones((n_images_val,), dtype=torch.int64, device=device)\n",
    "z_val = torch.randn((n_images_val, G.z_dim), device=device)\n",
    "c_labels_val = F.one_hot(labels_val, n_classes)\n",
    "if G.c_dim == 11:\n",
    "    c_classif_val = 0.8 + 0.2 * torch.rand((n_images_val, 1), device=device)\n",
    "    c_val = torch.cat((c_labels_val, c_classif_val), dim=1)\n",
    "elif G.c_dim == 10:\n",
    "    c_val = c_labels_val\n",
    "\n",
    "ws_val = G.mapping(z_val, c_val, truncation_psi=1)\n",
    "w_val = ws_val[:, 0, :]\n",
    "s_val, msp_val, tcp_val, well_classified_val = classify_from_ws(ws_val, labels_val)\n",
    "\n",
    "\n",
    "perfo_direction_w = (((w_train[well_classified_train.logical_not()] - w_train.mean(0)) / w_train.std(0)).mean(0)\n",
    "                       - ((w_train[well_classified_train] - w_train.mean(0)) / w_train.std(0)).mean(0))\n",
    "\n",
    "perfo_direction_s = (((s_train[well_classified_train.logical_not()] - s_train.mean(0)) / s_train.std(0)).mean(0)\n",
    "                       - ((s_train[well_classified_train] - s_train.mean(0)) / s_train.std(0)).mean(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "s_embedded = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(s_train[:n_samples].cpu().numpy())\n",
    "\n",
    "top_k = 10\n",
    "top_k_dims = (-np.abs(perfo_direction_s.cpu())).argsort()[:top_k]\n",
    "s_embedded_topk = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(s_train[:n_samples, top_k_dims].cpu().numpy())\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(10, 10))\n",
    "for i, s_emb in enumerate([s_embedded, s_embedded_topk]):\n",
    "    sc0 = axs[0, i].scatter(s_emb[:, 0], s_emb[:, 1], c=well_classified_train[:n_samples].cpu(), alpha=0.2, cmap='viridis')\n",
    "    sc1 = axs[1, i].scatter(s_emb[:, 0], s_emb[:, 1], c=msp_train[:n_samples].cpu(), alpha=0.2, cmap='viridis')\n",
    "    if G.c_dim == 11:\n",
    "        sc2 = axs[2, i].scatter(s_emb[:, 0], s_emb[:, 1], c=c_classif_train[:n_samples].cpu(), alpha=0.2, cmap='viridis')\n",
    "        fig.colorbar(sc2, ax=axs[2, i], label='MSP input')\n",
    "    fig.colorbar(sc0, ax=axs[0, i], label='well-classified')\n",
    "    fig.colorbar(sc1, ax=axs[1, i], label='MSP')\n",
    "fig.tight_layout()\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "axs[0, 0].set_title('S - All dimensions')\n",
    "axs[0, 1].set_title(f'S - Top {top_k} dimensions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c522a5618233109531e8d8fb7f5c3011399924e76f9423af51315557ed1c4c11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
