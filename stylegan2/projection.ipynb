{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import json\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import wasserstein_distance\n",
    "import numpy as np\n",
    "import functools\n",
    "import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "from stylegan2_ada_pytorch.torch_utils import misc\n",
    "import stylegan2_ada_pytorch.dnnlib\n",
    "import stylegan2_ada_pytorch.legacy\n",
    "from stylegan2_ada_pytorch.projector import project\n",
    "from stylegan2_ada_pytorch.training.dataset import ImageFolderDataset\n",
    "from classifiers.models import CNN_MNIST\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "path_results = Path.cwd().parent / 'results'\n",
    "# path_results = Path('w:/results/stylegan2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_images(images):\n",
    "    assert images.dim() == 4, \"Expected 4D (B x C x H x W) image tensor, got {}D\".format(images.dim())\n",
    "    # lo, hi = [-1, 1] # generator scale\n",
    "    # images = (images - lo) * (255 / (hi - lo)) # classifier scale\n",
    "    # images = torch.round(images.clamp(0, 255))#.to(torch.uint8).to(torch.float)\n",
    "    # images = (images * 127.5 + 128).clamp(0, 255)\n",
    "    images = ((images + 1) / 2).clamp(0, 1)\n",
    "    images = images[:, :, 2:30, 2:30] # remove padding\n",
    "\n",
    "    return images\n",
    "\n",
    "def plot_images(images, title=''):\n",
    "    images = images * 255\n",
    "    images = images.to(torch.uint8)\n",
    "    plt.figure()\n",
    "    plt.imshow(vutils.make_grid(images.cpu(), pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.grid(False)\n",
    "    plt.title(title)\n",
    "\n",
    "\n",
    "def generate_from_z(z):\n",
    "    for i in np.arange(0, z.shape[0], batch_size):\n",
    "        img = G(z[i:i+batch_size], c=None, noise_mode='const', force_fp32=True)\n",
    "        if i == 0: \n",
    "            imgs = img\n",
    "        else:\n",
    "            imgs = torch.cat((imgs, img))\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def plot_random_images(imgs):\n",
    "    # from generate.py: img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "    # imgs = (imgs * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "    imgs = postprocess_images(imgs)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(vutils.make_grid(imgs[torch.randint(0, imgs.shape[0], (100,))].cpu(), pad_value=255, nrow=10).permute(1,2,0))\n",
    "\n",
    "def plot_images_from_s(s):\n",
    "    imgs = generate_img_from_s(s)\n",
    "    imgs = postprocess_images(imgs)\n",
    "    plot_images(imgs)\n",
    "    \n",
    "def truncate(x, x_avg, psi):\n",
    "    # psi=0 means we get average value, \n",
    "    # psi=1 we get original value, \n",
    "    # 0<psi<1 we get interpolation between mean and original\n",
    "    return x_avg.lerp(x, psi)\n",
    "\n",
    "\n",
    "def styleSpace_dict2vec(styleSpace_dict):\n",
    "    styleSpace_vec = []\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            key = f'b{res}.{layer}'\n",
    "            values = styleSpace_dict[key]\n",
    "            if values.dim() == 1: values = values.unsqueeze(0)\n",
    "            styleSpace_vec.append(values)\n",
    "    styleSpace_vec = torch.cat(styleSpace_vec, dim=1)\n",
    "    return styleSpace_vec\n",
    "\n",
    "\n",
    "def styleSpace_vec2dict(styleSpace_vec):\n",
    "    if styleSpace_vec.dim() == 1:\n",
    "        styleSpace_vec = styleSpace_vec.unsqueeze(0)\n",
    "    styleSpace_dict = {}\n",
    "    dim_base = 0\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        block = getattr(G.synthesis, f'b{res}')\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            block_layer = getattr(block, layer)\n",
    "            dim_size = block_layer.affine.weight.shape[1]\n",
    "            key = f'b{res}.{layer}'\n",
    "            styleSpace_dict[key] = styleSpace_vec[:, dim_base:dim_base+dim_size]#.squeeze()\n",
    "            dim_base += dim_size\n",
    "    assert dim_base == styleSpace_vec.shape[1]\n",
    "    return styleSpace_dict\n",
    "\n",
    "\n",
    "def compute_styleSpace_vec_idx2coord():\n",
    "    vec_idx2coord = {}\n",
    "    idx = 0\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        block = getattr(G.synthesis, f'b{res}')\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            block_layer = getattr(block, layer)\n",
    "            dim_size = block_layer.affine.weight.shape[1]\n",
    "            for dim in range(dim_size):\n",
    "                vec_idx2coord[idx] = (f'b{res}.{layer}', dim)\n",
    "                idx += 1\n",
    "    return vec_idx2coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqcat.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqdog.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqwild.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/brecahad.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl'\n",
    "\n",
    "# path_model = path_results / 'stylegan2-training-runs' / '00011-mnist_stylegan2_noise-cond-auto4-original'\n",
    "# path_model = path_results / 'stylegan2-training-runs' / '00015-mnist_stylegan2_blur_noise-cond-auto4'\n",
    "path_model = path_results / 'stylegan2-training-runs' / '00016-mnist_stylegan2_blur_noise_maxSeverity3_proba50-cond-auto4'\n",
    "\n",
    "# find best model in folder\n",
    "if not str(path_model).endswith('pkl'): # local files\n",
    "    with open(path_model / 'metric-fid50k_full.jsonl', 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    best_fid = 1e6\n",
    "    for json_str in json_list:\n",
    "        json_line = json.loads(json_str)\n",
    "        if json_line['results']['fid50k_full'] < best_fid:\n",
    "            best_fid = json_line['results']['fid50k_full']\n",
    "            best_model = json_line['snapshot_pkl']\n",
    "    print('Best FID: {:.2f} ; best model : {}'.format(best_fid, best_model))\n",
    "    path_model = path_model / best_model\n",
    "\n",
    "    with open(path_model, 'rb') as f:\n",
    "        G = pickle.load(f)['G_ema'].to(device)  # torch.nn.Module\n",
    "\n",
    "else: # download pre-trained\n",
    "    with dnnlib.util.open_url(path_model) as f:\n",
    "        G = legacy.load_network_pkl(f)['G_ema'].to(device)\n",
    "\n",
    "if device == 'cpu': G.forward = functools.partial(G.forward, force_fp32=True)\n",
    "\n",
    "conditional = G.c_dim > 0\n",
    "\n",
    "# registor hooks to save intermediate values (images and style space)\n",
    "intermediate_images_torgb = {}\n",
    "def get_torgb(name):\n",
    "    def hook(module, input, output):\n",
    "        intermediate_images_torgb[name] = output.detach()\n",
    "    return hook\n",
    "intermediate_images_block = {}\n",
    "def get_block_img(name):\n",
    "    def hook(module, input, output):\n",
    "        intermediate_images_block[name] = output[1].detach()\n",
    "    return hook\n",
    "styleSpace_values = {}\n",
    "def get_styleSpace_values(name):\n",
    "    def hook(module, input, output):\n",
    "        styleSpace_values[name] = output.detach()\n",
    "    return hook\n",
    "for res in G.synthesis.block_resolutions:\n",
    "    block = getattr(G.synthesis, f'b{res}')\n",
    "    block.torgb.register_forward_hook(get_torgb(res))\n",
    "    block.register_forward_hook(get_block_img(res))\n",
    "    for layer in ['conv0', 'conv1', 'torgb']:\n",
    "        if res == 4 and layer == 'conv0': continue\n",
    "        block_layer = getattr(block, layer)\n",
    "        block_layer.affine.register_forward_hook(get_styleSpace_values(name=f'b{res}.{layer}'))\n",
    "\n",
    "        \n",
    "# backward hooks to get gradients relative to styleSpace\n",
    "styleSpace_grads = {}\n",
    "def get_styleSpace_grads(name):\n",
    "    def hook(self, grad_input, grad_output):\n",
    "        styleSpace_grads[name] = grad_output[0].detach()\n",
    "    return hook\n",
    "\n",
    "for res in G.synthesis.block_resolutions:\n",
    "    block = getattr(G.synthesis, f'b{res}')\n",
    "    block.torgb.register_forward_hook(get_torgb(res))\n",
    "    block.register_forward_hook(get_block_img(res))\n",
    "    for layer in ['conv0', 'conv1', 'torgb']:\n",
    "        if res == 4 and layer == 'conv0': continue\n",
    "        block_layer = getattr(block, layer)\n",
    "        block_layer.affine.register_full_backward_hook(get_styleSpace_grads(name=f'b{res}.{layer}'))\n",
    "\n",
    "        \n",
    "# dict to convert index to coordinate for stylespace vectors\n",
    "styleSpace_vec_idx2coord = compute_styleSpace_vec_idx2coord()\n",
    "\n",
    "\n",
    "# function to move a given style dimension\n",
    "def generate_img_new_style(ws, block_layer_name, index=0, direction=1):\n",
    "    def move_style(index, direction):\n",
    "        def hook(module, input, output):\n",
    "            output[:, index] += direction\n",
    "            return output\n",
    "        return hook\n",
    "\n",
    "    block_name, layer_name = block_layer_name.split('.')\n",
    "    block = getattr(G.synthesis, block_name)\n",
    "    block_layer = getattr(block, layer_name)\n",
    "    handle = block_layer.affine.register_forward_hook(move_style(index, direction))\n",
    "\n",
    "    if ws.dim() == 2:\n",
    "        ws = ws.unsqueeze(1).repeat((1, G.num_ws, 1))\n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    return img\n",
    "    \n",
    "    \n",
    "# function to modify a given style dimension\n",
    "def generate_img_new_style2(ws, block_layer_name, index, s_style_min, s_style_max, s_shift=1, positive_direction=True):\n",
    "    def move_style(index, weight_shift):\n",
    "        def hook(module, input, output):\n",
    "            output[:, index] += weight_shift\n",
    "            return output\n",
    "        return hook\n",
    "    \n",
    "    assert type(index) == int, 'Function only works for 1 style'\n",
    "    assert ws.shape[0] == 1, 'Works only for 1 image' # orig_value only for 1 image\n",
    "    \n",
    "    if ws.dim() == 2:\n",
    "        ws = ws.unsqueeze(1).repeat((1, G.num_ws, 1))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        G.synthesis(ws, noise_mode='const', force_fp32=True) # first pass to get style vector from hook\n",
    "    orig_value = styleSpace_values[block_layer_name][0, index]\n",
    "    target_value = (s_style_max if positive_direction else s_style_min)\n",
    "    weight_shift = s_shift * (target_value - orig_value)\n",
    "\n",
    "    block_name, layer_name = block_layer_name.split('.')\n",
    "    block = getattr(G.synthesis, block_name)\n",
    "    block_layer = getattr(block, layer_name)\n",
    "    handle = block_layer.affine.register_forward_hook(move_style(index, weight_shift))\n",
    "    \n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# function to modify a given style dimension\n",
    "def generate_img_new_style3(ws, block_layer_name, index, s_std, strength=5, positive_direction=True):\n",
    "    def move_style(index, weight_shift):\n",
    "        def hook(module, input, output):\n",
    "            output[:, index] += weight_shift\n",
    "            return output\n",
    "        return hook\n",
    "    \n",
    "    assert type(index) == int, 'Function only works for 1 style'\n",
    "    assert ws.shape[0] == 1, 'Works only for 1 image'\n",
    "    \n",
    "    if ws.dim() == 2:\n",
    "        ws = ws.unsqueeze(1).repeat((1, G.num_ws, 1))\n",
    "    \n",
    "    d = 1 if positive_direction else -1\n",
    "    weight_shift = d * strength * s_std\n",
    "\n",
    "    block_name, layer_name = block_layer_name.split('.')\n",
    "    block = getattr(G.synthesis, block_name)\n",
    "    block_layer = getattr(block, layer_name)\n",
    "    handle = block_layer.affine.register_forward_hook(move_style(index, weight_shift))\n",
    "    \n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# function to generate image from S\n",
    "def generate_img_from_s(s):\n",
    "    def set_style(values):\n",
    "        def hook(module, input, output):\n",
    "            output = values\n",
    "            return output\n",
    "        return hook\n",
    "    \n",
    "    if type(s) != dict: s = styleSpace_vec2dict(s)\n",
    "    assert s['b4.conv1'].dim() == 2, 'Should be of 2 dimensions: batch_size x s_dim'\n",
    "    batch_size = s['b4.conv1'].shape[0]\n",
    "    \n",
    "    handles = []\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        block = getattr(G.synthesis, f'b{res}')\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            block_layer = getattr(block, layer)\n",
    "            values = s[f'b{res}.{layer}']\n",
    "            handles.append(block_layer.affine.register_forward_hook(set_style(values)))\n",
    "    \n",
    "    dummy_ws = torch.zeros((batch_size, G.num_ws, G.w_dim), device=device)\n",
    "    img = G.synthesis(dummy_ws, noise_mode='const', force_fp32=True)\n",
    "\n",
    "    for h in handles: h.remove()\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 5\n",
    "z = torch.randn([n_images, G.z_dim], device=device)    # latent codes\n",
    "if conditional:\n",
    "    digits = torch.randint(0, G.c_dim, (n_images,), device=device)\n",
    "    c = F.one_hot(digits, G.c_dim)          \n",
    "else:\n",
    "    c = None\n",
    "misc.print_module_summary(G, [z, c])\n",
    "\n",
    "ws = G.mapping(z, c, truncation_psi=1)\n",
    "img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "img = postprocess_images(img)\n",
    "plot_images(img, title='original')\n",
    "\n",
    "\n",
    "img_ = generate_img_new_style(ws, block_layer_name='b4.conv1', index=3, direction=-1)\n",
    "img_ = postprocess_images(img_)\n",
    "plot_images(img_, title='with new style')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict digits\n",
    "classifier_digits = CNN_MNIST(output_dim=10).to(device)\n",
    "# classifier_digits.load_state_dict(torch.load(path_results / 'classifiers' / 'CNN_MNIST_weights_20220411_0826.pth', map_location=device)) # Confiance\n",
    "# classifier_digits.load_state_dict(torch.load(path_results / 'classifiers' / 'CNN_MNIST_weights_20220210_1601.pth', map_location=device))\n",
    "classifier_digits.load_state_dict(torch.load(path_results / 'classifiers' / 'CNN_mnist_stylegan2_blur_noise_maxSeverity3_proba50_20220510_1124.pth', map_location=device))\n",
    "classifier_digits.eval()\n",
    "\n",
    "# predict noise\n",
    "classifier_noise = CNN_MNIST(output_dim=6).to(device)\n",
    "# classifier_noise.load_state_dict(torch.load(path_results / 'classifiers' / 'CNN_noise_MNIST_weights_20220411_0841.pth', map_location=device)) # Confiance\n",
    "classifier_noise.load_state_dict(torch.load(path_results / 'classifiers' / 'CNN_MNIST_noise_weights_20220210_1728.pth', map_location=device))\n",
    "classifier_noise.eval()\n",
    "\n",
    "imgs = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "imgs = postprocess_images(imgs)\n",
    "digit_pred = classifier_digits(imgs).argmax(dim=1).cpu()\n",
    "noise_pred = classifier_noise(imgs).argmax(dim=1).cpu()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(min(n_images, 5)):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(imgs[i].cpu().squeeze(), cmap='gray')\n",
    "    plt.title(f'digit: {digit_pred[i].numpy()} \\n noise: {noise_pred[i].numpy()}')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data\n",
    "# transforms = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     torchvision.transforms.Resize(32),\n",
    "#     torchvision.transforms.Lambda(lambda x: 255*x)\n",
    "# ])\n",
    "# mnist_test = torchvision.datasets.MNIST(root='../data', train=False, transform=transforms)\n",
    "#\n",
    "# idx = 3\n",
    "# img = mnist_test[idx][0]\n",
    "# digit = mnist_test[idx][1]\n",
    "\n",
    "\n",
    "dataset = 'mnistTest_stylegan2_blur_noise_maxSeverity3_proba50'\n",
    "# dataset = 'mnistTest_stylegan2'\n",
    "\n",
    "# original data\n",
    "path_data = Path.cwd().parent / 'data/MNIST' / f'{dataset}.zip'\n",
    "ds_original = ImageFolderDataset(path_data, use_labels=True)\n",
    "\n",
    "idx = 1\n",
    "img = ds_original[idx][0]\n",
    "digit = ds_original[idx][1].argmax()\n",
    "\n",
    "img = torch.tensor(img, device=device)\n",
    "digits_ = torch.tensor(digit, device=device)\n",
    "c = F.one_hot(digits_, G.c_dim)   \n",
    "\n",
    "# project\n",
    "projected_w_steps = project(G, img, c, classifier_digits=classifier_digits, regularize_classif_weight=0, device=device, num_steps=1000)\n",
    "# projected_w_steps = project(G, img, c, device=device, num_steps=3000)\n",
    "projected_w = projected_w_steps[-1][0].unsqueeze(0)\n",
    "\n",
    "# save video\n",
    "outdir = 'proj_out'\n",
    "\n",
    "target_pil = torchvision.transforms.ToPILImage()(img.repeat((3, 1, 1))/255)\n",
    "target_uint8 = np.array(target_pil, dtype=np.uint8)\n",
    "\n",
    "# # Render debug output: optional video and projected image and W vector.\n",
    "# os.makedirs(outdir, exist_ok=True)\n",
    "# video = imageio.get_writer(f'{outdir}/proj.mp4', mode='I', fps=10, codec='libx264', bitrate='16M')\n",
    "# print (f'Saving optimization progress video \"{outdir}/proj.mp4\"')\n",
    "# for projected_w in projected_w_steps:\n",
    "#     synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n",
    "#     synth_image = synth_image.repeat((1, 3, 1, 1))\n",
    "#     synth_image = (synth_image + 1) * (255/2)\n",
    "#     synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "#     video.append_data(np.concatenate([target_uint8, synth_image], axis=1))\n",
    "# video.close()\n",
    "\n",
    "# Save final projected frame and W vector.\n",
    "target_pil.save(f'{outdir}/target.png')\n",
    "projected_w = projected_w_steps[-1]\n",
    "synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n",
    "synth_image = synth_image.repeat((1, 3, 1, 1))\n",
    "synth_image = (synth_image + 1) * (255/2)\n",
    "synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "PIL.Image.fromarray(synth_image, 'RGB').save(f'{outdir}/proj.png')\n",
    "np.savez(f'{outdir}/projected_w.npz', w=projected_w.unsqueeze(0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_target = (img.unsqueeze(0)).to(device)\n",
    "x_target = x_target[:, :, 2:30, 2:30] / 255\n",
    "print(classifier_digits(x_target))\n",
    "\n",
    "x_proj = torch.from_numpy(synth_image[:, :, 0]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "x_proj = x_proj[:, :, 2:30, 2:30] / 255\n",
    "print(classifier_digits(x_proj))\n",
    "\n",
    "dist = (classifier_digits(x_target) - classifier_digits(x_proj)).square().sum()\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read projected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mnistTest_stylegan2_blur_noise_maxSeverity3_proba50'\n",
    "# dataset = 'mnistTest_stylegan2'\n",
    "# options = ''\n",
    "options = '_classifWeight0.001'\n",
    "\n",
    "# original data\n",
    "path_data = Path.cwd().parent / 'data/MNIST' / f'{dataset}.zip'\n",
    "ds_original = ImageFolderDataset(path_data, use_labels=True)\n",
    "\n",
    "# projected latent codes\n",
    "file = path_results / 'projected_data' / f'{dataset}_model00016{options}_all.npz'\n",
    "projected_data = np.load(file)\n",
    "projected_w = torch.from_numpy(projected_data['projected_w']).to(device)\n",
    "digits = torch.from_numpy(projected_data['digits']).to(device)\n",
    "\n",
    "assert all(projected_data['indices'] == np.arange(len(ds_original)))\n",
    "assert all(projected_data['digits'] == [ds_original[i][1].argmax() for i in range(len(ds_original))])\n",
    "\n",
    "\n",
    "\n",
    "# PREDICT CLASSES OF PROJECTED DATA\n",
    "\n",
    "s_proj_all = None\n",
    "class_predicted = None\n",
    "batch_size = 64\n",
    "for w, labels in zip(DataLoader(projected_w, batch_size=batch_size), DataLoader(digits, batch_size=batch_size)):\n",
    "    batch_size_t = len(labels)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # compute output\n",
    "        imgs = G.synthesis(w.unsqueeze(1).repeat((1, G.num_ws, 1)), noise_mode='const', force_fp32=True)\n",
    "        imgs = postprocess_images(imgs)\n",
    "\n",
    "        digits_pred = classifier_digits(imgs)\n",
    "        class_pred_t = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "        class_softmax = F.softmax(digits_pred, dim=1)[torch.arange(batch_size_t), labels]\n",
    "        \n",
    "    # style values (styleSpace_values from hook)\n",
    "    s_vec = styleSpace_dict2vec(styleSpace_values)\n",
    "\n",
    "    s_proj_all = s_vec if s_proj_all is None else torch.cat((s_proj_all, s_vec))\n",
    "    class_predicted = class_pred_t if class_predicted is None else torch.cat((class_predicted, class_pred_t))\n",
    "\n",
    "s_proj_wellclassified = s_proj_all[class_predicted == digits]\n",
    "s_proj_misclassified = s_proj_all[class_predicted != digits]\n",
    "\n",
    "\n",
    "# PREDICT CLASSES OF ORIGINAL DATA AND COMPARE WITH PROJECTED DATA\n",
    "\n",
    "# path_data = Path.cwd().parent / 'data/MNIST' / f'{dataset}.zip'\n",
    "# ds = ImageFolderDataset(path_data, use_labels=True)\n",
    "\n",
    "class_predicted_mnist = None\n",
    "for x, y in DataLoader(ds_original, batch_size=64):\n",
    "    x = (x / 255)[:, :, 2:30, 2:30]\n",
    "    digits_pred = classifier_digits(x.to(device))\n",
    "    class_pred_t = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "    class_predicted_mnist = class_pred_t if class_predicted_mnist is None else torch.cat((class_predicted_mnist, class_pred_t))\n",
    "\n",
    "tp_idx = class_predicted[class_predicted == digits] == class_predicted_mnist[class_predicted == digits]\n",
    "tp = tp_idx.sum()\n",
    "tn_idx = class_predicted[class_predicted != digits] == class_predicted_mnist[class_predicted != digits]\n",
    "tn = tn_idx.sum()\n",
    "fp_idx = class_predicted[class_predicted == digits] != class_predicted_mnist[class_predicted == digits]\n",
    "fp = fp_idx.sum()\n",
    "fn_idx = class_predicted[class_predicted != digits] != class_predicted_mnist[class_predicted != digits]\n",
    "fn = fn_idx.sum()\n",
    "\n",
    "assert tp + tn + fp + fn == len(class_predicted)\n",
    "\n",
    "print('True positive:', tp.item())\n",
    "print('True negative:', tn.item())\n",
    "print('False positive:', fp.item())\n",
    "print('False negative:', fn.item())\n",
    "\n",
    "\n",
    "# FILTER OUT WRONG SAMPLES (FALSE POSITIVE OR NEGATIVE)\n",
    "s_proj_wellclassified_fp = s_proj_wellclassified[fp_idx]\n",
    "s_proj_misclassified_fn = s_proj_misclassified[fn_idx]\n",
    "s_proj_wellclassified = s_proj_wellclassified[tp_idx]\n",
    "s_proj_misclassified = s_proj_misclassified[tn_idx]\n",
    "\n",
    "\n",
    "# PLOT SAMPLES\n",
    "n_images = 8\n",
    "plot_images_from_s(s_proj_wellclassified[:n_images])\n",
    "plt.title('well-classified (correct)')\n",
    "\n",
    "plot_images_from_s(s_proj_wellclassified_fp[:n_images])\n",
    "plt.title('well-classified (wrong)')\n",
    "\n",
    "plot_images_from_s(s_proj_misclassified[:n_images])\n",
    "plt.title('misclassified (correct)')\n",
    "\n",
    "plot_images_from_s(s_proj_misclassified_fn[:n_images])\n",
    "plt.title('misclassified (wrong)')\n",
    "\n",
    "\n",
    "# mnistTest_stylegan2_blur_noise_maxSeverity3_proba50\n",
    "# True positive: 9509\n",
    "# True negative: 158\n",
    "# False positive: 87\n",
    "# False negative: 246\n",
    "\n",
    "# mnistTest_stylegan2_blur_noise_maxSeverity3_proba50_classifWeight0.001\n",
    "# True positive: 9628\n",
    "# True negative: 209\n",
    "# False positive: 47\n",
    "# False negative: 116\n",
    "\n",
    "# mnistTest_stylegan2\n",
    "# True positive: 9779\n",
    "# True negative: 120\n",
    "# False positive: 49\n",
    "# False negative: 52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 5\n",
    "\n",
    "z = torch.randn([projected_w.shape[0], G.z_dim], device=device)    # latent codes\n",
    "if conditional:\n",
    "    digits = torch.randint(0, G.c_dim, (projected_w.shape[0],), device=device)\n",
    "    c = F.one_hot(digits, G.c_dim)\n",
    "else:\n",
    "    c = None\n",
    "\n",
    "ws = G.mapping(z, c, truncation_psi=1)\n",
    "natural_w = ws[:, 0, :].cpu().numpy()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(projected_w.cpu().numpy()[:, dim], alpha=0.6, label='projected')\n",
    "plt.hist(natural_w[:, dim], alpha=0.6, label='natural')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wasserstein_distance(projected_w.cpu().numpy()[:, dim], natural_w[:, dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 510\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(ds_original[i][0][0], cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "\n",
    "plot_images_from_s(s_proj_all[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('alc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf0e85ac5c104e7dc18f35eb0f3f17acfc5d19b9cd0ff3830c651a628ffe0833"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
