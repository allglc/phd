{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import json\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import functools\n",
    "import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "from stylegan2_ada_pytorch.torch_utils import misc\n",
    "import stylegan2_ada_pytorch.dnnlib\n",
    "import stylegan2_ada_pytorch.legacy\n",
    "from stylegan2_ada_pytorch.projector import project\n",
    "from stylegan2_ada_pytorch.training.dataset import ImageFolderDataset\n",
    "from classifiers.models import CNN_MNIST\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "path_results = Path.cwd().parent / 'results'\n",
    "# path_results = Path('w:/results/stylegan2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_images(images):\n",
    "    assert images.dim() == 4, \"Expected 4D (B x C x H x W) image tensor, got {}D\".format(images.dim())\n",
    "    # lo, hi = [-1, 1] # generator scale\n",
    "    # images = (images - lo) * (255 / (hi - lo)) # classifier scale\n",
    "    # images = torch.round(images.clamp(0, 255))#.to(torch.uint8).to(torch.float)\n",
    "    # images = (images * 127.5 + 128).clamp(0, 255)\n",
    "    images = ((images + 1) / 2).clamp(0, 1)\n",
    "    images = images[:, :, 2:30, 2:30] # remove padding\n",
    "\n",
    "    return images\n",
    "\n",
    "def plot_images(images, title=''):\n",
    "    images = images * 255\n",
    "    images = images.to(torch.uint8)\n",
    "    plt.figure()\n",
    "    plt.imshow(vutils.make_grid(images.cpu(), pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.grid(False)\n",
    "    plt.title(title)\n",
    "\n",
    "\n",
    "def generate_from_z(z):\n",
    "    for i in np.arange(0, z.shape[0], batch_size):\n",
    "        img = G(z[i:i+batch_size], c=None, noise_mode='const', force_fp32=True)\n",
    "        if i == 0: \n",
    "            imgs = img\n",
    "        else:\n",
    "            imgs = torch.cat((imgs, img))\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def plot_random_images(imgs):\n",
    "    # from generate.py: img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "    # imgs = (imgs * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "    imgs = postprocess_images(imgs)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(vutils.make_grid(imgs[torch.randint(0, imgs.shape[0], (100,))].cpu(), pad_value=255, nrow=10).permute(1,2,0))\n",
    "\n",
    "def plot_images_from_s(s):\n",
    "    imgs = generate_img_from_s(s)\n",
    "    imgs = postprocess_images(imgs)\n",
    "    plot_images(imgs)\n",
    "    \n",
    "def truncate(x, x_avg, psi):\n",
    "    # psi=0 means we get average value, \n",
    "    # psi=1 we get original value, \n",
    "    # 0<psi<1 we get interpolation between mean and original\n",
    "    return x_avg.lerp(x, psi)\n",
    "\n",
    "\n",
    "def styleSpace_dict2vec(styleSpace_dict):\n",
    "    styleSpace_vec = []\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            key = f'b{res}.{layer}'\n",
    "            values = styleSpace_dict[key]\n",
    "            if values.dim() == 1: values = values.unsqueeze(0)\n",
    "            styleSpace_vec.append(values)\n",
    "    styleSpace_vec = torch.cat(styleSpace_vec, dim=1)\n",
    "    return styleSpace_vec\n",
    "\n",
    "\n",
    "def styleSpace_vec2dict(styleSpace_vec):\n",
    "    if styleSpace_vec.dim() == 1:\n",
    "        styleSpace_vec = styleSpace_vec.unsqueeze(0)\n",
    "    styleSpace_dict = {}\n",
    "    dim_base = 0\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        block = getattr(G.synthesis, f'b{res}')\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            block_layer = getattr(block, layer)\n",
    "            dim_size = block_layer.affine.weight.shape[1]\n",
    "            key = f'b{res}.{layer}'\n",
    "            styleSpace_dict[key] = styleSpace_vec[:, dim_base:dim_base+dim_size]#.squeeze()\n",
    "            dim_base += dim_size\n",
    "    assert dim_base == styleSpace_vec.shape[1]\n",
    "    return styleSpace_dict\n",
    "\n",
    "\n",
    "def compute_styleSpace_vec_idx2coord():\n",
    "    vec_idx2coord = {}\n",
    "    idx = 0\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        block = getattr(G.synthesis, f'b{res}')\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            block_layer = getattr(block, layer)\n",
    "            dim_size = block_layer.affine.weight.shape[1]\n",
    "            for dim in range(dim_size):\n",
    "                vec_idx2coord[idx] = (f'b{res}.{layer}', dim)\n",
    "                idx += 1\n",
    "    return vec_idx2coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqcat.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqdog.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqwild.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/brecahad.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl'\n",
    "\n",
    "# path_model = path_results / 'stylegan2-training-runs' / '00011-mnist_stylegan2_noise-cond-auto4-original'\n",
    "# path_model = path_results / 'stylegan2-training-runs' / '00015-mnist_stylegan2_blur_noise-cond-auto4'\n",
    "path_model = path_results / 'stylegan2-training-runs' / '00016-mnist_stylegan2_blur_noise_maxSeverity3_proba50-cond-auto4'\n",
    "\n",
    "# find best model in folder\n",
    "if not str(path_model).endswith('pkl'):\n",
    "    with open(path_model / 'metric-fid50k_full.jsonl', 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    best_fid = 1e6\n",
    "    for json_str in json_list:\n",
    "        json_line = json.loads(json_str)\n",
    "        if json_line['results']['fid50k_full'] < best_fid:\n",
    "            best_fid = json_line['results']['fid50k_full']\n",
    "            best_model = json_line['snapshot_pkl']\n",
    "    print('Best FID: {:.2f} ; best model : {}'.format(best_fid, best_model))\n",
    "    path_model = path_model / best_model\n",
    "\n",
    "    with open(path_model, 'rb') as f:\n",
    "        G = pickle.load(f)['G_ema'].to(device)  # torch.nn.Module\n",
    "\n",
    "else:\n",
    "    with dnnlib.util.open_url(path_model) as f:\n",
    "        G = legacy.load_network_pkl(f)['G_ema'].to(device)\n",
    "\n",
    "if device == 'cpu': G.forward = functools.partial(G.forward, force_fp32=True)\n",
    "\n",
    "conditional = G.c_dim > 0\n",
    "\n",
    "# registor hooks to save intermediate values (images and style space)\n",
    "intermediate_images_torgb = {}\n",
    "def get_torgb(name):\n",
    "    def hook(module, input, output):\n",
    "        intermediate_images_torgb[name] = output.detach()\n",
    "    return hook\n",
    "intermediate_images_block = {}\n",
    "def get_block_img(name):\n",
    "    def hook(module, input, output):\n",
    "        intermediate_images_block[name] = output[1].detach()\n",
    "    return hook\n",
    "styleSpace_values = {}\n",
    "def get_styleSpace_values(name):\n",
    "    def hook(module, input, output):\n",
    "        styleSpace_values[name] = output.detach()\n",
    "    return hook\n",
    "for res in G.synthesis.block_resolutions:\n",
    "    block = getattr(G.synthesis, f'b{res}')\n",
    "    block.torgb.register_forward_hook(get_torgb(res))\n",
    "    block.register_forward_hook(get_block_img(res))\n",
    "    for layer in ['conv0', 'conv1', 'torgb']:\n",
    "        if res == 4 and layer == 'conv0': continue\n",
    "        block_layer = getattr(block, layer)\n",
    "        block_layer.affine.register_forward_hook(get_styleSpace_values(name=f'b{res}.{layer}'))\n",
    "\n",
    "        \n",
    "# backward hooks to get gradients relative to styleSpace\n",
    "styleSpace_grads = {}\n",
    "def get_styleSpace_grads(name):\n",
    "    def hook(self, grad_input, grad_output):\n",
    "        styleSpace_grads[name] = grad_output[0].detach()\n",
    "    return hook\n",
    "\n",
    "for res in G.synthesis.block_resolutions:\n",
    "    block = getattr(G.synthesis, f'b{res}')\n",
    "    block.torgb.register_forward_hook(get_torgb(res))\n",
    "    block.register_forward_hook(get_block_img(res))\n",
    "    for layer in ['conv0', 'conv1', 'torgb']:\n",
    "        if res == 4 and layer == 'conv0': continue\n",
    "        block_layer = getattr(block, layer)\n",
    "        block_layer.affine.register_full_backward_hook(get_styleSpace_grads(name=f'b{res}.{layer}'))\n",
    "\n",
    "        \n",
    "# dict to convert index to coordinate for stylespace vectors\n",
    "styleSpace_vec_idx2coord = compute_styleSpace_vec_idx2coord()\n",
    "\n",
    "\n",
    "# function to move a given style dimension\n",
    "def generate_img_new_style(ws, block_layer_name, index=0, direction=1):\n",
    "    def move_style(index, direction):\n",
    "        def hook(module, input, output):\n",
    "            output[:, index] += direction\n",
    "            return output\n",
    "        return hook\n",
    "\n",
    "    block_name, layer_name = block_layer_name.split('.')\n",
    "    block = getattr(G.synthesis, block_name)\n",
    "    block_layer = getattr(block, layer_name)\n",
    "    handle = block_layer.affine.register_forward_hook(move_style(index, direction))\n",
    "\n",
    "    if ws.dim() == 2:\n",
    "        ws = ws.unsqueeze(1).repeat((1, G.num_ws, 1))\n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    return img\n",
    "    \n",
    "    \n",
    "# function to modify a given style dimension\n",
    "def generate_img_new_style2(ws, block_layer_name, index, s_style_min, s_style_max, s_shift=1, positive_direction=True):\n",
    "    def move_style(index, weight_shift):\n",
    "        def hook(module, input, output):\n",
    "            output[:, index] += weight_shift\n",
    "            return output\n",
    "        return hook\n",
    "    \n",
    "    assert type(index) == int, 'Function only works for 1 style'\n",
    "    assert ws.shape[0] == 1, 'Works only for 1 image' # orig_value only for 1 image\n",
    "    \n",
    "    if ws.dim() == 2:\n",
    "        ws = ws.unsqueeze(1).repeat((1, G.num_ws, 1))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        G.synthesis(ws, noise_mode='const', force_fp32=True) # first pass to get style vector from hook\n",
    "    orig_value = styleSpace_values[block_layer_name][0, index]\n",
    "    target_value = (s_style_max if positive_direction else s_style_min)\n",
    "    weight_shift = s_shift * (target_value - orig_value)\n",
    "\n",
    "    block_name, layer_name = block_layer_name.split('.')\n",
    "    block = getattr(G.synthesis, block_name)\n",
    "    block_layer = getattr(block, layer_name)\n",
    "    handle = block_layer.affine.register_forward_hook(move_style(index, weight_shift))\n",
    "    \n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# function to modify a given style dimension\n",
    "def generate_img_new_style3(ws, block_layer_name, index, s_std, strength=5, positive_direction=True):\n",
    "    def move_style(index, weight_shift):\n",
    "        def hook(module, input, output):\n",
    "            output[:, index] += weight_shift\n",
    "            return output\n",
    "        return hook\n",
    "    \n",
    "    assert type(index) == int, 'Function only works for 1 style'\n",
    "    assert ws.shape[0] == 1, 'Works only for 1 image'\n",
    "    \n",
    "    if ws.dim() == 2:\n",
    "        ws = ws.unsqueeze(1).repeat((1, G.num_ws, 1))\n",
    "    \n",
    "    d = 1 if positive_direction else -1\n",
    "    weight_shift = d * strength * s_std\n",
    "\n",
    "    block_name, layer_name = block_layer_name.split('.')\n",
    "    block = getattr(G.synthesis, block_name)\n",
    "    block_layer = getattr(block, layer_name)\n",
    "    handle = block_layer.affine.register_forward_hook(move_style(index, weight_shift))\n",
    "    \n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# function to generate image from S\n",
    "def generate_img_from_s(s):\n",
    "    def set_style(values):\n",
    "        def hook(module, input, output):\n",
    "            output = values\n",
    "            return output\n",
    "        return hook\n",
    "    \n",
    "    if type(s) != dict: s = styleSpace_vec2dict(s)\n",
    "    assert s['b4.conv1'].dim() == 2, 'Should be of 2 dimensions: batch_size x s_dim'\n",
    "    batch_size = s['b4.conv1'].shape[0]\n",
    "    \n",
    "    handles = []\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        block = getattr(G.synthesis, f'b{res}')\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            block_layer = getattr(block, layer)\n",
    "            values = s[f'b{res}.{layer}']\n",
    "            handles.append(block_layer.affine.register_forward_hook(set_style(values)))\n",
    "    \n",
    "    dummy_ws = torch.zeros((batch_size, G.num_ws, G.w_dim), device=device)\n",
    "    img = G.synthesis(dummy_ws, noise_mode='const', force_fp32=True)\n",
    "\n",
    "    for h in handles: h.remove()\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 5\n",
    "z = torch.randn([n_images, G.z_dim], device=device)    # latent codes\n",
    "if conditional:\n",
    "    digits = torch.randint(0, G.c_dim, (n_images,), device=device)\n",
    "    c = F.one_hot(digits, G.c_dim)          \n",
    "else:\n",
    "    c = None\n",
    "misc.print_module_summary(G, [z, c])\n",
    "\n",
    "ws = G.mapping(z, c, truncation_psi=1)\n",
    "img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "img = postprocess_images(img)\n",
    "plot_images(img, title='original')\n",
    "\n",
    "\n",
    "img_ = generate_img_new_style(ws, block_layer_name='b4.conv1', index=3, direction=-1)\n",
    "img_ = postprocess_images(img_)\n",
    "plot_images(img_, title='with new style')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict digits\n",
    "classifier_digits = CNN_MNIST(output_dim=10).to(device)\n",
    "# classifier_digits.load_state_dict(torch.load(path_results / 'classifiers' / 'CNN_MNIST_weights_20220411_0826.pth', map_location=device)) # Confiance\n",
    "# classifier_digits.load_state_dict(torch.load(path_results / 'classifiers' / 'CNN_MNIST_weights_20220210_1601.pth', map_location=device))\n",
    "classifier_digits.load_state_dict(torch.load(path_results / 'classifiers' / 'CNN_mnist_stylegan2_blur_noise_maxSeverity3_proba50_20220510_1124.pth', map_location=device))\n",
    "classifier_digits.eval()\n",
    "\n",
    "# predict noise\n",
    "classifier_noise = CNN_MNIST(output_dim=6).to(device)\n",
    "# classifier_noise.load_state_dict(torch.load(path_results / 'classifiers' / 'CNN_noise_MNIST_weights_20220411_0841.pth', map_location=device)) # Confiance\n",
    "classifier_noise.load_state_dict(torch.load(path_results / 'classifiers' / 'CNN_MNIST_noise_weights_20220210_1728.pth', map_location=device))\n",
    "classifier_noise.eval()\n",
    "\n",
    "imgs = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "imgs = postprocess_images(imgs)\n",
    "digit_pred = classifier_digits(imgs).argmax(dim=1).cpu()\n",
    "noise_pred = classifier_noise(imgs).argmax(dim=1).cpu()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(min(n_images, 5)):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(imgs[i].cpu().squeeze(), cmap='gray')\n",
    "    plt.title(f'digit: {digit_pred[i].numpy()} \\n noise: {noise_pred[i].numpy()}')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample images in latent space to get a set of latent codes\n",
    "n_images = 1\n",
    "z = torch.randn([n_images, G.z_dim], device=device)    # latent codes\n",
    "if conditional:\n",
    "    digits = torch.randint(0, G.c_dim, (n_images,), device=device)\n",
    "    c = F.one_hot(digits, G.c_dim)          \n",
    "else:\n",
    "    c = None\n",
    "ws = G.mapping(z, c, truncation_psi=1)\n",
    "\n",
    "imgs_orig = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "imgs_orig = postprocess_images(imgs_orig)\n",
    "with torch.no_grad():\n",
    "    digits_pred_orig = classifier_digits(imgs_orig).cpu()\n",
    "    confidence_orig = digits_pred_orig.max(dim=1).values\n",
    "    maxLogit_orig = digits_pred_orig.max(axis=1).values\n",
    "    maxSoftmax_orig = F.softmax(digits_pred_orig, dim=1).max(axis=1).values\n",
    "\n",
    "plot_images(imgs_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## AttFind (slow!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_change = pd.Series(name='confidence_change', dtype=np.float32)\n",
    "for block_layer, v in styleSpace_values.items():\n",
    "    print(block_layer)\n",
    "    for dimension in range(v.shape[1]):\n",
    "        # for direction in [-1, 1]:\n",
    "        direction = 1\n",
    "        imgs_newStyle = generate_img_new_style(ws, block_layer, index=dimension, direction=direction)\n",
    "        imgs_newStyle = postprocess_images(imgs_newStyle)\n",
    "        with torch.no_grad():\n",
    "            digits_pred_newStyle = classifier_digits(imgs_newStyle).cpu()\n",
    "            confidence_newStyle = digits_pred_newStyle.max(axis=1).values\n",
    "\n",
    "        confidence_change[f'{block_layer}_{dimension}_{direction}'] = (confidence_newStyle - confidence_orig).mean()\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_change_sorted = confidence_change[confidence_change.abs().sort_values(ascending=False).index]\n",
    "\n",
    "top_k = 5\n",
    "titles_plot = ['original']\n",
    "imgs_plot = [imgs_orig]\n",
    "maxLogit = [maxLogit_orig]\n",
    "for k, v in confidence_change_sorted[:top_k].items():\n",
    "    # print(f'{k}: {v}')\n",
    "    block_layer, dimension, direction = k.split('_')\n",
    "    imgs_newStyle = generate_img_new_style(ws, block_layer, index=int(dimension), direction=-1*int(direction))\n",
    "    imgs_newStyle = postprocess_images(imgs_newStyle)\n",
    "    titles_plot.append('{}: $\\Delta$={:.2f}'.format(k, v))\n",
    "    imgs_plot.append(imgs_newStyle)\n",
    "    with torch.no_grad():\n",
    "        digits_pred_newStyle = classifier_digits(imgs_newStyle).cpu()\n",
    "        # maxSoftmax_newStyle = F.softmax(digits_pred_newStyle, dim=1).max(axis=1).values\n",
    "        maxLogit_newStyle = digits_pred_newStyle.max(axis=1).values\n",
    "    maxLogit.append(maxLogit_newStyle)\n",
    "\n",
    "fig, axs = plt.subplots(n_images, top_k+1, figsize=(20, 5))\n",
    "for k, imgs in enumerate(imgs_plot):\n",
    "    for i in range(n_images):\n",
    "        ax = axs[i, k] if n_images > 1 else axs[k]\n",
    "        ax.imshow(imgs[i].cpu().squeeze(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.grid(False)\n",
    "        ax.set_title(titles_plot[k] + '\\n' + '{:.2f}'.format(maxLogit[k].numpy().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Gradient of score wrt W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 1\n",
    "z = torch.randn([n_images, G.z_dim], device=device)    # latent codes\n",
    "if conditional:\n",
    "    digits = torch.randint(0, G.c_dim, (n_images,), device=device)\n",
    "    c = F.one_hot(digits, G.c_dim)          \n",
    "else:\n",
    "    c = None\n",
    "\n",
    "# compute input\n",
    "w = G.mapping(z, c, truncation_psi=1, truncation_cutoff=8)[:, 0, :] # keep only the first element\n",
    "w.requires_grad = True\n",
    "\n",
    "# compute output\n",
    "imgs = G.synthesis(w.unsqueeze(1).repeat((1, G.num_ws, 1)), noise_mode='const', force_fp32=True)\n",
    "imgs = postprocess_images(imgs)\n",
    "\n",
    "digits_pred = classifier_digits(imgs)\n",
    "max_logit = digits_pred.max(axis=1).values.mean()\n",
    "max_softmax = F.softmax(digits_pred, dim=1).max(axis=1).values.mean()\n",
    "class_pred = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "\n",
    "# grad of output relative to input\n",
    "grad = torch.autograd.grad(max_logit, w)[0]\n",
    "grad = grad.mean(axis=0)\n",
    "\n",
    "plot_images(imgs)\n",
    "plt.title('top class: {} : {:.0f}%'.format(class_pred.cpu().numpy(), 100*max_softmax.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "top_k_idxs = grad.abs().topk(top_k).indices\n",
    "mask = torch.zeros_like(grad).index_fill(dim=0, index=top_k_idxs, value=1)\n",
    "\n",
    "w_new = w - 0.05*mask*grad\n",
    "\n",
    "imgs = G.synthesis(w_new.unsqueeze(1).repeat((1, G.num_ws, 1)), noise_mode='const', force_fp32=True)\n",
    "imgs = postprocess_images(imgs)\n",
    "\n",
    "digits_pred = classifier_digits(imgs)\n",
    "# max_logit = digits_pred.max(axis=1).values\n",
    "max_softmax = F.softmax(digits_pred, dim=1).max(axis=1).values.mean()\n",
    "class_pred = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "\n",
    "plot_images(imgs)\n",
    "plt.title('top class: {} : {:.0f}%'.format(class_pred.cpu().numpy(), 100*max_softmax.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gradient of score wrt style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 1\n",
    "z = torch.randn([n_images, G.z_dim], device=device)    # latent codes\n",
    "if conditional:\n",
    "    digits = torch.randint(0, G.c_dim, (n_images,), device=device)\n",
    "    c = F.one_hot(digits, G.c_dim)\n",
    "    class_generated = c.argmax(axis=1)\n",
    "else:\n",
    "    c = None\n",
    "\n",
    "# compute input\n",
    "w = G.mapping(z, c, truncation_psi=1, truncation_cutoff=8)[:, 0, :] # keep only the first element\n",
    "w.requires_grad = True\n",
    "\n",
    "# compute output\n",
    "imgs_orig = G.synthesis(w.unsqueeze(1).repeat((1, G.num_ws, 1)), noise_mode='const', force_fp32=True)\n",
    "imgs_orig = postprocess_images(imgs_orig)\n",
    "\n",
    "digits_pred_orig = classifier_digits(imgs_orig)\n",
    "# max_logit = digits_pred.max(axis=1).values\n",
    "# max_softmax = F.softmax(digits_pred, dim=1).max(axis=1).values.mean()\n",
    "# class_pred = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "class_logit_orig = digits_pred_orig[torch.arange(n_images), class_generated]\n",
    "class_softmax_orig = F.softmax(digits_pred_orig, dim=1)[torch.arange(n_images), class_generated]\n",
    "\n",
    "# backpropagate gradient\n",
    "loss = class_logit_orig.mean() # goal is to reduce the score for the class\n",
    "loss.backward() # compute gradients and access them thanks to the backward hook\n",
    "styleSpace_grads_vec = styleSpace_dict2vec(styleSpace_grads)\n",
    "\n",
    "plot_images(imgs_orig)\n",
    "plt.title('Class generated: {}; predicted at {:.0f}%'.format(class_generated.squeeze().cpu().numpy(), 100*class_softmax_orig.squeeze().detach().cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "top_k_idxs = styleSpace_grads_vec.mean(axis=0).abs().topk(top_k).indices.cpu().numpy()\n",
    "top_k_vals = styleSpace_grads_vec.mean(axis=0)[top_k_idxs]\n",
    "\n",
    "styleSpace_vec_idx2coord = compute_styleSpace_vec_idx2coord()\n",
    "\n",
    "imgs_plot = [imgs_orig]\n",
    "class_softmax_all = [class_softmax_orig]\n",
    "for idx in top_k_idxs:\n",
    "    block_layer_name, index = styleSpace_vec_idx2coord[idx]\n",
    "    direction = - styleSpace_grads_vec.mean(axis=0)[idx].sign()\n",
    "    imgs_newStyle = generate_img_new_style(w, block_layer_name, index, direction=3*direction)\n",
    "    imgs_newStyle = postprocess_images(imgs_newStyle)\n",
    "    imgs_plot.append(imgs_newStyle)\n",
    "    with torch.no_grad():\n",
    "        digits_pred_newStyle = classifier_digits(imgs_newStyle).cpu()\n",
    "        class_softmax_newStyle = F.softmax(digits_pred_newStyle, dim=1)[torch.arange(n_images), class_generated]\n",
    "    class_softmax_all.append(class_softmax_newStyle)\n",
    "\n",
    "fig, axs = plt.subplots(n_images, top_k+1, figsize=(15, 5))\n",
    "for k, img in enumerate(imgs_plot):\n",
    "    ax =  axs[k]\n",
    "    ax.imshow(img.detach().cpu().squeeze(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "    title = 'original' if k == 0 else '$s_{' + str(top_k_idxs[k-1]) + '}$'\n",
    "    title += '\\n{}: {:.0f}%'.format(class_generated.squeeze().cpu().numpy(), 100*class_softmax_all[k].squeeze().detach().cpu().numpy())\n",
    "    ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at mostly well classified images (condition = class pred), and check directions to degrade perfo\n",
    "\n",
    "Which value of direction ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple images - one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 1000\n",
    "batch_size = 32\n",
    "class_selected = 0\n",
    "\n",
    "w_ok = None\n",
    "while True:\n",
    "    z = torch.randn([batch_size, G.z_dim], device=device)    # latent codes\n",
    "    if conditional:\n",
    "        digits = class_selected*torch.ones((batch_size, ), dtype=torch.int64, device=device)\n",
    "        c = F.one_hot(digits, G.c_dim)\n",
    "        class_generated = c.argmax(axis=1)\n",
    "    else:\n",
    "        c = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # compute input\n",
    "        w = G.mapping(z, c, truncation_psi=1, truncation_cutoff=8)[:, 0, :] # keep only the first element\n",
    "\n",
    "        # compute output\n",
    "        imgs_orig = G.synthesis(w.unsqueeze(1).repeat((1, G.num_ws, 1)), noise_mode='const', force_fp32=True)\n",
    "        imgs_orig = postprocess_images(imgs_orig)\n",
    "\n",
    "        digits_pred_orig = classifier_digits(imgs_orig)\n",
    "        # max_logit = digits_pred.max(axis=1).values\n",
    "        # max_softmax = F.softmax(digits_pred, dim=1).max(axis=1).values.mean()\n",
    "        class_pred = F.softmax(digits_pred_orig, dim=1).max(axis=1).indices\n",
    "        class_softmax_orig = F.softmax(digits_pred_orig, dim=1)[torch.arange(batch_size), class_generated]\n",
    "    \n",
    "    w_ok = w[class_pred == class_selected] if w_ok is None else torch.cat((w_ok, w[class_pred == class_selected]))\n",
    "    if w_ok.shape[0] >= n_images:\n",
    "        w_ok = w_ok[:n_images]\n",
    "        break\n",
    "ws_ok = w_ok.unsqueeze(1).repeat((1, G.num_ws, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute input\n",
    "class_generated = class_selected*torch.ones((batch_size, ), dtype=torch.int64, device=device)\n",
    "\n",
    "class_softmax_orig_all = None\n",
    "imgs_orig_all = None\n",
    "styleSpace_values_vec_all = None\n",
    "styleSpace_grads_vec_all = None\n",
    "for ws in DataLoader(ws_ok, batch_size=batch_size):\n",
    "    ws.requires_grad = True\n",
    "    # compute output\n",
    "    imgs_orig = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "    imgs_orig = postprocess_images(imgs_orig)\n",
    "\n",
    "    digits_pred_orig = classifier_digits(imgs_orig)\n",
    "    # max_logit = digits_pred.max(axis=1).values\n",
    "    # max_softmax = F.softmax(digits_pred, dim=1).max(axis=1).values.mean()\n",
    "    class_pred = F.softmax(digits_pred_orig, dim=1).max(axis=1).indices\n",
    "    class_logits_orig = digits_pred_orig[torch.arange(ws.shape[0]), class_generated[:ws.shape[0]]] # class_generated[:ws.shape[0]] to handle last batch\n",
    "    class_softmax_orig = F.softmax(digits_pred_orig, dim=1)[torch.arange(ws.shape[0]), class_generated[:ws.shape[0]]] # class_generated[:ws.shape[0]] to handle last batch\n",
    "    assert all(class_pred == class_selected), 'class_pred != class_selected'\n",
    "    \n",
    "    # style values (styleSpace_values from hook)\n",
    "    styleSpace_values_vec = styleSpace_dict2vec(styleSpace_values)\n",
    "    \n",
    "    # backpropagate gradient to get its values (styleSpace_grads from hook)\n",
    "    # loss = class_logits_orig.mean() # goal is to reduce the score for the class\n",
    "    loss = class_softmax_orig.mean() # goal is to reduce the score for the class, SEEMS TO WORK BETTER THAN USING LOGIT\n",
    "    loss.backward() # compute gradients and access them thanks to the backward hook\n",
    "    styleSpace_grads_vec = styleSpace_dict2vec(styleSpace_grads)\n",
    "    \n",
    "    # record variables\n",
    "    class_softmax_orig_all = class_softmax_orig.detach().cpu() if class_softmax_orig_all is None else torch.cat((class_softmax_orig_all, class_softmax_orig.detach().cpu()))\n",
    "    imgs_orig_all = imgs_orig.detach().cpu() if imgs_orig_all is None else torch.cat((imgs_orig_all, imgs_orig.detach().cpu()))\n",
    "    styleSpace_values_vec_all = styleSpace_values_vec if styleSpace_values_vec_all is None else torch.cat((styleSpace_values_vec_all, styleSpace_values_vec))\n",
    "    styleSpace_grads_vec_all = styleSpace_grads_vec if styleSpace_grads_vec_all is None else torch.cat((styleSpace_grads_vec_all, styleSpace_grads_vec))\n",
    "    \n",
    "    \n",
    "plot_images(imgs_orig_all[:100])\n",
    "plt.title('100 first images')\n",
    "\n",
    "style_min_vec = styleSpace_values_vec_all.min(dim=0).values\n",
    "style_min = styleSpace_vec2dict(style_min_vec)\n",
    "style_max_vec = styleSpace_values_vec_all.max(dim=0).values\n",
    "style_max = styleSpace_vec2dict(style_max_vec)\n",
    "style_std_vec = styleSpace_values_vec_all.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 10\n",
    "\n",
    "top_k = 5\n",
    "top_k_idxs = styleSpace_grads_vec_all.mean(axis=0).abs().topk(top_k).indices.cpu().numpy()\n",
    "top_k_vals = styleSpace_grads_vec_all.mean(axis=0)[top_k_idxs]\n",
    "\n",
    "styleSpace_vec_idx2coord = compute_styleSpace_vec_idx2coord()\n",
    "\n",
    "imgs_plot = torch.empty((n_images, top_k+1, 1, 28, 28))\n",
    "imgs_plot[:, 0] = imgs_orig_all[:n_images]\n",
    "class_softmax_all = torch.empty((n_images, top_k+1))\n",
    "class_softmax_all[:, 0] = class_softmax_orig_all[:n_images]\n",
    "for i, ws in enumerate(ws_ok[:n_images]):\n",
    "    ws = ws.unsqueeze(0) # expand batch dim\n",
    "    for j, idx in enumerate(top_k_idxs):\n",
    "        block_layer_name, index = styleSpace_vec_idx2coord[idx]\n",
    "        direction = styleSpace_grads_vec_all.mean(axis=0)[idx].sign() <= 0\n",
    "        # direction = not(direction)\n",
    "        imgs_newStyle = generate_img_new_style2(ws, block_layer_name, index, style_min[block_layer_name][index], style_max[block_layer_name][index], s_shift=2, positive_direction=direction)\n",
    "        imgs_newStyle = postprocess_images(imgs_newStyle)\n",
    "        with torch.no_grad():\n",
    "            digits_pred_newStyle = classifier_digits(imgs_newStyle)\n",
    "        class_softmax_newStyle = F.softmax(digits_pred_newStyle, dim=1)[0, class_selected]\n",
    "        \n",
    "        # record variables\n",
    "        imgs_plot[i, j+1] = imgs_newStyle.detach().cpu()\n",
    "        class_softmax_all[i, j+1] = class_softmax_newStyle.cpu()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(n_images, top_k+1, figsize=(8, 20))\n",
    "for i in range(n_images): # for each image\n",
    "    for s in range(top_k+1): # for each style\n",
    "        ax = axs[i, s]\n",
    "        ax.imshow(imgs_plot[i, s].squeeze(), vmin=0, vmax=1, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.grid(False)\n",
    "        title = 'original' if s == 0 else '$s_{' + str(top_k_idxs[s-1]) + '}$'\n",
    "        title += '\\n{}: {:.0f}%'.format(class_selected, 100*class_softmax_all[i, s])\n",
    "        ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Find noise regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get latent codes of low and high noise samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_images = 1000\n",
    "class_selected = 0\n",
    "\n",
    "noise_pred_all = None\n",
    "w_low_noise = None\n",
    "w_high_noise = None\n",
    "while True:\n",
    "    z = torch.randn([batch_size, G.z_dim], device=device)    # latent codes\n",
    "    if conditional:\n",
    "        # digits = torch.randint(0, G.c_dim, (batch_size,), device=device)\n",
    "        digits = class_selected*torch.ones((batch_size, ), dtype=torch.int64, device=device)\n",
    "        c = F.one_hot(digits, G.c_dim)          \n",
    "    else:\n",
    "        c = None\n",
    "\n",
    "    ws = G.mapping(z, c, truncation_psi=1)\n",
    "    imgs = G.synthesis(ws, noise_mode='const')\n",
    "    imgs = postprocess_images(imgs)\n",
    "\n",
    "    # noise_pred_all = classifier_noise(imgs).argmax(dim=1).cpu().int() if noise_pred_all is None else torch.cat((noise_pred_all, classifier_noise(imgs).argmax(dim=1).cpu().int()))\n",
    "    noise_pred = classifier_noise(imgs).argmax(dim=1).cpu().int()\n",
    "\n",
    "    w_low_noise = ws[noise_pred == 0][:, 0, :] if w_low_noise is None else torch.cat((w_low_noise, ws[noise_pred == 0][:, 0, :]))\n",
    "    w_high_noise = ws[noise_pred == 3][:, 0, :] if w_high_noise is None else torch.cat((w_high_noise, ws[noise_pred == 3][:, 0, :]))\n",
    "    if len(w_low_noise) > n_images and len(w_high_noise) > n_images:\n",
    "        w_low_noise = w_low_noise[:n_images]\n",
    "        w_high_noise = w_high_noise[:n_images]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styleSpace_values_low_noise_vec = None\n",
    "for w in DataLoader(w_low_noise, batch_size=batch_size):\n",
    "    w.requires_grad = True\n",
    "    ws = w.unsqueeze(1).repeat((1, G.num_ws, 1))\n",
    "    # compute output\n",
    "    imgs_orig = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "    imgs_orig = postprocess_images(imgs_orig)\n",
    "    \n",
    "    # style values (styleSpace_values from hook)\n",
    "    styleSpace_values_vec = styleSpace_dict2vec(styleSpace_values)\n",
    "\n",
    "    # record variables\n",
    "    styleSpace_values_low_noise_vec = styleSpace_values_vec if styleSpace_values_low_noise_vec is None else torch.cat((styleSpace_values_low_noise_vec, styleSpace_values_vec))\n",
    "\n",
    "\n",
    "styleSpace_values_high_noise_vec = None\n",
    "for w in DataLoader(w_high_noise, batch_size=batch_size):\n",
    "    w.requires_grad = True\n",
    "    ws = w.unsqueeze(1).repeat((1, G.num_ws, 1))\n",
    "    # compute output\n",
    "    imgs_orig = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "    imgs_orig = postprocess_images(imgs_orig)\n",
    "    \n",
    "    # style values (styleSpace_values from hook)\n",
    "    styleSpace_values_vec = styleSpace_dict2vec(styleSpace_values)\n",
    "\n",
    "    # record variables\n",
    "    styleSpace_values_high_noise_vec = styleSpace_values_vec if styleSpace_values_high_noise_vec is None else torch.cat((styleSpace_values_high_noise_vec, styleSpace_values_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(np.arange(len(styleSpace_values_high_noise_vec.mean(0).cpu().numpy())), styleSpace_values_high_noise_vec.mean(0).cpu().numpy(), s=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(np.arange(len(styleSpace_values_low_noise_vec.mean(0).cpu().numpy())), styleSpace_values_low_noise_vec.mean(0).cpu().numpy(), s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.cat((styleSpace_values_high_noise_vec, styleSpace_values_low_noise_vec)).mean(0)\n",
    "std = torch.cat((styleSpace_values_high_noise_vec, styleSpace_values_low_noise_vec)).std(0)\n",
    "noise_direction = (((styleSpace_values_high_noise_vec - mean) / std).mean(0) - ((styleSpace_values_low_noise_vec - mean) / std).mean(0)).cpu().numpy()\n",
    "top_k_dims = (-np.abs(noise_direction)).argsort()[:20]\n",
    "print(top_k_dims)\n",
    "plt.figure()\n",
    "plt.scatter(np.arange(len(noise_direction)), noise_direction, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "style_min_np = styleSpace_dict2vec(style_min).cpu().numpy()[0]\n",
    "plt.scatter(np.arange(len(style_min_np)), style_min_np, s=1, alpha=0.5)\n",
    "style_max_np = styleSpace_dict2vec(style_max).cpu().numpy()[0]\n",
    "plt.scatter(np.arange(len(style_max_np)), style_max_np, s=1, alpha=0.5)\n",
    "plt.scatter(np.arange(len(noise_direction)), noise_direction, s=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_direction_w = (w_high_noise.mean(0) - w_low_noise.mean(0)).cpu().numpy()\n",
    "plt.figure()\n",
    "plt.scatter(np.arange(len(noise_direction_w)), noise_direction_w, s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Style dimensions with the most impact on noise : low-noise vs. high-noise samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_noise_dimensions = 5\n",
    "\n",
    "for i in range(top_k_noise_dimensions):\n",
    "    k = (-np.abs(noise_direction)).argsort()[i]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(styleSpace_values_high_noise_vec[:, k].cpu().numpy(), bins=20, edgecolor='none', alpha=0.8)\n",
    "    plt.hist(styleSpace_values_low_noise_vec[:, k].cpu().numpy(), bins=20, edgecolor='none', alpha=0.8)\n",
    "    plt.axvline(style_min_vec[k].cpu().numpy())\n",
    "    plt.axvline(style_max_vec[k].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 10\n",
    "\n",
    "top_k = 10\n",
    "top_k_idxs = (-np.abs(noise_direction)).argsort()[:top_k]\n",
    "\n",
    "styleSpace_vec_idx2coord = compute_styleSpace_vec_idx2coord()\n",
    "\n",
    "imgs_plot = torch.empty((n_images, top_k+1, 1, 28, 28))\n",
    "imgs_plot[:, 0] = imgs_orig_all[:n_images]\n",
    "class_softmax_all = torch.empty((n_images, top_k+1))\n",
    "class_softmax_all[:, 0] = class_softmax_orig_all[:n_images]\n",
    "for i, ws in enumerate(ws_ok[:n_images]):\n",
    "    ws = ws.unsqueeze(0) # expand batch dim\n",
    "    for j, idx in enumerate(top_k_idxs):\n",
    "        block_layer_name, index = styleSpace_vec_idx2coord[idx]\n",
    "        direction = np.sign(noise_direction[idx]) >= 0\n",
    "        # direction = not(direction)\n",
    "        imgs_newStyle = generate_img_new_style3(ws, block_layer_name, index, s_std=style_std_vec[idx], strength=10, positive_direction=direction)\n",
    "        imgs_newStyle = postprocess_images(imgs_newStyle)\n",
    "        with torch.no_grad():\n",
    "            digits_pred_newStyle = classifier_digits(imgs_newStyle)\n",
    "        class_softmax_newStyle = F.softmax(digits_pred_newStyle, dim=1)[0, class_selected]\n",
    "        \n",
    "        # record variables\n",
    "        imgs_plot[i, j+1] = imgs_newStyle.detach().cpu()\n",
    "        class_softmax_all[i, j+1] = class_softmax_newStyle.cpu()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(n_images, top_k+1, figsize=(15, 20))\n",
    "for i in range(n_images): # for each image\n",
    "    for s in range(top_k+1): # for each style\n",
    "        ax = axs[i, s]\n",
    "        ax.imshow(imgs_plot[i, s].squeeze(), vmin=0, vmax=1, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.grid(False)\n",
    "        title = 'original' if s == 0 else '$s_{' + str(top_k_idxs[s-1]) + '}$'\n",
    "        title += '\\n{}: {:.0f}%'.format(class_selected, 100*class_softmax_all[i, s])\n",
    "        ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 20\n",
    "top_k_idxs = (-np.abs(noise_direction)).argsort()[:top_k]\n",
    "\n",
    "strength = 10\n",
    "\n",
    "styleSpace_values_low_noise_vec_shifted = styleSpace_values_low_noise_vec.clone()\n",
    "for k in top_k_idxs:\n",
    "    positive_direction = noise_direction[k] >= 0\n",
    "    d = 1 if positive_direction else -1\n",
    "    weight_shift = weight_shift = d * strength * style_std_vec[k]\n",
    "    styleSpace_values_low_noise_vec_shifted[:, k] += weight_shift\n",
    "\n",
    "n_images = 8\n",
    "imgs = generate_img_from_s(styleSpace_values_low_noise_vec[:n_images])\n",
    "imgs = postprocess_images(imgs)\n",
    "plot_images(imgs)\n",
    "plt.title('low noise samples')\n",
    "\n",
    "imgs = generate_img_from_s(styleSpace_values_low_noise_vec_shifted[:n_images])\n",
    "imgs = postprocess_images(imgs)\n",
    "plot_images(imgs)\n",
    "plt.title('low noise samples shifted to high noise')\n",
    "\n",
    "imgs = generate_img_from_s(styleSpace_values_high_noise_vec[:n_images])\n",
    "imgs = postprocess_images(imgs)\n",
    "plot_images(imgs)\n",
    "plt.title('some high noise samples to compare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance from low-noise center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_noise_center = styleSpace_values_low_noise_vec.mean(0).unsqueeze(0)\n",
    "\n",
    "dist_low2center = torch.cdist(styleSpace_values_low_noise_vec, low_noise_center).cpu().numpy()\n",
    "dist_high2center = torch.cdist(styleSpace_values_high_noise_vec, low_noise_center).cpu().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(dist_low2center, bins=20, edgecolor='none', alpha=0.8, label='low noise')\n",
    "plt.hist(dist_high2center, bins=20, edgecolor='none', alpha=0.8, label='high noise')\n",
    "plt.legend()\n",
    "plt.title('distance from low noise center using all dimensions');\n",
    "\n",
    "\n",
    "top_k_dims = (-np.abs(noise_direction)).argsort()[:10]\n",
    "\n",
    "dist_low2center = torch.cdist(styleSpace_values_low_noise_vec[:, top_k_dims], low_noise_center[:, top_k_dims]).cpu().numpy()\n",
    "dist_high2center = torch.cdist(styleSpace_values_high_noise_vec[:, top_k_dims], low_noise_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(dist_low2center, bins=20, edgecolor='none', alpha=0.8, label='low noise')\n",
    "plt.hist(dist_high2center, bins=20, edgecolor='none', alpha=0.8, label='high noise')\n",
    "plt.legend()\n",
    "plt.title('distance from low noise center using top noise dimensions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_misclassified = 0\n",
    "for w in DataLoader(w_low_noise, batch_size):\n",
    "    imgs = G.synthesis(w.unsqueeze(1).repeat((1, G.num_ws, 1)), noise_mode='const')\n",
    "    imgs = postprocess_images(imgs)\n",
    "    digits_pred = classifier_digits(imgs)\n",
    "    class_pred = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "    nb_misclassified += (class_pred != 0).sum().cpu().numpy().item()\n",
    "accuracy = 100 - 100 * nb_misclassified / len(w_high_noise)\n",
    "print(f'accuracy low noise samples: {accuracy}%')\n",
    "\n",
    "nb_misclassified = 0\n",
    "for w in DataLoader(w_high_noise, batch_size):\n",
    "    imgs = G.synthesis(w.unsqueeze(1).repeat((1, G.num_ws, 1)), noise_mode='const')\n",
    "    imgs = postprocess_images(imgs)\n",
    "    digits_pred = classifier_digits(imgs)\n",
    "    class_pred = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "    nb_misclassified += (class_pred != 0).sum().cpu().numpy().item()\n",
    "accuracy = 100 - 100 * nb_misclassified / len(w_high_noise)\n",
    "print(f'accuracy high noise samples: {accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 1000\n",
    "batch_size = 32\n",
    "class_selected = 0\n",
    "\n",
    "w_wellclassified = None\n",
    "w_misclassified = None\n",
    "s_wellclassified = None\n",
    "s_misclassified = None\n",
    "while True:\n",
    "    z = torch.randn([batch_size, G.z_dim], device=device)    # latent codes\n",
    "    if conditional:\n",
    "        digits = class_selected*torch.ones((batch_size, ), dtype=torch.int64, device=device)\n",
    "        c = F.one_hot(digits, G.c_dim)\n",
    "        class_generated = c.argmax(axis=1)\n",
    "    else:\n",
    "        c = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # compute input\n",
    "        w = G.mapping(z, c, truncation_psi=1, truncation_cutoff=8)[:, 0, :] # keep only the first element\n",
    "\n",
    "        # compute output\n",
    "        imgs = G.synthesis(w.unsqueeze(1).repeat((1, G.num_ws, 1)), noise_mode='const', force_fp32=True)\n",
    "        imgs = postprocess_images(imgs)\n",
    "\n",
    "        digits_pred = classifier_digits(imgs)\n",
    "        class_pred = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "        class_softmax = F.softmax(digits_pred, dim=1)[torch.arange(batch_size), class_generated]\n",
    "        \n",
    "    # style values (styleSpace_values from hook)\n",
    "    styleSpace_values_vec = styleSpace_dict2vec(styleSpace_values)\n",
    "    \n",
    "    w_wellclassified = w[class_pred == class_selected] if w_wellclassified is None else torch.cat((w_wellclassified, w[class_pred == class_selected]))\n",
    "    w_misclassified = w[class_pred != class_selected] if w_misclassified is None else torch.cat((w_misclassified, w[class_pred != class_selected]))\n",
    "    s_wellclassified = styleSpace_values_vec[class_pred == class_selected] if s_wellclassified is None else torch.cat((s_wellclassified, styleSpace_values_vec[class_pred == class_selected]))\n",
    "    s_misclassified = styleSpace_values_vec[class_pred != class_selected] if s_misclassified is None else torch.cat((s_misclassified, styleSpace_values_vec[class_pred != class_selected]))\n",
    "    if (w_wellclassified.shape[0] + w_misclassified.shape[0]) >= n_images:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_wellclassified2center = torch.cdist(s_wellclassified, low_noise_center).cpu().numpy()\n",
    "dist_misclassified2center = torch.cdist(s_misclassified, low_noise_center).cpu().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "plt.hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "plt.legend()\n",
    "plt.title('distance from low noise center using all dimensions');\n",
    "\n",
    "\n",
    "\n",
    "dist_wellclassified2center = torch.cdist(s_wellclassified[:, top_k_dims], low_noise_center[:, top_k_dims]).cpu().numpy()\n",
    "dist_misclassified2center = torch.cdist(s_misclassified[:, top_k_dims], low_noise_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "plt.hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "plt.legend()\n",
    "plt.title('distance from low noise center using top noise dimensions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## misclassified vs. well classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_selected = 0\n",
    "class_selected = 'all'\n",
    "n_images = 100000\n",
    "batch_size = 32\n",
    "\n",
    "if class_selected == 'all': \n",
    "    digits = torch.randint(0, G.c_dim, (n_images,), dtype=torch.int64, device=device)\n",
    "else:\n",
    "    digits = class_selected*torch.ones((n_images, ), dtype=torch.int64, device=device)\n",
    "\n",
    "z_all = None\n",
    "w_all = None\n",
    "s_all = None\n",
    "class_predicted = None\n",
    "for labels in DataLoader(digits, batch_size):\n",
    "    batch_size_t = len(labels)\n",
    "    z = torch.randn([batch_size_t, G.z_dim], device=device)    # sample latent codes\n",
    "    c = F.one_hot(labels, G.c_dim)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # compute input\n",
    "        w = G.mapping(z, c, truncation_psi=1, truncation_cutoff=8)[:, 0, :] # keep only the first element\n",
    "\n",
    "        # compute output\n",
    "        imgs = G.synthesis(w.unsqueeze(1).repeat((1, G.num_ws, 1)), noise_mode='const', force_fp32=True)\n",
    "        imgs = postprocess_images(imgs)\n",
    "\n",
    "        digits_pred = classifier_digits(imgs)\n",
    "        class_pred_t = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "        class_softmax = F.softmax(digits_pred, dim=1)[torch.arange(batch_size_t), labels]\n",
    "        \n",
    "    # style values (styleSpace_values from hook)\n",
    "    s_vec = styleSpace_dict2vec(styleSpace_values)\n",
    "\n",
    "    z_all = z if z_all is None else torch.cat((z_all, z))\n",
    "    w_all = w if w_all is None else torch.cat((w_all, w))\n",
    "    s_all = s_vec if s_all is None else torch.cat((s_all, s_vec))\n",
    "    class_predicted = class_pred_t if class_predicted is None else torch.cat((class_predicted, class_pred_t))\n",
    "\n",
    "z_wellclassified = z_all[class_predicted == digits]\n",
    "z_misclassified = z_all[class_predicted != digits]\n",
    "w_wellclassified = w_all[class_predicted == digits]\n",
    "w_misclassified = w_all[class_predicted != digits]\n",
    "s_wellclassified = s_all[class_predicted == digits]\n",
    "s_misclassified = s_all[class_predicted != digits]\n",
    "digits_wellclassified = digits[class_predicted == digits]\n",
    "digits_misclassified = digits[class_predicted != digits]\n",
    "\n",
    "\n",
    "style_min_vec = s_all.min(dim=0).values\n",
    "style_min = styleSpace_vec2dict(style_min_vec)\n",
    "style_max_vec = s_all.max(dim=0).values\n",
    "style_max = styleSpace_vec2dict(style_max_vec)\n",
    "style_std_vec = s_all.std(dim=0)\n",
    "\n",
    "print('Accuracy: {:.2f}% ; {}/{} misclassified samples'.format(100 * w_wellclassified.shape[0] / (w_wellclassified.shape[0] + w_misclassified.shape[0]), w_misclassified.shape[0], (w_wellclassified.shape[0] + w_misclassified.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "z_embedded = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(z_all[:n_samples].cpu().numpy())\n",
    "w_embedded = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(w_all[:n_samples].cpu().numpy())\n",
    "s_embedded = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(s_all[:n_samples].cpu().numpy())\n",
    "\n",
    "colors = ['C0' if class_predicted[i] == digits[i] else 'C1' for i in range(n_samples)]\n",
    "labels = ['well-classified' if class_predicted[i] == digits[i] else 'misclassified' for i in range(n_samples)]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('t-SNE')\n",
    "axs[0].scatter(z_embedded[:, 0], z_embedded[:, 1], c=colors, alpha=0.3)\n",
    "axs[0].set_title('in Z space')\n",
    "axs[1].scatter(w_embedded[:, 0], w_embedded[:, 1], c=colors, alpha=0.3)\n",
    "axs[1].set_title('in W space')\n",
    "axs[2].scatter(s_embedded[:, 0], s_embedded[:, 1], c=colors, alpha=0.3)\n",
    "axs[2].set_title('in S space')\n",
    "for ax in axs:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "\n",
    "wellclassified = (class_predicted == digits)[:n_samples].cpu().numpy()\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('t-SNE')\n",
    "\n",
    "axs[0].set_title('in Z space')\n",
    "axs[0].scatter(z_embedded[wellclassified, 0], z_embedded[wellclassified, 1], c='C0', label='well-classified', alpha=0.1)\n",
    "axs[0].scatter(z_embedded[np.logical_not(wellclassified), 0], z_embedded[np.logical_not(wellclassified), 1], c='C1', label='misclassified', alpha=0.1)\n",
    "axs[1].set_title('in W space')\n",
    "axs[1].scatter(w_embedded[wellclassified, 0], w_embedded[wellclassified, 1], c='C0', label='well-classified', alpha=0.1)\n",
    "axs[1].scatter(w_embedded[np.logical_not(wellclassified), 0], w_embedded[np.logical_not(wellclassified), 1], c='C1', label='misclassified', alpha=0.1)\n",
    "axs[2].set_title('in S space')\n",
    "axs[2].scatter(s_embedded[wellclassified, 0], s_embedded[wellclassified, 1], c='C0', label='well-classified', alpha=0.1)\n",
    "axs[2].scatter(s_embedded[np.logical_not(wellclassified), 0], s_embedded[np.logical_not(wellclassified), 1], c='C1', label='misclassified', alpha=0.1)\n",
    "for ax in axs:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 8\n",
    "\n",
    "imgs = generate_img_from_s(s_wellclassified[:n_images])\n",
    "imgs = postprocess_images(imgs)\n",
    "plot_images(imgs)\n",
    "plt.title('well-classified')\n",
    "\n",
    "imgs = generate_img_from_s(s_misclassified[:n_images])\n",
    "imgs = postprocess_images(imgs)\n",
    "plot_images(imgs)\n",
    "plt.title('misclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellclassified_center = s_wellclassified.mean(0).unsqueeze(0)\n",
    "\n",
    "# All directions\n",
    "dist_wellclassified2center = torch.cdist(s_wellclassified, wellclassified_center).cpu().numpy()\n",
    "dist_misclassified2center = torch.cdist(s_misclassified, wellclassified_center).cpu().numpy()\n",
    "\n",
    "hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "axs[0].hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('distance from well-classified center using all dimensions')\n",
    "axs[1].plot(distance, accuracy)\n",
    "axs[1].set_xlabel('distance from well-classified center using all dimensions')\n",
    "axs[1].set_ylabel('accuracy [%]')\n",
    "\n",
    "# Top perfo directions \n",
    "# NORMALIZED\n",
    "perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "# ORIGINAL\n",
    "# perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "# STYLESPACE\n",
    "# s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "# perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "top_k_dims = (-np.abs(perfo_direction)).argsort()[:10]\n",
    "\n",
    "\n",
    "dist_wellclassified2center = torch.cdist(s_wellclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "dist_misclassified2center = torch.cdist(s_misclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "axs[0].hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('distance from well-classified center using top perfo dimensions')\n",
    "axs[1].plot(distance, accuracy)\n",
    "axs[1].set_xlabel('distance from well-classified center using top perfo dimensions')\n",
    "axs[1].set_ylabel('accuracy [%]')\n",
    "\n",
    "# # Top softmax directions\n",
    "# top_k = 10\n",
    "# top_k_dims = styleSpace_grads_vec_all.mean(axis=0).abs().topk(top_k).indices.cpu().numpy()\n",
    "\n",
    "# dist_wellclassified2center = torch.cdist(s_wellclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "# dist_misclassified2center = torch.cdist(s_misclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "# hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "# hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "# distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "# accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "# axs[0].hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "# axs[0].hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "# axs[0].legend()\n",
    "# axs[0].set_title('distance from well-classified center using top softmax dimensions')\n",
    "# axs[1].plot(distance, accuracy)\n",
    "# axs[1].set_xlabel('distance from well-classified center using top softmax dimensions')\n",
    "# axs[1].set_ylabel('accuracy [%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_center = s_all.mean(0).unsqueeze(0)\n",
    "\n",
    "# All directions\n",
    "dist_wellclassified2center = torch.cdist(s_wellclassified, s_center).cpu().numpy()\n",
    "dist_misclassified2center = torch.cdist(s_misclassified, s_center).cpu().numpy()\n",
    "\n",
    "hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "axs[0].hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('distance from global center using all dimensions')\n",
    "axs[1].plot(distance, accuracy)\n",
    "axs[1].set_xlabel('distance from global center using all dimensions')\n",
    "axs[1].set_ylabel('accuracy [%]')\n",
    "\n",
    "# Top perfo directions NORMALIZED VECTORS\n",
    "perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "# Top perfo directions\n",
    "# perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "# Top perfo directions LIKE STYLESPACE\n",
    "# s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "# perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "top_k_dims = (-np.abs(perfo_direction)).argsort()[:10]\n",
    "\n",
    "dist_wellclassified2center = torch.cdist(s_wellclassified[:, top_k_dims], s_center[:, top_k_dims]).cpu().numpy()\n",
    "dist_misclassified2center = torch.cdist(s_misclassified[:, top_k_dims], s_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "axs[0].hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('distance from global center using top perfo dimensions')\n",
    "axs[1].plot(distance, accuracy)\n",
    "axs[1].set_xlabel('distance from global center using top perfo dimensions')\n",
    "axs[1].set_ylabel('accuracy [%]')\n",
    "\n",
    "# # Top softmax directions\n",
    "# top_k = 10\n",
    "# top_k_dims = styleSpace_grads_vec_all.mean(axis=0).abs().topk(top_k).indices.cpu().numpy()\n",
    "\n",
    "# dist_wellclassified2center = torch.cdist(s_wellclassified[:, top_k_dims], s_center[:, top_k_dims]).cpu().numpy()\n",
    "# dist_misclassified2center = torch.cdist(s_misclassified[:, top_k_dims], s_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "# hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "# hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "# distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "# accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "# axs[0].hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "# axs[0].hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "# axs[0].legend()\n",
    "# axs[0].set_title('distance from global center using top softmax dimensions')\n",
    "# axs[1].plot(distance, accuracy)\n",
    "# axs[1].set_xlabel('distance from global center using top softmax dimensions')\n",
    "# axs[1].set_ylabel('accuracy [%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(np.arange(len(perfo_direction)), np.abs(perfo_direction[(-np.abs(perfo_direction)).argsort()]))\n",
    "plt.xlabel('dimension')\n",
    "plt.ylabel('absolute difference')\n",
    "\n",
    "top_k = 5\n",
    "fig, axs = plt.subplots(1, top_k, figsize=(20, 5))\n",
    "for i in range(top_k):\n",
    "    k = (-np.abs(perfo_direction)).argsort()[i]\n",
    "    \n",
    "    axs[i].set_title(r'$s_{' + str(k) + '}$')\n",
    "    axs[i].hist(s_wellclassified[:, k].cpu().numpy(), bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "    axs[i].hist(s_misclassified[:, k].cpu().numpy(), bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "    axs[i].axvline(style_min_vec[k].cpu().numpy(), color='k', ls='--')\n",
    "    # plt.text(1.1*style_min_vec[k].cpu().numpy(), 100, 'empirical min',rotation=90)\n",
    "    axs[i].axvline(style_max_vec[k].cpu().numpy(), color='k', ls='--')\n",
    "    # plt.text(1.1*style_max_vec[k].cpu().numpy(), 100, 'empirical max',rotation=90)\n",
    "    axs[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 50\n",
    "top_k_idxs = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "strength = 10\n",
    "\n",
    "s_wellclassified_shifted = s_wellclassified.clone()\n",
    "for k in top_k_idxs:\n",
    "    positive_direction = perfo_direction[k] >= 0\n",
    "    # positive_direction = not(positive_direction)\n",
    "    d = 1 if positive_direction else -1\n",
    "    weight_shift = d * strength * style_std_vec[k]\n",
    "    s_wellclassified_shifted[:, k] += weight_shift\n",
    "\n",
    "\n",
    "n_images = 8\n",
    "imgs_orig = generate_img_from_s(s_wellclassified[:n_images])\n",
    "imgs_orig = postprocess_images(imgs_orig)\n",
    "imgs_orig = imgs_orig * 255\n",
    "imgs_orig = imgs_orig.to(torch.uint8).cpu()\n",
    "\n",
    "imgs_corr = generate_img_from_s(s_wellclassified_shifted[:n_images])\n",
    "imgs_corr = postprocess_images(imgs_corr)\n",
    "imgs_corr = imgs_corr * 255\n",
    "imgs_corr = imgs_corr.to(torch.uint8).cpu()\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15, 4))\n",
    "axs[0].imshow(vutils.make_grid(imgs_orig, pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('well-classified samples')\n",
    "axs[1].imshow(vutils.make_grid(imgs_corr, pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "axs[1].set_title('same samples after corruption')\n",
    "axs[1].axis('off')\n",
    "\n",
    "\n",
    "s_misclassified_shifted = s_misclassified.clone()\n",
    "for k in top_k_idxs:\n",
    "    positive_direction = perfo_direction[k] >= 0\n",
    "    positive_direction = not(positive_direction)\n",
    "    d = 1 if positive_direction else -1\n",
    "    weight_shift = d * strength * style_std_vec[k]\n",
    "    s_misclassified_shifted[:, k] += weight_shift\n",
    "\n",
    "\n",
    "imgs_orig = generate_img_from_s(s_misclassified[:n_images])\n",
    "imgs_orig = postprocess_images(imgs_orig)\n",
    "imgs_orig = imgs_orig * 255\n",
    "imgs_orig = imgs_orig.to(torch.uint8).cpu()\n",
    "\n",
    "imgs_clean = generate_img_from_s(s_misclassified_shifted[:n_images])\n",
    "imgs_clean = postprocess_images(imgs_clean)\n",
    "imgs_clean = imgs_clean * 255\n",
    "imgs_clean = imgs_clean.to(torch.uint8).cpu()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15, 4))\n",
    "axs[0].imshow(vutils.make_grid(imgs_orig, pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('misclassified samples')\n",
    "axs[1].imshow(vutils.make_grid(imgs_clean, pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "axs[1].set_title('same samples after cleaning')\n",
    "axs[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strength = 10\n",
    "top_k_list = [1, 10, 20, 50, 100, 150, 250, 500, 1000, 2000, 3000, s_wellclassified.shape[1]]\n",
    "\n",
    "df = pd.Series(index=top_k_list, dtype='float64')\n",
    "show_n_images = 8\n",
    "imgs_visualize = {}\n",
    "\n",
    "for top_k in top_k_list:\n",
    "\n",
    "    top_k_idxs = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "    s_wellclassified_shifted = s_wellclassified.clone()\n",
    "    for k in top_k_idxs:\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_wellclassified_shifted[:, k] += weight_shift\n",
    "\n",
    "\n",
    "    # get misclassifications nb resulting from the shift\n",
    "    nb_misclassifications = 0\n",
    "    for s, labels in zip(DataLoader(s_wellclassified_shifted, batch_size), DataLoader(digits[class_predicted == digits], batch_size)):\n",
    "\n",
    "        imgs = generate_img_from_s(s)\n",
    "        imgs = postprocess_images(imgs)\n",
    "        imgs_visualize[top_k] = imgs[:show_n_images]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            digits_pred = classifier_digits(imgs)\n",
    "            class_pred = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "            nb_misclassifications += (class_pred != labels).sum()\n",
    "            \n",
    "    accuracy = 100 - 100 * nb_misclassifications / s_wellclassified_shifted.shape[0]\n",
    "    df[top_k] = accuracy\n",
    "    print('Accuracy: {:.2f}% ; {}/{} misclassified samples'.format(accuracy, nb_misclassifications, s_wellclassified_shifted.shape[0]))\n",
    "\n",
    "plt.figure()\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strength = 5\n",
    "# top_k_list = [1, 10, 20, 50, 100, 150, 250, 500, 1000, 2000, 3000, s_wellclassified.shape[1]]\n",
    "top_k_list = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "df = pd.Series(index=top_k_list, dtype='float64')\n",
    "show_n_images = 8\n",
    "imgs_visualize = {}\n",
    "\n",
    "for top_k in top_k_list:\n",
    "\n",
    "    # shift images\n",
    "    top_k_idxs = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "    s_shifted_all = s_all.clone()\n",
    "    for k in top_k_idxs:\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_shifted_all[:, k] += weight_shift\n",
    "\n",
    "\n",
    "    # get classifications after the shift\n",
    "    class_predicted_after_shift = None\n",
    "    for s, labels in zip(DataLoader(s_shifted_all, batch_size), DataLoader(digits, batch_size)):\n",
    "\n",
    "        imgs = generate_img_from_s(s)\n",
    "        imgs = postprocess_images(imgs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            digits_pred = classifier_digits(imgs)\n",
    "            class_pred_t = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "\n",
    "        if class_predicted_after_shift is None: imgs_visualize[top_k] = imgs[:show_n_images] # save from 1st batch\n",
    "        class_predicted_after_shift = class_pred_t if class_predicted_after_shift is None else torch.cat((class_predicted_after_shift, class_pred_t))\n",
    "\n",
    "    s_shifted_wellclassified = s_shifted_all[class_predicted_after_shift == digits]\n",
    "    s_shifted_misclassified = s_shifted_all[class_predicted_after_shift != digits]\n",
    "    accuracy = 100 * s_shifted_wellclassified.shape[0] / s_shifted_all.shape[0]\n",
    "    df[top_k] = accuracy\n",
    "    print('Accuracy: {:.2f}% ; {}/{} misclassified samples'.format(accuracy, s_shifted_misclassified.shape[0], s_shifted_all.shape[0]))\n",
    "\n",
    "    # Top perfo directions \n",
    "    # NORMALIZED\n",
    "    perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "    # ORIGINAL\n",
    "    # perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "    # STYLESPACE\n",
    "    # s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "    # perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "    top_k_dims = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "    dist_wellclassified2center = torch.cdist(s_shifted_wellclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "    dist_misclassified2center = torch.cdist(s_shifted_misclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "    hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "    hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "    distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "    accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "\n",
    "    # PLOT\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n",
    "    axs[0].hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "    axs[0].hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "    axs[0].legend()\n",
    "    axs[0].set_xlabel(f'distance from well-classified center using top {top_k} perfo dimensions')\n",
    "    axs[0].set_ylabel('number of values')\n",
    "    axs[1].plot(distance, accuracy)\n",
    "    axs[1].set_xlabel(f'distance from well-classified center using top {top_k} perfo dimensions')\n",
    "    axs[1].set_ylabel('accuracy [%]')\n",
    "\n",
    "    plot_images(imgs_visualize[top_k])\n",
    "    plt.title(top_k)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at individual styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_norm_diff = (s_wellclassified - s_misclassified.mean(0)) / s_misclassified.std(0)\n",
    "relevance = s_norm_diff.mean(0) / s_norm_diff.std(0)\n",
    "perfo_direction = relevance.cpu().numpy()\n",
    "top_k_idxs = (-np.abs(relevance.cpu().numpy())).argsort()[:top_k]\n",
    "top_k_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "relevance = s_norm_diff.mean(0) / s_norm_diff.std(0)\n",
    "perfo_direction = relevance.cpu().numpy()\n",
    "top_k_idxs = (-np.abs(relevance.cpu().numpy())).argsort()[:top_k]\n",
    "top_k_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_norm_diff = (s_misclassified - s_all.mean(0)) / s_all.std(0)\n",
    "relevance = s_norm_diff.mean(0) / s_norm_diff.std(0)\n",
    "perfo_direction = relevance.cpu().numpy()\n",
    "top_k_idxs = (-np.abs(relevance.cpu().numpy())).argsort()[:top_k]\n",
    "top_k_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 10\n",
    "\n",
    "\n",
    "# ORIGINAL\n",
    "# perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "\n",
    "# NORMALIZED\n",
    "perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "\n",
    "# STYLESPACE\n",
    "# s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "# perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "top_k_list = [0, 1, 3, 6, 9, 12, 15, 20]\n",
    "imgs_plot = torch.empty((top_k+1, n_images, 1, 28, 28))\n",
    "class_softmax_all = torch.empty((top_k+1, n_images))\n",
    "\n",
    "for i, top_k_subset in enumerate(top_k_list):\n",
    "\n",
    "    top_k_idxs = (-np.abs(perfo_direction)).argsort()[:top_k_subset]\n",
    "    strength = 10\n",
    "\n",
    "    # manipulate images\n",
    "    s_wellclassified_shifted = s_wellclassified[:n_images].clone()\n",
    "    for k in top_k_idxs:\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_wellclassified_shifted[:, k] += weight_shift\n",
    "\n",
    "    imgs_newStyle = generate_img_from_s(s_wellclassified_shifted)\n",
    "    imgs_newStyle = postprocess_images(imgs_newStyle)\n",
    "    if type(class_selected) == int:\n",
    "        with torch.no_grad():\n",
    "            digits_pred_newStyle = classifier_digits(imgs_newStyle)\n",
    "        class_softmax_newStyle = F.softmax(digits_pred_newStyle, dim=1)[0, class_selected]\n",
    "\n",
    "    # record variables\n",
    "    imgs_plot[i] = imgs_newStyle.cpu()\n",
    "    if type(class_selected) == int: class_softmax_all[i] = class_softmax_newStyle.cpu()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(n_images, len(top_k_list), figsize=(12, 20))\n",
    "for i in range(n_images): # for each image\n",
    "    for j, nb_k in enumerate(top_k_list): # for each style\n",
    "        ax = axs[i, j]\n",
    "        ax.imshow(imgs_plot[j, i].squeeze(), vmin=0, vmax=1, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.grid(False)\n",
    "        title = 'original' if j == 0 else f'nb top_k: {nb_k}'\n",
    "        if type(class_selected) == int: title += '\\n{}: {:.0f}%'.format(class_selected, 100*class_softmax_all[j, i])\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### images vs. distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 10\n",
    "top_k = 10\n",
    "\n",
    "\n",
    "\n",
    "# Top perfo directions \n",
    "# NORMALIZED\n",
    "perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "# ORIGINAL\n",
    "# perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "# STYLESPACE\n",
    "# s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "# perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "top_k_dims = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "\n",
    "dist_wellclassified2center = torch.cdist(s_wellclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "dist_misclassified2center = torch.cdist(s_misclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "axs[0].hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('distance from well-classified center using top perfo dimensions')\n",
    "axs[1].plot(distance, accuracy)\n",
    "axs[1].set_xlabel('distance from well-classified center using top perfo dimensions')\n",
    "axs[1].set_ylabel('accuracy [%]')\n",
    "\n",
    "\n",
    "\n",
    "# SAMPLE IMAGES\n",
    "distances_low2high = dist_wellclassified2center.squeeze().argsort()\n",
    "idx_images = distances_low2high[np.random.randint(1000, size=n_images)]\n",
    "\n",
    "s = s_wellclassified[idx_images, :]\n",
    "s_digits = digits_wellclassified[idx_images]\n",
    "\n",
    "\n",
    "strengths = np.linspace(0, 3, num=6)\n",
    "imgs_plot = torch.empty((len(strengths), n_images, 1, 28, 28))\n",
    "distances = []\n",
    "confidences = []\n",
    "\n",
    "for i, strength in enumerate(strengths):\n",
    "    # manipulate images\n",
    "    s_shifted = s.clone()\n",
    "    for k in top_k_dims:\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_shifted[:, k] += weight_shift\n",
    "\n",
    "    imgs = generate_img_from_s(s_shifted)\n",
    "    imgs = postprocess_images(imgs)\n",
    "\n",
    "    digits_pred = classifier_digits(imgs)\n",
    "    class_logit = digits_pred[torch.arange(n_images), s_digits]\n",
    "    class_softmax = F.softmax(digits_pred, dim=1)[torch.arange(n_images), s_digits]\n",
    "\n",
    "    imgs_plot[i] = imgs\n",
    "    distances.append(torch.cdist(s_shifted[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy())\n",
    "    confidences.append(class_softmax.detach().cpu().numpy())\n",
    "\n",
    "fig, axs = plt.subplots(n_images, imgs_plot.shape[0], figsize=(12, 20))\n",
    "for i in range(n_images): # for each image\n",
    "    for j in range(imgs_plot.shape[0]):\n",
    "        ax = axs[i, j]\n",
    "        ax.imshow(imgs_plot[j, i].squeeze(), vmin=0, vmax=1, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.grid(False)\n",
    "        title = 'd={:.1f} ; c={:.0f}%'.format(distances[j][i, 0], 100*confidences[j][i])\n",
    "        ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize(32),\n",
    "    torchvision.transforms.Lambda(lambda x: 255*x)\n",
    "])\n",
    "mnist_test = torchvision.datasets.MNIST(root='../data', train=False, transform=transforms)\n",
    "\n",
    "idx = 3\n",
    "img = mnist_test[idx][0]\n",
    "digit = mnist_test[idx][1]\n",
    "\n",
    "digits_ = torch.tensor(digit, device=device)\n",
    "c = F.one_hot(digits_, G.c_dim)   \n",
    "\n",
    "# project\n",
    "projected_w_steps = project(G, img, c, device=device)\n",
    "projected_w = projected_w_steps[-1][0].unsqueeze(0)\n",
    "\n",
    "# save video\n",
    "outdir = 'proj_out'\n",
    "\n",
    "target_pil = torchvision.transforms.ToPILImage()(img.repeat((3, 1, 1))/255)\n",
    "target_uint8 = np.array(target_pil, dtype=np.uint8)\n",
    "\n",
    "# Render debug output: optional video and projected image and W vector.\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "video = imageio.get_writer(f'{outdir}/proj.mp4', mode='I', fps=10, codec='libx264', bitrate='16M')\n",
    "print (f'Saving optimization progress video \"{outdir}/proj.mp4\"')\n",
    "for projected_w in projected_w_steps:\n",
    "    synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n",
    "    synth_image = synth_image.repeat((1, 3, 1, 1))\n",
    "    synth_image = (synth_image + 1) * (255/2)\n",
    "    synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "    video.append_data(np.concatenate([target_uint8, synth_image], axis=1))\n",
    "video.close()\n",
    "\n",
    "# Save final projected frame and W vector.\n",
    "target_pil.save(f'{outdir}/target.png')\n",
    "projected_w = projected_w_steps[-1]\n",
    "synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n",
    "synth_image = synth_image.repeat((1, 3, 1, 1))\n",
    "synth_image = (synth_image + 1) * (255/2)\n",
    "synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "PIL.Image.fromarray(synth_image, 'RGB').save(f'{outdir}/proj.png')\n",
    "np.savez(f'{outdir}/projected_w.npz', w=projected_w.unsqueeze(0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_target = (img.unsqueeze(0)).to(device)\n",
    "x_target = x_target[:, :, 2:30, 2:30] / 255\n",
    "print(classifier_digits(x_target))\n",
    "\n",
    "x_proj = torch.from_numpy(synth_image[:, :, 0]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "x_proj = x_proj[:, :, 2:30, 2:30] / 255\n",
    "print(classifier_digits(x_proj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read projected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'mnistTest_stylegan2_blur_noise_maxSeverity3_proba50'\n",
    "# dataset = 'mnistTest_stylegan2'\n",
    "\n",
    "file = path_results / 'projected_data' / f'{dataset}_model00016_all.npz'\n",
    "projected_data = np.load(file)\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(root='../data', train=False)\n",
    "\n",
    "assert all(projected_data['indices'] == np.arange(len(mnist_test)))\n",
    "assert all(projected_data['digits'] == mnist_test.targets.cpu().numpy())\n",
    "\n",
    "# projected_data['indices']\n",
    "projected_w = torch.from_numpy(projected_data['projected_w']).to(device)\n",
    "digits = torch.from_numpy(projected_data['digits']).to(device)\n",
    "# projected_w[0]\n",
    "\n",
    "\n",
    "# PREDICT CLASSES OF PROJECTED DATA\n",
    "\n",
    "s_proj_all = None\n",
    "class_predicted = None\n",
    "batch_size = 64\n",
    "for w, labels in zip(DataLoader(projected_w, batch_size=batch_size), DataLoader(digits, batch_size=batch_size)):\n",
    "    batch_size_t = len(labels)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # compute output\n",
    "        imgs = G.synthesis(w.unsqueeze(1).repeat((1, G.num_ws, 1)), noise_mode='const', force_fp32=True)\n",
    "        imgs = postprocess_images(imgs)\n",
    "\n",
    "        digits_pred = classifier_digits(imgs)\n",
    "        class_pred_t = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "        class_softmax = F.softmax(digits_pred, dim=1)[torch.arange(batch_size_t), labels]\n",
    "        \n",
    "    # style values (styleSpace_values from hook)\n",
    "    s_vec = styleSpace_dict2vec(styleSpace_values)\n",
    "\n",
    "    s_proj_all = s_vec if s_proj_all is None else torch.cat((s_proj_all, s_vec))\n",
    "    class_predicted = class_pred_t if class_predicted is None else torch.cat((class_predicted, class_pred_t))\n",
    "\n",
    "s_proj_wellclassified = s_proj_all[class_predicted == digits]\n",
    "s_proj_misclassified = s_proj_all[class_predicted != digits]\n",
    "\n",
    "\n",
    "# PREDICT CLASSES OF ORIGINAL DATA AND COMPARE WITH PROJECTED DATA\n",
    "\n",
    "path_data = Path.cwd().parent / 'data/MNIST' / f'{dataset}.zip'\n",
    "ds = ImageFolderDataset(path_data, use_labels=True)\n",
    "\n",
    "class_predicted_mnist = None\n",
    "for x, y in DataLoader(ds, batch_size=64):\n",
    "    x = (x / 255)[:, :, 2:30, 2:30]\n",
    "    digits_pred = classifier_digits(x.to(device))\n",
    "    class_pred_t = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "    class_predicted_mnist = class_pred_t if class_predicted_mnist is None else torch.cat((class_predicted_mnist, class_pred_t))\n",
    "\n",
    "tp_idx = class_predicted[class_predicted == digits] == class_predicted_mnist[class_predicted == digits]\n",
    "tp = tp_idx.sum()\n",
    "tn_idx = class_predicted[class_predicted != digits] == class_predicted_mnist[class_predicted != digits]\n",
    "tn = tn_idx.sum()\n",
    "fp_idx = class_predicted[class_predicted == digits] != class_predicted_mnist[class_predicted == digits]\n",
    "fp = fp_idx.sum()\n",
    "fn_idx = class_predicted[class_predicted != digits] != class_predicted_mnist[class_predicted != digits]\n",
    "fn = fn_idx.sum()\n",
    "\n",
    "assert tp + tn + fp + fn == len(class_predicted)\n",
    "\n",
    "print('True positive:', tp.item())\n",
    "print('True negative:', tn.item())\n",
    "print('False positive:', fp.item())\n",
    "print('False negative:', fn.item())\n",
    "\n",
    "\n",
    "# FILTER OUT WRONG SAMPLES (FALSE POSITIVE OR NEGATIVE)\n",
    "s_proj_wellclassified_fp = s_proj_wellclassified[fp_idx]\n",
    "s_proj_misclassified_fn = s_proj_misclassified[fn_idx]\n",
    "s_proj_wellclassified = s_proj_wellclassified[tp_idx]\n",
    "s_proj_misclassified = s_proj_misclassified[tn_idx]\n",
    "\n",
    "\n",
    "# PLOT SAMPLES\n",
    "n_images = 8\n",
    "plot_images_from_s(s_proj_wellclassified[:n_images])\n",
    "plt.title('well-classified (correct)')\n",
    "\n",
    "plot_images_from_s(s_proj_wellclassified_fp[:n_images])\n",
    "plt.title('well-classified (wrong)')\n",
    "\n",
    "plot_images_from_s(s_proj_misclassified[:n_images])\n",
    "plt.title('misclassified (correct)')\n",
    "\n",
    "plot_images_from_s(s_proj_misclassified_fn[:n_images])\n",
    "plt.title('misclassified (wrong)')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(mnist_test[3][0], cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "\n",
    "plot_images_from_s(s_proj_all[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellclassified_center = s_wellclassified.mean(0).unsqueeze(0)\n",
    "\n",
    "# All directions\n",
    "dist_wellclassified2center = torch.cdist(s_proj_wellclassified, wellclassified_center).cpu().numpy()\n",
    "dist_misclassified2center = torch.cdist(s_proj_misclassified, wellclassified_center).cpu().numpy()\n",
    "\n",
    "hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "axs[0].hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('distance from well-classified center using all dimensions')\n",
    "axs[1].plot(distance, accuracy)\n",
    "axs[1].set_xlabel('distance from well-classified center using all dimensions')\n",
    "axs[1].set_ylabel('accuracy [%]')\n",
    "\n",
    "# Top perfo directions \n",
    "# NORMALIZED\n",
    "perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "# ORIGINAL\n",
    "# perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "# STYLESPACE\n",
    "# s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "# perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "top_k_dims = (-np.abs(perfo_direction)).argsort()[:10]\n",
    "\n",
    "\n",
    "dist_wellclassified2center = torch.cdist(s_proj_wellclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "dist_misclassified2center = torch.cdist(s_proj_misclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "axs[0].hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('distance from well-classified center using top perfo dimensions')\n",
    "axs[1].plot(distance, accuracy)\n",
    "axs[1].set_xlabel('distance from well-classified center using top perfo dimensions')\n",
    "axs[1].set_ylabel('accuracy [%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move projected image to the well-classified center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 8\n",
    "s = s_proj_misclassified[:n_images]\n",
    "wellclassified_center\n",
    "\n",
    "s_ = truncate(s, wellclassified_center, 0.9)\n",
    "\n",
    "plot_images_from_s(s_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 20\n",
    "top_k_idxs = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "strength = 10\n",
    "\n",
    "s = s_proj_wellclassified[:n_images]\n",
    "s_shifted = s.clone()\n",
    "\n",
    "for k in top_k_idxs:\n",
    "    positive_direction = perfo_direction[k] >= 0\n",
    "    # positive_direction = not(positive_direction)\n",
    "    d = 1 if positive_direction else -1\n",
    "    weight_shift = d * strength * style_std_vec[k]\n",
    "    s_shifted[:, k] += weight_shift\n",
    "\n",
    "\n",
    "n_images = 8\n",
    "imgs = generate_img_from_s(s)\n",
    "imgs = postprocess_images(imgs)\n",
    "plot_images(imgs)\n",
    "plt.title('well classified samples')\n",
    "\n",
    "imgs = generate_img_from_s(s_shifted)\n",
    "imgs = postprocess_images(imgs)\n",
    "plot_images(imgs)\n",
    "plt.title('well classified samples corrupted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 15\n",
    "BIGGER_SIZE = 15\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### samples real vs generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 8\n",
    "\n",
    "# REAL IMAGES\n",
    "\n",
    "dataset = 'mnist_stylegan2_blur_noise_maxSeverity3_proba50'\n",
    "path_data = Path.cwd().parent / 'data/MNIST' / f'{dataset}.zip'\n",
    "ds = ImageFolderDataset(path_data, use_labels=True)\n",
    "\n",
    "rand_idx = np.random.randint(len(ds), size=n_images)\n",
    "\n",
    "real_images = None\n",
    "for i in rand_idx:\n",
    "    img = torch.tensor(ds[i][0][:, 2:30, 2:30]).unsqueeze(0)\n",
    "    real_images = img if real_images is None else torch.cat((real_images, img))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(vutils.make_grid(real_images.cpu(), pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "plt.axis('off')\n",
    "plt.grid(False)\n",
    "\n",
    "\n",
    "# GENERATED IMAGES\n",
    "\n",
    "z = torch.randn([n_images, G.z_dim], device=device)    # latent codes\n",
    "if conditional:\n",
    "    digits_ = torch.randint(0, G.c_dim, (n_images,), device=device)\n",
    "    c = F.one_hot(digits_, G.c_dim)          \n",
    "else:\n",
    "    c = None\n",
    "\n",
    "ws = G.mapping(z, c, truncation_psi=1)\n",
    "img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "img = postprocess_images(img)\n",
    "plot_images(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(z_embedded[wellclassified, 0], z_embedded[wellclassified, 1], c='C0', label='well-classified', alpha=0.2)\n",
    "plt.scatter(z_embedded[np.logical_not(wellclassified), 0], z_embedded[np.logical_not(wellclassified), 1], c='C1', label='misclassified', alpha=0.2)\n",
    "plt.legend(loc=\"lower left\", fontsize='x-large')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_results / 'figures' / 'tsne_z')\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(w_embedded[wellclassified, 0], w_embedded[wellclassified, 1], c='C0', label='well-classified', alpha=0.2)\n",
    "plt.scatter(w_embedded[np.logical_not(wellclassified), 0], w_embedded[np.logical_not(wellclassified), 1], c='C1', label='misclassified', alpha=0.2)\n",
    "plt.legend(loc=\"lower left\", fontsize='x-large')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_results / 'figures' / 'tsne_w')\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(s_embedded[wellclassified, 0], s_embedded[wellclassified, 1], c='C0', label='well-classified', alpha=0.2)\n",
    "plt.scatter(s_embedded[np.logical_not(wellclassified), 0], s_embedded[np.logical_not(wellclassified), 1], c='C1', label='misclassified', alpha=0.2)\n",
    "plt.legend(loc=\"lower left\", fontsize='x-large')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_results / 'figures' / 'tsne_s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### histograms dimensions S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 3\n",
    "fig, axs = plt.subplots(1, top_k, figsize=(13, 4))\n",
    "for i in range(top_k):\n",
    "    k = (-np.abs(perfo_direction)).argsort()[i]\n",
    "    \n",
    "    # axs[i].set_title(r'$s_{' + str(k) + '}$')\n",
    "    axs[i].hist(s_wellclassified[:, k].cpu().numpy(), bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "    axs[i].hist(s_misclassified[:, k].cpu().numpy(), bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "    axs[i].set_ylabel('number of samples')\n",
    "    axs[i].set_xlabel(r'value for dimension $s_{' + str(k) + '}$')\n",
    "    # axs[i].axvline(style_min_vec[k].cpu().numpy(), color='k', ls='--')\n",
    "    # plt.text(1.1*style_min_vec[k].cpu().numpy(), 100, 'empirical min',rotation=90)\n",
    "    # axs[i].axvline(style_max_vec[k].cpu().numpy(), color='k', ls='--')\n",
    "    # plt.text(1.1*style_max_vec[k].cpu().numpy(), 100, 'empirical max',rotation=90)\n",
    "    axs[i].legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_results / 'figures' / 'hist_top_dims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_idx = np.random.randint(len(perfo_direction), size=3)\n",
    "\n",
    "fig, axs = plt.subplots(1, top_k, figsize=(13, 4))\n",
    "for i, idx in enumerate(rnd_idx):\n",
    "    k = (-np.abs(perfo_direction)).argsort()[idx]\n",
    "    \n",
    "    # axs[i].set_title(r'$s_{' + str(k) + '}$')\n",
    "    axs[i].hist(s_wellclassified[:, k].cpu().numpy(), bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "    axs[i].hist(s_misclassified[:, k].cpu().numpy(), bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "    axs[i].set_ylabel('number of samples')\n",
    "    axs[i].set_xlabel(r'value for dimension $s_{' + str(k) + '}$')\n",
    "    # axs[i].axvline(style_min_vec[k].cpu().numpy(), color='k', ls='--')\n",
    "    # plt.text(1.1*style_min_vec[k].cpu().numpy(), 100, 'empirical min',rotation=90)\n",
    "    # axs[i].axvline(style_max_vec[k].cpu().numpy(), color='k', ls='--')\n",
    "    # plt.text(1.1*style_max_vec[k].cpu().numpy(), 100, 'empirical max',rotation=90)\n",
    "    axs[i].legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_results / 'figures' / 'hist_random_dims')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy vs distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = 0.7\n",
    "top_k_list = [10, 100, 2000]\n",
    "\n",
    "wellclassified_center = s_wellclassified.mean(0).unsqueeze(0)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel('(normalized) distance from well-classified center')\n",
    "plt.ylabel('accuracy [%]')\n",
    "\n",
    "for top_k in top_k_list:\n",
    "    # Top perfo directions \n",
    "    # NORMALIZED\n",
    "    perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "    # ORIGINAL\n",
    "    # perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "    # STYLESPACE\n",
    "    # s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "    # perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "    top_k_dims = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "\n",
    "    dist_wellclassified2center = torch.cdist(s_wellclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "    dist_misclassified2center = torch.cdist(s_misclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "    # Normalize\n",
    "    dist_misclassified2center -= dist_wellclassified2center.min()\n",
    "    dist_wellclassified2center -= dist_wellclassified2center.min()\n",
    "    dist_misclassified2center /= cut * dist_wellclassified2center.max()\n",
    "    dist_wellclassified2center /= cut * dist_wellclassified2center.max()\n",
    "\n",
    "    hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "    hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "    distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "    accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "    plt.plot(distance[distance<1], accuracy[distance<1], label=f'using top {top_k} dimensions')\n",
    "    \n",
    "# All directions\n",
    "dist_wellclassified2center = torch.cdist(s_wellclassified, wellclassified_center).cpu().numpy()\n",
    "dist_misclassified2center = torch.cdist(s_misclassified, wellclassified_center).cpu().numpy()\n",
    "\n",
    "# Normalize\n",
    "dist_misclassified2center -= dist_wellclassified2center.min()\n",
    "dist_wellclassified2center -= dist_wellclassified2center.min()\n",
    "dist_misclassified2center /= cut * dist_wellclassified2center.max()\n",
    "dist_wellclassified2center /= cut * dist_wellclassified2center.max()\n",
    "\n",
    "hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "plt.plot(distance[distance<1], accuracy[distance<1], label='using all dimensions')\n",
    "plt.legend()\n",
    "plt.savefig(path_results / 'figures' / 'accuracy_vs_distance_wellclassified_center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "n_images = 8\n",
    "\n",
    "dist_all2center = torch.cdist(s_all, wellclassified_center).cpu().numpy()\n",
    "dist_all2center -= dist_all2center.min()\n",
    "dist_all2center /= cut * dist_all2center.max()\n",
    "\n",
    "for d in distances:\n",
    "\n",
    "    idx = (np.abs(dist_all2center - d) <= 0.01).squeeze()\n",
    "    s = s_all[idx][:n_images]\n",
    "\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_center = s_all.mean(0).unsqueeze(0)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel('(normalized) distance from global center')\n",
    "plt.ylabel('accuracy [%]')\n",
    "\n",
    "for top_k in top_k_list:\n",
    "    # Top perfo directions \n",
    "    # NORMALIZED\n",
    "    perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "    # ORIGINAL\n",
    "    # perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "    # STYLESPACE\n",
    "    # s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "    # perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "    top_k_dims = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "\n",
    "    dist_wellclassified2center = torch.cdist(s_wellclassified[:, top_k_dims], s_center[:, top_k_dims]).cpu().numpy()\n",
    "    dist_misclassified2center = torch.cdist(s_misclassified[:, top_k_dims], s_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "    # Normalize\n",
    "    dist_misclassified2center -= dist_wellclassified2center.min()\n",
    "    dist_wellclassified2center -= dist_wellclassified2center.min()\n",
    "    dist_misclassified2center /= cut * dist_wellclassified2center.max()\n",
    "    dist_wellclassified2center /= cut * dist_wellclassified2center.max()\n",
    "\n",
    "    hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "    hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "    distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "    accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "    plt.plot(distance[distance<1], accuracy[distance<1], label=f'using top {top_k} dimensions')\n",
    "    \n",
    "# All directions\n",
    "dist_wellclassified2center = torch.cdist(s_wellclassified, s_center).cpu().numpy()\n",
    "dist_misclassified2center = torch.cdist(s_misclassified, s_center).cpu().numpy()\n",
    "\n",
    "# Normalize\n",
    "dist_misclassified2center -= dist_wellclassified2center.min()\n",
    "dist_wellclassified2center -= dist_wellclassified2center.min()\n",
    "dist_misclassified2center /= cut * dist_wellclassified2center.max()\n",
    "dist_wellclassified2center /= cut * dist_wellclassified2center.max()\n",
    "\n",
    "hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "plt.plot(distance[distance<1], accuracy[distance<1], label='using all dimensions')\n",
    "plt.legend()\n",
    "plt.savefig(path_results / 'figures' / 'accuracy_vs_distance_global_center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image manipulation : visualize corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "strength = 5\n",
    "n_images = 8\n",
    "\n",
    "top_k_idxs = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "# ORIGINAL IMAGES\n",
    "imgs_orig = generate_img_from_s(s_wellclassified[:n_images])\n",
    "imgs_orig = postprocess_images(imgs_orig)\n",
    "digits_pred = classifier_digits(imgs_orig)\n",
    "class_logit = digits_pred[torch.arange(n_images), digits_wellclassified[:n_images]]\n",
    "class_softmax_orig = F.softmax(digits_pred, dim=1)[torch.arange(n_images), digits_wellclassified[:n_images]]\n",
    "\n",
    "# CORRUPT\n",
    "s_wellclassified_shifted = s_wellclassified[:n_images].clone()\n",
    "for k in top_k_idxs:\n",
    "    positive_direction = perfo_direction[k] >= 0\n",
    "    # positive_direction = not(positive_direction)\n",
    "    d = 1 if positive_direction else -1\n",
    "    weight_shift = d * strength * style_std_vec[k]\n",
    "    s_wellclassified_shifted[:, k] += weight_shift\n",
    "\n",
    "imgs_corr = generate_img_from_s(s_wellclassified_shifted)\n",
    "imgs_corr = postprocess_images(imgs_corr)\n",
    "digits_pred = classifier_digits(imgs_corr)\n",
    "class_logit = digits_pred[torch.arange(n_images), digits_wellclassified[:n_images]]\n",
    "class_softmax_corr = F.softmax(digits_pred, dim=1)[torch.arange(n_images), digits_wellclassified[:n_images]]\n",
    "\n",
    "# CLEAN\n",
    "s_wellclassified_shifted = s_wellclassified[:n_images].clone()\n",
    "for k in top_k_idxs:\n",
    "    positive_direction = perfo_direction[k] >= 0\n",
    "    positive_direction = not(positive_direction)\n",
    "    d = 1 if positive_direction else -1\n",
    "    weight_shift = d * strength * style_std_vec[k]\n",
    "    s_wellclassified_shifted[:, k] += weight_shift\n",
    "\n",
    "imgs_clean = generate_img_from_s(s_wellclassified_shifted)\n",
    "imgs_clean = postprocess_images(imgs_clean)\n",
    "digits_pred = classifier_digits(imgs_clean)\n",
    "class_logit = digits_pred[torch.arange(n_images), digits_wellclassified[:n_images]]\n",
    "class_softmax_clean = F.softmax(digits_pred, dim=1)[torch.arange(n_images), digits_wellclassified[:n_images]]\n",
    "\n",
    "# PLOT\n",
    "imgs_orig = imgs_orig * 255\n",
    "imgs_orig = imgs_orig.to(torch.uint8).cpu()\n",
    "imgs_corr = imgs_corr * 255\n",
    "imgs_corr = imgs_corr.to(torch.uint8).cpu()\n",
    "imgs_clean = imgs_clean * 255\n",
    "imgs_clean = imgs_clean.to(torch.uint8).cpu()\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 5))\n",
    "axs[0].imshow(vutils.make_grid(imgs_orig, pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('well-classified samples')\n",
    "axs[1].imshow(vutils.make_grid(imgs_corr, pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "axs[1].set_title(f'after corruption (top_k={top_k}; strength={strength})')\n",
    "axs[1].axis('off')\n",
    "axs[2].imshow(vutils.make_grid(imgs_clean, pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "axs[2].set_title(f'after cleaning (top_k={top_k}; strength={strength})')\n",
    "axs[2].axis('off')\n",
    "# plt.tight_layout()\n",
    "plt.savefig(path_results / 'figures' / 'image_manipulation', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(class_softmax_orig)\n",
    "print(class_softmax_corr)\n",
    "print(class_softmax_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### images vs distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf0e85ac5c104e7dc18f35eb0f3f17acfc5d19b9cd0ff3830c651a628ffe0833"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
