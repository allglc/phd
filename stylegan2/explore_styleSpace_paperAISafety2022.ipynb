{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import json\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import functools\n",
    "import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "from stylegan2_ada_pytorch.torch_utils import misc\n",
    "import stylegan2_ada_pytorch.dnnlib\n",
    "import stylegan2_ada_pytorch.legacy\n",
    "from stylegan2_ada_pytorch.projector import project\n",
    "from stylegan2_ada_pytorch.training.dataset import ImageFolderDataset\n",
    "from classifiers.models import CNN_MNIST\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# path_results = Path.cwd().parent / 'results'\n",
    "path_results = Path('w:/results/stylegan2')\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 15\n",
    "BIGGER_SIZE = 15\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFIER = 'clean'\n",
    "CLASSIFIER = 'corrupt'\n",
    "\n",
    "CLASS_SELECTED = 'all'\n",
    "# CLASS_SELECTED = 0\n",
    "\n",
    "# DIRECTION = 'normalized difference'\n",
    "DIRECTION = 'gradient'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_images(images):\n",
    "    assert images.dim() == 4, \"Expected 4D (B x C x H x W) image tensor, got {}D\".format(images.dim())\n",
    "    # lo, hi = [-1, 1] # generator scale\n",
    "    # images = (images - lo) * (255 / (hi - lo)) # classifier scale\n",
    "    # images = torch.round(images.clamp(0, 255))#.to(torch.uint8).to(torch.float)\n",
    "    # images = (images * 127.5 + 128).clamp(0, 255)\n",
    "    images = ((images + 1) / 2).clamp(0, 1)\n",
    "    images = images[:, :, 2:30, 2:30] # remove padding\n",
    "\n",
    "    return images\n",
    "\n",
    "def plot_images(images, title=''):\n",
    "    images = images * 255\n",
    "    images = images.to(torch.uint8)\n",
    "    plt.figure()\n",
    "    plt.imshow(vutils.make_grid(images.cpu(), pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.grid(False)\n",
    "    plt.title(title)\n",
    "\n",
    "\n",
    "def generate_from_z(z):\n",
    "    for i in np.arange(0, z.shape[0], batch_size):\n",
    "        img = G(z[i:i+batch_size], c=None, noise_mode='const', force_fp32=True)\n",
    "        if i == 0: \n",
    "            imgs = img\n",
    "        else:\n",
    "            imgs = torch.cat((imgs, img))\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def plot_random_images(imgs):\n",
    "    # from generate.py: img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "    # imgs = (imgs * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "    imgs = postprocess_images(imgs)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(vutils.make_grid(imgs[torch.randint(0, imgs.shape[0], (100,))].cpu(), pad_value=255, nrow=10).permute(1,2,0))\n",
    "\n",
    "def plot_images_from_s(s):\n",
    "    imgs = generate_img_from_s(s)\n",
    "    imgs = postprocess_images(imgs)\n",
    "    plot_images(imgs)\n",
    "    \n",
    "def truncate(x, x_avg, psi):\n",
    "    # psi=0 means we get average value, \n",
    "    # psi=1 we get original value, \n",
    "    # 0<psi<1 we get interpolation between mean and original\n",
    "    return x_avg.lerp(x, psi)\n",
    "\n",
    "\n",
    "def styleSpace_dict2vec(styleSpace_dict):\n",
    "    styleSpace_vec = []\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            key = f'b{res}.{layer}'\n",
    "            values = styleSpace_dict[key]\n",
    "            if values.dim() == 1: values = values.unsqueeze(0)\n",
    "            styleSpace_vec.append(values)\n",
    "    styleSpace_vec = torch.cat(styleSpace_vec, dim=1)\n",
    "    return styleSpace_vec\n",
    "\n",
    "\n",
    "def styleSpace_vec2dict(styleSpace_vec):\n",
    "    if styleSpace_vec.dim() == 1:\n",
    "        styleSpace_vec = styleSpace_vec.unsqueeze(0)\n",
    "    styleSpace_dict = {}\n",
    "    dim_base = 0\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        block = getattr(G.synthesis, f'b{res}')\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            block_layer = getattr(block, layer)\n",
    "            dim_size = block_layer.affine.weight.shape[1]\n",
    "            key = f'b{res}.{layer}'\n",
    "            styleSpace_dict[key] = styleSpace_vec[:, dim_base:dim_base+dim_size]#.squeeze()\n",
    "            dim_base += dim_size\n",
    "    assert dim_base == styleSpace_vec.shape[1]\n",
    "    return styleSpace_dict\n",
    "\n",
    "\n",
    "def compute_styleSpace_vec_idx2coord():\n",
    "    vec_idx2coord = {}\n",
    "    idx = 0\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        block = getattr(G.synthesis, f'b{res}')\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            block_layer = getattr(block, layer)\n",
    "            dim_size = block_layer.affine.weight.shape[1]\n",
    "            for dim in range(dim_size):\n",
    "                vec_idx2coord[idx] = (f'b{res}.{layer}', dim)\n",
    "                idx += 1\n",
    "    return vec_idx2coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqcat.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqdog.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqwild.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/brecahad.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl'\n",
    "# path_model = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl'\n",
    "\n",
    "# path_model = path_results / 'stylegan2-training-runs' / '00011-mnist_stylegan2_noise-cond-auto4-original'\n",
    "# path_model = path_results / 'stylegan2-training-runs' / '00015-mnist_stylegan2_blur_noise-cond-auto4'\n",
    "path_model = path_results / 'stylegan2-training-runs' / '00016-mnist_stylegan2_blur_noise_maxSeverity3_proba50-cond-auto4'\n",
    "\n",
    "# find best model in folder\n",
    "if not str(path_model).endswith('pkl'):\n",
    "    with open(path_model / 'metric-fid50k_full.jsonl', 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    best_fid = 1e6\n",
    "    for json_str in json_list:\n",
    "        json_line = json.loads(json_str)\n",
    "        if json_line['results']['fid50k_full'] < best_fid:\n",
    "            best_fid = json_line['results']['fid50k_full']\n",
    "            best_model = json_line['snapshot_pkl']\n",
    "    print('Best FID: {:.2f} ; best model : {}'.format(best_fid, best_model))\n",
    "    path_model = path_model / best_model\n",
    "\n",
    "    with open(path_model, 'rb') as f:\n",
    "        G = pickle.load(f)['G_ema'].to(device)  # torch.nn.Module\n",
    "\n",
    "else:\n",
    "    with dnnlib.util.open_url(path_model) as f:\n",
    "        G = legacy.load_network_pkl(f)['G_ema'].to(device)\n",
    "\n",
    "if device == 'cpu': G.forward = functools.partial(G.forward, force_fp32=True)\n",
    "\n",
    "conditional = G.c_dim > 0\n",
    "\n",
    "# registor hooks to save intermediate values (images and style space)\n",
    "intermediate_images_torgb = {}\n",
    "def get_torgb(name):\n",
    "    def hook(module, input, output):\n",
    "        intermediate_images_torgb[name] = output.detach()\n",
    "    return hook\n",
    "intermediate_images_block = {}\n",
    "def get_block_img(name):\n",
    "    def hook(module, input, output):\n",
    "        intermediate_images_block[name] = output[1].detach()\n",
    "    return hook\n",
    "styleSpace_values = {}\n",
    "def get_styleSpace_values(name):\n",
    "    def hook(module, input, output):\n",
    "        styleSpace_values[name] = output.detach()\n",
    "    return hook\n",
    "for res in G.synthesis.block_resolutions:\n",
    "    block = getattr(G.synthesis, f'b{res}')\n",
    "    block.torgb.register_forward_hook(get_torgb(res))\n",
    "    block.register_forward_hook(get_block_img(res))\n",
    "    for layer in ['conv0', 'conv1', 'torgb']:\n",
    "        if res == 4 and layer == 'conv0': continue\n",
    "        block_layer = getattr(block, layer)\n",
    "        block_layer.affine.register_forward_hook(get_styleSpace_values(name=f'b{res}.{layer}'))\n",
    "\n",
    "        \n",
    "# backward hooks to get gradients relative to styleSpace\n",
    "styleSpace_grads = {}\n",
    "def get_styleSpace_grads(name):\n",
    "    def hook(self, grad_input, grad_output):\n",
    "        styleSpace_grads[name] = grad_output[0].detach()\n",
    "    return hook\n",
    "\n",
    "for res in G.synthesis.block_resolutions:\n",
    "    block = getattr(G.synthesis, f'b{res}')\n",
    "    block.torgb.register_forward_hook(get_torgb(res))\n",
    "    block.register_forward_hook(get_block_img(res))\n",
    "    for layer in ['conv0', 'conv1', 'torgb']:\n",
    "        if res == 4 and layer == 'conv0': continue\n",
    "        block_layer = getattr(block, layer)\n",
    "        block_layer.affine.register_full_backward_hook(get_styleSpace_grads(name=f'b{res}.{layer}'))\n",
    "\n",
    "        \n",
    "# dict to convert index to coordinate for stylespace vectors\n",
    "styleSpace_vec_idx2coord = compute_styleSpace_vec_idx2coord()\n",
    "\n",
    "\n",
    "# function to move a given style dimension\n",
    "def generate_img_new_style(ws, block_layer_name, index=0, direction=1):\n",
    "    def move_style(index, direction):\n",
    "        def hook(module, input, output):\n",
    "            output[:, index] += direction\n",
    "            return output\n",
    "        return hook\n",
    "\n",
    "    block_name, layer_name = block_layer_name.split('.')\n",
    "    block = getattr(G.synthesis, block_name)\n",
    "    block_layer = getattr(block, layer_name)\n",
    "    handle = block_layer.affine.register_forward_hook(move_style(index, direction))\n",
    "\n",
    "    if ws.dim() == 2:\n",
    "        ws = ws.unsqueeze(1).repeat((1, G.num_ws, 1))\n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    return img\n",
    "    \n",
    "    \n",
    "# function to modify a given style dimension\n",
    "def generate_img_new_style2(ws, block_layer_name, index, s_style_min, s_style_max, s_shift=1, positive_direction=True):\n",
    "    def move_style(index, weight_shift):\n",
    "        def hook(module, input, output):\n",
    "            output[:, index] += weight_shift\n",
    "            return output\n",
    "        return hook\n",
    "    \n",
    "    assert type(index) == int, 'Function only works for 1 style'\n",
    "    assert ws.shape[0] == 1, 'Works only for 1 image' # orig_value only for 1 image\n",
    "    \n",
    "    if ws.dim() == 2:\n",
    "        ws = ws.unsqueeze(1).repeat((1, G.num_ws, 1))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        G.synthesis(ws, noise_mode='const', force_fp32=True) # first pass to get style vector from hook\n",
    "    orig_value = styleSpace_values[block_layer_name][0, index]\n",
    "    target_value = (s_style_max if positive_direction else s_style_min)\n",
    "    weight_shift = s_shift * (target_value - orig_value)\n",
    "\n",
    "    block_name, layer_name = block_layer_name.split('.')\n",
    "    block = getattr(G.synthesis, block_name)\n",
    "    block_layer = getattr(block, layer_name)\n",
    "    handle = block_layer.affine.register_forward_hook(move_style(index, weight_shift))\n",
    "    \n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# function to modify a given style dimension\n",
    "def generate_img_new_style3(ws, block_layer_name, index, s_std, strength=5, positive_direction=True):\n",
    "    def move_style(index, weight_shift):\n",
    "        def hook(module, input, output):\n",
    "            output[:, index] += weight_shift\n",
    "            return output\n",
    "        return hook\n",
    "    \n",
    "    assert type(index) == int, 'Function only works for 1 style'\n",
    "    assert ws.shape[0] == 1, 'Works only for 1 image'\n",
    "    \n",
    "    if ws.dim() == 2:\n",
    "        ws = ws.unsqueeze(1).repeat((1, G.num_ws, 1))\n",
    "    \n",
    "    d = 1 if positive_direction else -1\n",
    "    weight_shift = d * strength * s_std\n",
    "\n",
    "    block_name, layer_name = block_layer_name.split('.')\n",
    "    block = getattr(G.synthesis, block_name)\n",
    "    block_layer = getattr(block, layer_name)\n",
    "    handle = block_layer.affine.register_forward_hook(move_style(index, weight_shift))\n",
    "    \n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# function to generate image from S\n",
    "def generate_img_from_s(s):\n",
    "    def set_style(values):\n",
    "        def hook(module, input, output):\n",
    "            output = values\n",
    "            return output\n",
    "        return hook\n",
    "    \n",
    "    if type(s) != dict: s = styleSpace_vec2dict(s)\n",
    "    assert s['b4.conv1'].dim() == 2, 'Should be of 2 dimensions: batch_size x s_dim'\n",
    "    batch_size = s['b4.conv1'].shape[0]\n",
    "    \n",
    "    handles = []\n",
    "    for res in G.synthesis.block_resolutions:\n",
    "        block = getattr(G.synthesis, f'b{res}')\n",
    "        for layer in ['conv0', 'conv1', 'torgb']:\n",
    "            if res == 4 and layer == 'conv0': continue\n",
    "            block_layer = getattr(block, layer)\n",
    "            values = s[f'b{res}.{layer}']\n",
    "            handles.append(block_layer.affine.register_forward_hook(set_style(values)))\n",
    "    \n",
    "    dummy_ws = torch.zeros((batch_size, G.num_ws, G.w_dim), device=device)\n",
    "    img = G.synthesis(dummy_ws, noise_mode='const', force_fp32=True)\n",
    "\n",
    "    for h in handles: h.remove()\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 5\n",
    "z = torch.randn([n_images, G.z_dim], device=device)    # latent codes\n",
    "if conditional:\n",
    "    digits = torch.randint(0, G.c_dim, (n_images,), device=device)\n",
    "    c = F.one_hot(digits, G.c_dim)          \n",
    "else:\n",
    "    c = None\n",
    "misc.print_module_summary(G, [z, c])\n",
    "\n",
    "ws = G.mapping(z, c, truncation_psi=1)\n",
    "img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "img = postprocess_images(img)\n",
    "plot_images(img, title='original')\n",
    "\n",
    "\n",
    "img_ = generate_img_new_style(ws, block_layer_name='b4.conv1', index=3, direction=-1)\n",
    "img_ = postprocess_images(img_)\n",
    "plot_images(img_, title='with new style')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict digits\n",
    "classifier_digits = CNN_MNIST(output_dim=10).to(device)\n",
    "# classifier_digits.load_state_dict(torch.load(path_results / 'classifiers' / 'CNN_MNIST_weights_20220411_0826.pth', map_location=device)) # Confiance\n",
    "\n",
    "if CLASSIFIER == 'clean':\n",
    "    classifier_digits.load_state_dict(torch.load(path_results / 'classifiers' / 'CNN_MNIST_weights_20220210_1601.pth', map_location=device))\n",
    "elif CLASSIFIER == 'corrupt':\n",
    "    classifier_digits.load_state_dict(torch.load(path_results / 'classifiers' / 'CNN_mnist_stylegan2_blur_noise_maxSeverity3_proba50_20220510_1124.pth', map_location=device))\n",
    "classifier_digits.eval()\n",
    "\n",
    "# predict noise\n",
    "classifier_noise = CNN_MNIST(output_dim=6).to(device)\n",
    "# classifier_noise.load_state_dict(torch.load(path_results / 'classifiers' / 'CNN_noise_MNIST_weights_20220411_0841.pth', map_location=device)) # Confiance\n",
    "classifier_noise.load_state_dict(torch.load(path_results / 'classifiers' / 'CNN_MNIST_noise_weights_20220210_1728.pth', map_location=device))\n",
    "classifier_noise.eval()\n",
    "\n",
    "imgs = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "imgs = postprocess_images(imgs)\n",
    "digit_pred = classifier_digits(imgs).argmax(dim=1).cpu()\n",
    "noise_pred = classifier_noise(imgs).argmax(dim=1).cpu()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(min(n_images, 5)):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(imgs[i].cpu().squeeze(), cmap='gray')\n",
    "    plt.title(f'digit: {digit_pred[i].numpy()} \\n noise: {noise_pred[i].numpy()}')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## misclassified vs. well classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_images = 10000\n",
    "batch_size = 32\n",
    "\n",
    "if CLASS_SELECTED == 'all': \n",
    "    digits = torch.randint(0, G.c_dim, (n_images,), dtype=torch.int64, device=device)\n",
    "else:\n",
    "    digits = CLASS_SELECTED*torch.ones((n_images, ), dtype=torch.int64, device=device)\n",
    "\n",
    "z_all = None\n",
    "w_all = None\n",
    "s_all = None\n",
    "s_grads_all = None\n",
    "class_predicted = None\n",
    "for labels in DataLoader(digits, batch_size):\n",
    "    batch_size_t = len(labels)\n",
    "    z = torch.randn([batch_size_t, G.z_dim], device=device)    # sample latent codes\n",
    "    c = F.one_hot(labels, G.c_dim)\n",
    "\n",
    "    # compute input\n",
    "    w = G.mapping(z, c, truncation_psi=1, truncation_cutoff=8)[:, 0, :] # keep only the first element\n",
    "    w.requires_grad = True\n",
    "\n",
    "    # compute output\n",
    "    imgs = G.synthesis(w.unsqueeze(1).repeat((1, G.num_ws, 1)), noise_mode='const', force_fp32=True)\n",
    "    imgs = postprocess_images(imgs)\n",
    "\n",
    "    digits_pred = classifier_digits(imgs)\n",
    "    class_logit = digits_pred[torch.arange(batch_size_t), labels]\n",
    "    class_pred_t = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "    class_softmax = F.softmax(digits_pred, dim=1)[torch.arange(batch_size_t), labels]\n",
    "        \n",
    "    # style values (styleSpace_values from hook)\n",
    "    s_vec = styleSpace_dict2vec(styleSpace_values)\n",
    "    \n",
    "    # backpropagate gradient to get its values (styleSpace_grads from hook)\n",
    "    # loss = class_logit.mean() # goal is to reduce the score for the class\n",
    "    loss = class_softmax.mean() # goal is to reduce the score for the class, SEEMS TO WORK BETTER THAN USING LOGIT\n",
    "    loss.backward() # compute gradients and access them thanks to the backward hook\n",
    "    s_grads_vec = styleSpace_dict2vec(styleSpace_grads)\n",
    "\n",
    "    z_all = z if z_all is None else torch.cat((z_all, z))\n",
    "    w_all = w if w_all is None else torch.cat((w_all, w))\n",
    "    s_all = s_vec if s_all is None else torch.cat((s_all, s_vec))\n",
    "    s_grads_all = s_grads_vec if s_grads_all is None else torch.cat((s_grads_all, s_grads_vec))\n",
    "    class_predicted = class_pred_t if class_predicted is None else torch.cat((class_predicted, class_pred_t))\n",
    "\n",
    "z_wellclassified = z_all[class_predicted == digits]\n",
    "z_misclassified = z_all[class_predicted != digits]\n",
    "w_wellclassified = w_all[class_predicted == digits]\n",
    "w_misclassified = w_all[class_predicted != digits]\n",
    "s_wellclassified = s_all[class_predicted == digits]\n",
    "s_misclassified = s_all[class_predicted != digits]\n",
    "digits_wellclassified = digits[class_predicted == digits]\n",
    "digits_misclassified = digits[class_predicted != digits]\n",
    "\n",
    "\n",
    "style_min_vec = s_all.min(dim=0).values\n",
    "style_min = styleSpace_vec2dict(style_min_vec)\n",
    "style_max_vec = s_all.max(dim=0).values\n",
    "style_max = styleSpace_vec2dict(style_max_vec)\n",
    "style_std_vec = s_all.std(dim=0)\n",
    "\n",
    "print('Accuracy: {:.2f}% ; {}/{} misclassified samples'.format(100 * w_wellclassified.shape[0] / (w_wellclassified.shape[0] + w_misclassified.shape[0]), w_misclassified.shape[0], (w_wellclassified.shape[0] + w_misclassified.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples = 10000\n",
    "\n",
    "# z_embedded = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(z_all[:n_samples].cpu().numpy())\n",
    "# w_embedded = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(w_all[:n_samples].cpu().numpy())\n",
    "# s_embedded = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(s_all[:n_samples].cpu().numpy())\n",
    "\n",
    "# wellclassified = (class_predicted == digits)[:n_samples].cpu().numpy()\n",
    "\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# plt.scatter(z_embedded[wellclassified, 0], z_embedded[wellclassified, 1], c='C0', label='well-classified', alpha=0.2)\n",
    "# plt.scatter(z_embedded[np.logical_not(wellclassified), 0], z_embedded[np.logical_not(wellclassified), 1], c='C1', label='misclassified', alpha=0.2)\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(path_results / 'figures' / 'tsne_z')\n",
    "\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# plt.scatter(w_embedded[wellclassified, 0], w_embedded[wellclassified, 1], c='C0', label='well-classified', alpha=0.2)\n",
    "# plt.scatter(w_embedded[np.logical_not(wellclassified), 0], w_embedded[np.logical_not(wellclassified), 1], c='C1', label='misclassified', alpha=0.2)\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(path_results / 'figures' / 'tsne_w')\n",
    "\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# plt.scatter(s_embedded[wellclassified, 0], s_embedded[wellclassified, 1], c='C0', label='well-classified', alpha=0.2)\n",
    "# plt.scatter(s_embedded[np.logical_not(wellclassified), 0], s_embedded[np.logical_not(wellclassified), 1], c='C1', label='misclassified', alpha=0.2)\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(path_results / 'figures' / 'tsne_s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_images = 8\n",
    "\n",
    "# imgs = generate_img_from_s(s_wellclassified[:n_images])\n",
    "# imgs = postprocess_images(imgs)\n",
    "# plot_images(imgs)\n",
    "# plt.title('well-classified')\n",
    "\n",
    "# imgs = generate_img_from_s(s_misclassified[:n_images])\n",
    "# imgs = postprocess_images(imgs)\n",
    "# plot_images(imgs)\n",
    "# plt.title('misclassified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real vs. generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_images = 8\n",
    "\n",
    "# # REAL IMAGES\n",
    "\n",
    "# dataset = 'mnist_stylegan2_blur_noise_maxSeverity3_proba50'\n",
    "# path_data = Path.cwd().parent / 'data/MNIST' / f'{dataset}.zip'\n",
    "# ds = ImageFolderDataset(path_data, use_labels=True)\n",
    "\n",
    "# rand_idx = np.random.randint(len(ds), size=n_images)\n",
    "\n",
    "# real_images = None\n",
    "# for i in rand_idx:\n",
    "#     img = torch.tensor(ds[i][0][:, 2:30, 2:30]).unsqueeze(0)\n",
    "#     real_images = img if real_images is None else torch.cat((real_images, img))\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(vutils.make_grid(real_images.cpu(), pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "# plt.axis('off')\n",
    "# plt.grid(False)\n",
    "\n",
    "\n",
    "# # GENERATED IMAGES\n",
    "\n",
    "# z = torch.randn([n_images, G.z_dim], device=device)    # latent codes\n",
    "# if conditional:\n",
    "#     digits_ = torch.randint(0, G.c_dim, (n_images,), device=device)\n",
    "#     c = F.one_hot(digits_, G.c_dim)          \n",
    "# else:\n",
    "#     c = None\n",
    "\n",
    "# ws = G.mapping(z, c, truncation_psi=1)\n",
    "# img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "# img = postprocess_images(img)\n",
    "# plot_images(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 100\n",
    "wellclassified_center = s_wellclassified.mean(0).unsqueeze(0)\n",
    "\n",
    "# All directions\n",
    "dist_wellclassified2center = torch.cdist(s_wellclassified, wellclassified_center).cpu().numpy()\n",
    "dist_misclassified2center = torch.cdist(s_misclassified, wellclassified_center).cpu().numpy()\n",
    "\n",
    "hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "axs[0].hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('distance from well-classified center using all dimensions')\n",
    "axs[1].plot(distance, accuracy)\n",
    "axs[1].set_xlabel('distance from well-classified center using all dimensions')\n",
    "axs[1].set_ylabel('accuracy [%]')\n",
    "\n",
    "# Top perfo directions \n",
    "## NORMALIZED\n",
    "perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "## ORIGINAL\n",
    "# perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "## STYLESPACE\n",
    "# s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "# perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "\n",
    "top_k_dims = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "\n",
    "dist_wellclassified2center = torch.cdist(s_wellclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "dist_misclassified2center = torch.cdist(s_misclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "axs[0].hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('distance from well-classified center using top perfo dimensions')\n",
    "axs[1].plot(distance, accuracy)\n",
    "axs[1].set_xlabel('distance from well-classified center using top perfo dimensions')\n",
    "axs[1].set_ylabel('accuracy [%]')\n",
    "\n",
    "# Top softmax directions\n",
    "perfo_direction = (- s_grads_all.mean(axis=0) * style_std_vec).cpu().numpy()\n",
    "top_k_dims = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "dist_wellclassified2center = torch.cdist(s_wellclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "dist_misclassified2center = torch.cdist(s_misclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "axs[0].hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('distance from well-classified center using top softmax dimensions')\n",
    "axs[1].plot(distance, accuracy)\n",
    "axs[1].set_xlabel('distance from well-classified center using top softmax dimensions')\n",
    "axs[1].set_ylabel('accuracy [%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Corner cases (top dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subsets = 10\n",
    "top_k = 10\n",
    "softmax_target = 0.5\n",
    "\n",
    "# Top perfo directions \n",
    "## NORMALIZED\n",
    "if DIRECTION == 'normalized difference':\n",
    "    perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "## ORIGINAL\n",
    "# perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "## STYLESPACE\n",
    "# s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "# perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "## GRADIENT\n",
    "elif DIRECTION == 'gradient':\n",
    "    perfo_direction = (- s_grads_all.mean(axis=0) * style_std_vec).cpu().numpy()\n",
    "\n",
    "top_k_dims = (-np.abs(perfo_direction)).argsort()\n",
    "top_k_dims_iter = iter(top_k_dims)\n",
    "\n",
    "s = wellclassified_center\n",
    "# s = s_wellclassified[2].unsqueeze(0)\n",
    "s_digits = CLASS_SELECTED if CLASS_SELECTED != 'all' else None\n",
    "\n",
    "\n",
    "imgs_plot = torch.empty((n_subsets+1, 28, 28))\n",
    "imgs_plot[0] = postprocess_images(generate_img_from_s(s))\n",
    "scores = [F.softmax(classifier_digits(imgs_plot[0:1].unsqueeze(0).to(device)), dim=1)[0, s_digits].item()]\n",
    "strength_cum = []\n",
    "top_k_dims_chosen = []\n",
    "for n in range(n_subsets):\n",
    "    print(n)\n",
    "    k = next(top_k_dims_iter)\n",
    "    \n",
    "    s_shifted = s.clone()\n",
    "    strength = 5\n",
    "    strength_cum.append(0)\n",
    "    class_softmax = 1\n",
    "    while class_softmax > softmax_target:\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_shifted[:, k] += weight_shift\n",
    "\n",
    "        img = generate_img_from_s(s_shifted)\n",
    "        img = postprocess_images(img)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            digits_pred = classifier_digits(img)\n",
    "            class_softmax = F.softmax(digits_pred, dim=1)[0, s_digits].item()\n",
    "        # print(class_softmax)\n",
    "        strength_cum[n] += strength\n",
    "        \n",
    "        if strength_cum[n] > 100: # go to next dim\n",
    "            print('skip')\n",
    "            k = next(top_k_dims_iter)\n",
    "            strength_cum[n] = 0\n",
    "            s_shifted = s.clone()\n",
    "        \n",
    "    strength = 0.01\n",
    "    while class_softmax < softmax_target - 0.005:\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_shifted[:, k] -= weight_shift\n",
    "\n",
    "        img = generate_img_from_s(s_shifted)\n",
    "        img = postprocess_images(img)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            digits_pred = classifier_digits(img)\n",
    "            class_softmax = F.softmax(digits_pred, dim=1)[0, s_digits].item()\n",
    "            \n",
    "        strength_cum[n] -= strength\n",
    "\n",
    "    imgs_plot[n+1] = img.cpu().squeeze()\n",
    "    scores.append(class_softmax)\n",
    "    top_k_dims_chosen.append(k)\n",
    "        \n",
    "\n",
    "fig, axs = plt.subplots(1, n_subsets+1, figsize=(15, 5))\n",
    "for i in range(n_subsets+1): # for each image\n",
    "    ax = axs[i]\n",
    "    ax.imshow(imgs_plot[i], vmin=0, vmax=1, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "    if i == 0:\n",
    "        title = 'original' + '\\np({})={:.2f}'.format(s_digits, scores[i])\n",
    "    else:\n",
    "        k = top_k_dims_chosen[i-1]\n",
    "        arrow = r'$\\nearrow$' if perfo_direction[k] >= 0 else r'$\\searrow$'\n",
    "        title = r'$s_{' + str(k) + '}$' + arrow + '\\np({})={:.2f}'.format(s_digits, scores[i])\n",
    "    ax.set_title(title)\n",
    "    \n",
    "plt.savefig(path_results / 'figures' / f'corner_cases.{DIRECTION}.{CLASS_SELECTED}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at degradation evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strength_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_dims[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subsets = 10\n",
    "top_k = 10\n",
    "\n",
    "# Top perfo directions \n",
    "## NORMALIZED\n",
    "if DIRECTION == 'normalized difference':\n",
    "    perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "## ORIGINAL\n",
    "# perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "## STYLESPACE\n",
    "# s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "# perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "## GRADIENT\n",
    "elif DIRECTION == 'gradient':\n",
    "    perfo_direction = (- s_grads_all.mean(axis=0) * style_std_vec).cpu().numpy()\n",
    "\n",
    "top_k_dims = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "s = wellclassified_center\n",
    "# s = s_wellclassified[2].unsqueeze(0)\n",
    "s_digits = CLASS_SELECTED if CLASS_SELECTED != 'all' else None\n",
    "\n",
    "\n",
    "for distance_intermediate in [0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5]:\n",
    "    imgs_plot = torch.empty((n_subsets, 28, 28))\n",
    "    scores = []\n",
    "    for n in range(n_subsets):\n",
    "        print(n)\n",
    "\n",
    "        strength = strength_cum[n] * distance_intermediate\n",
    "\n",
    "        s_shifted = s.clone()\n",
    "        class_softmax = 1\n",
    "\n",
    "        k = top_k_dims[n]\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_shifted[:, k] += weight_shift\n",
    "\n",
    "        img = generate_img_from_s(s_shifted)\n",
    "        img = postprocess_images(img)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            digits_pred = classifier_digits(img)\n",
    "            class_softmax = F.softmax(digits_pred, dim=1)[0, s_digits].item()\n",
    "        # print(class_softmax)\n",
    "\n",
    "        imgs_plot[n] = img.cpu().squeeze()\n",
    "        scores.append(class_softmax)\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(1, n_subsets, figsize=(15, 5))\n",
    "    for i in range(n_subsets): # for each image\n",
    "        ax = axs[i]\n",
    "        ax.imshow(imgs_plot[i], vmin=0, vmax=1, cmap='gray')\n",
    "        ax.grid(False)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(f'progression: {int(distance_intermediate*100)}%', fontsize='x-small')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "        k = top_k_dims[i]\n",
    "        arrow = r'$\\nearrow$' if perfo_direction[k] >= 0 else r'$\\searrow$'\n",
    "        title = r'$s_{' + str(k) + '}$' + arrow + '\\np({})={:.2f}'.format(s_digits, scores[i])\n",
    "        ax.set_title(title)\n",
    "\n",
    "    plt.savefig(path_results / 'figures' / f'corner_cases.{DIRECTION}.{CLASS_SELECTED}.progression{int(100*distance_intermediate)}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subsets = 10\n",
    "top_k = 10\n",
    "\n",
    "# Top perfo directions \n",
    "## NORMALIZED\n",
    "if DIRECTION == 'normalized difference':\n",
    "    perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "## ORIGINAL\n",
    "# perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "## STYLESPACE\n",
    "# s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "# perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "## GRADIENT\n",
    "elif DIRECTION == 'gradient':\n",
    "    perfo_direction = (- s_grads_all.mean(axis=0) * style_std_vec).cpu().numpy()\n",
    "\n",
    "top_k_dims = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "s = wellclassified_center\n",
    "# s = s_wellclassified[2].unsqueeze(0)\n",
    "s_digits = CLASS_SELECTED if CLASS_SELECTED != 'all' else None\n",
    "\n",
    "\n",
    "imgs_plot = torch.empty((n_subsets+1, 28, 28))\n",
    "imgs_plot[0] = postprocess_images(generate_img_from_s(s))\n",
    "scores = [F.softmax(classifier_digits(imgs_plot[0:1].unsqueeze(0).to(device)), dim=1)[0, s_digits].item()]\n",
    "for n in range(1, n_subsets+1):\n",
    "    print(n)\n",
    "\n",
    "    s_shifted = s.clone()\n",
    "    strength = 1\n",
    "    class_softmax = 1\n",
    "    while class_softmax > 0.5:\n",
    "        k = top_k_dims[n-1]\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_shifted[:, k] += weight_shift\n",
    "\n",
    "        img = generate_img_from_s(s_shifted)\n",
    "        img = postprocess_images(img)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            digits_pred = classifier_digits(img)\n",
    "            class_softmax = F.softmax(digits_pred, dim=1)[0, s_digits].item()\n",
    "        # print(class_softmax)\n",
    "\n",
    "    imgs_plot[n] = img.cpu().squeeze()\n",
    "    scores.append(class_softmax)\n",
    "        \n",
    "\n",
    "fig, axs = plt.subplots(1, n_subsets+1, figsize=(15, 5))\n",
    "for i in range(n_subsets+1): # for each image\n",
    "    ax = axs[i]\n",
    "    ax.imshow(imgs_plot[i], vmin=0, vmax=1, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "    title = 'p({})={:.2f}'.format(s_digits, scores[i])\n",
    "    ax.set_title(title)\n",
    "\n",
    "    \n",
    "#################\n",
    "\n",
    "s = s_wellclassified[0].unsqueeze(0)\n",
    "imgs_plot = torch.empty((n_subsets+1, 28, 28))\n",
    "imgs_plot[0] = postprocess_images(generate_img_from_s(s))\n",
    "scores = [F.softmax(classifier_digits(imgs_plot[0:1].unsqueeze(0).to(device)), dim=1)[0, s_digits].item()]\n",
    "for n in range(1, n_subsets+1):\n",
    "    print(n)\n",
    "    s_shifted = s.clone()\n",
    "    strength = 1\n",
    "    class_softmax = 1\n",
    "    while class_softmax > 0.5:\n",
    "        k = top_k_dims[n-1]\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_shifted[:, k] += weight_shift\n",
    "\n",
    "        img = generate_img_from_s(s_shifted)\n",
    "        img = postprocess_images(img)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            digits_pred = classifier_digits(img)\n",
    "            class_softmax = F.softmax(digits_pred, dim=1)[0, s_digits].item()\n",
    "        # print(class_softmax)\n",
    "    imgs_plot[n] = img.cpu().squeeze()\n",
    "    scores.append(class_softmax)\n",
    "fig, axs = plt.subplots(1, n_subsets+1, figsize=(15, 5))\n",
    "for i in range(n_subsets+1): # for each image\n",
    "    ax = axs[i]\n",
    "    ax.imshow(imgs_plot[i], vmin=0, vmax=1, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "    title = 'p({})={:.2f}'.format(s_digits, scores[i])\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    \n",
    "    \n",
    "s = s_wellclassified[100].unsqueeze(0)\n",
    "imgs_plot = torch.empty((n_subsets+1, 28, 28))\n",
    "imgs_plot[0] = postprocess_images(generate_img_from_s(s))\n",
    "scores = [F.softmax(classifier_digits(imgs_plot[0:1].unsqueeze(0).to(device)), dim=1)[0, s_digits].item()]\n",
    "for n in range(1, n_subsets+1):\n",
    "    print(n)\n",
    "    s_shifted = s.clone()\n",
    "    strength = 1\n",
    "    class_softmax = 1\n",
    "    while class_softmax > 0.5:\n",
    "        k = top_k_dims[n-1]\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_shifted[:, k] += weight_shift\n",
    "\n",
    "        img = generate_img_from_s(s_shifted)\n",
    "        img = postprocess_images(img)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            digits_pred = classifier_digits(img)\n",
    "            class_softmax = F.softmax(digits_pred, dim=1)[0, s_digits].item()\n",
    "        # print(class_softmax)\n",
    "    imgs_plot[n] = img.cpu().squeeze()\n",
    "    scores.append(class_softmax)\n",
    "fig, axs = plt.subplots(1, n_subsets+1, figsize=(15, 5))\n",
    "for i in range(n_subsets+1): # for each image\n",
    "    ax = axs[i]\n",
    "    ax.imshow(imgs_plot[i], vmin=0, vmax=1, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "    title = 'p({})={:.2f}'.format(s_digits, scores[i])\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "s = s_wellclassified[30].unsqueeze(0)\n",
    "imgs_plot = torch.empty((n_subsets+1, 28, 28))\n",
    "imgs_plot[0] = postprocess_images(generate_img_from_s(s))\n",
    "scores = [F.softmax(classifier_digits(imgs_plot[0:1].unsqueeze(0).to(device)), dim=1)[0, s_digits].item()]\n",
    "for n in range(1, n_subsets+1):\n",
    "    print(n)\n",
    "    s_shifted = s.clone()\n",
    "    strength = 1\n",
    "    class_softmax = 1\n",
    "    while class_softmax > 0.5:\n",
    "        k = top_k_dims[n-1]\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_shifted[:, k] += weight_shift\n",
    "\n",
    "        img = generate_img_from_s(s_shifted)\n",
    "        img = postprocess_images(img)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            digits_pred = classifier_digits(img)\n",
    "            class_softmax = F.softmax(digits_pred, dim=1)[0, s_digits].item()\n",
    "        # print(class_softmax)\n",
    "    imgs_plot[n] = img.cpu().squeeze()\n",
    "    scores.append(class_softmax)\n",
    "fig, axs = plt.subplots(1, n_subsets+1, figsize=(15, 5))\n",
    "for i in range(n_subsets+1): # for each image\n",
    "    ax = axs[i]\n",
    "    ax.imshow(imgs_plot[i], vmin=0, vmax=1, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "    title = 'p({})={:.2f}'.format(s_digits, scores[i])\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    \n",
    "    \n",
    "s = s_wellclassified[3000].unsqueeze(0)\n",
    "imgs_plot = torch.empty((n_subsets+1, 28, 28))\n",
    "imgs_plot[0] = postprocess_images(generate_img_from_s(s))\n",
    "scores = [F.softmax(classifier_digits(imgs_plot[0:1].unsqueeze(0).to(device)), dim=1)[0, s_digits].item()]\n",
    "for n in range(1, n_subsets+1):\n",
    "    print(n)\n",
    "    s_shifted = s.clone()\n",
    "    strength = 1\n",
    "    class_softmax = 1\n",
    "    while class_softmax > 0.5:\n",
    "        k = top_k_dims[n-1]\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_shifted[:, k] += weight_shift\n",
    "\n",
    "        img = generate_img_from_s(s_shifted)\n",
    "        img = postprocess_images(img)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            digits_pred = classifier_digits(img)\n",
    "            class_softmax = F.softmax(digits_pred, dim=1)[0, s_digits].item()\n",
    "        # print(class_softmax)\n",
    "    imgs_plot[n] = img.cpu().squeeze()\n",
    "    scores.append(class_softmax)\n",
    "fig, axs = plt.subplots(1, n_subsets+1, figsize=(15, 5))\n",
    "for i in range(n_subsets+1): # for each image\n",
    "    ax = axs[i]\n",
    "    ax.imshow(imgs_plot[i], vmin=0, vmax=1, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "    title = 'p({})={:.2f}'.format(s_digits, scores[i])\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Corner cases (random subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_subsets = 10\n",
    "# top_k = 100\n",
    "# top_k_subset = 10\n",
    "\n",
    "# # Top perfo directions \n",
    "# ## NORMALIZED\n",
    "# if DIRECTION == 'normalized difference':\n",
    "#     perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "# ## ORIGINAL\n",
    "# # perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "# ## STYLESPACE\n",
    "# # s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "# # perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "# ## GRADIENT\n",
    "# elif DIRECTION == 'gradient':\n",
    "#     perfo_direction = (- s_grads_all.mean(axis=0) * style_std_vec).cpu().numpy()\n",
    "\n",
    "# top_k_dims = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "# s = wellclassified_center\n",
    "# # s = s_wellclassified[2].unsqueeze(0)\n",
    "# s_digits = CLASS_SELECTED\n",
    "\n",
    "\n",
    "# imgs_plot = torch.empty((n_subsets+1, 28, 28))\n",
    "# imgs_plot[0] = postprocess_images(generate_img_from_s(s))\n",
    "# scores = [F.softmax(classifier_digits(imgs_plot[0:1].unsqueeze(0).to(device)), dim=1)[0, s_digits].item()]\n",
    "# for n in range(1, n_subsets+1):\n",
    "#     print(n)\n",
    "    \n",
    "#     top_k_dims_subset = top_k_dims[np.random.randint(top_k, size=top_k_subset)]\n",
    "\n",
    "#     s_shifted = s.clone()\n",
    "#     strength = 0.5\n",
    "#     class_softmax = 1\n",
    "#     while class_softmax > 0.5:\n",
    "#         for k in top_k_dims_subset:\n",
    "#             positive_direction = perfo_direction[k] >= 0\n",
    "#             # positive_direction = not(positive_direction)\n",
    "#             d = 1 if positive_direction else -1\n",
    "#             weight_shift = d * strength * style_std_vec[k]\n",
    "#             s_shifted[:, k] += weight_shift\n",
    "\n",
    "#         img = generate_img_from_s(s_shifted)\n",
    "#         img = postprocess_images(img)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             digits_pred = classifier_digits(img)\n",
    "#             class_softmax = F.softmax(digits_pred, dim=1)[0, s_digits].item()\n",
    "#         # print(class_softmax)\n",
    "\n",
    "#     imgs_plot[n] = img.cpu().squeeze()\n",
    "#     scores.append(class_softmax)\n",
    "        \n",
    "\n",
    "# fig, axs = plt.subplots(1, n_subsets+1, figsize=(15, 5))\n",
    "# for i in range(n_subsets+1): # for each image\n",
    "#     ax = axs[i]\n",
    "#     ax.imshow(imgs_plot[i], vmin=0, vmax=1, cmap='gray')\n",
    "#     ax.axis('off')\n",
    "#     ax.grid(False)\n",
    "#     title = 'p({})={:.2f}'.format(s_digits, scores[i])\n",
    "#     ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms dimensions S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP DIMS\n",
    " \n",
    "top_k = 3\n",
    "fig, axs = plt.subplots(1, top_k, figsize=(13, 4))\n",
    "for i in range(top_k):\n",
    "    k = (-np.abs(perfo_direction)).argsort()[i]\n",
    "    \n",
    "    # axs[i].set_title(r'$s_{' + str(k) + '}$')\n",
    "    axs[i].hist(s_wellclassified[:, k].cpu().numpy(), bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "    axs[i].hist(s_misclassified[:, k].cpu().numpy(), bins=20, edgecolor='none', alpha=0.8, label='mis-classified')\n",
    "    axs[i].set_ylabel('number of samples')\n",
    "    axs[i].set_xlabel(r'value for dimension $s_{' + str(k) + '}$')\n",
    "    # axs[i].axvline(style_min_vec[k].cpu().numpy(), color='k', ls='--')\n",
    "    # plt.text(1.1*style_min_vec[k].cpu().numpy(), 100, 'empirical min',rotation=90)\n",
    "    # axs[i].axvline(style_max_vec[k].cpu().numpy(), color='k', ls='--')\n",
    "    # plt.text(1.1*style_max_vec[k].cpu().numpy(), 100, 'empirical max',rotation=90)\n",
    "    axs[i].legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_results / 'figures' / 'hist_top_dims')\n",
    "\n",
    "\n",
    "\n",
    "# RANDOM DIMS\n",
    "\n",
    "rnd_idx = np.random.randint(len(perfo_direction), size=3)\n",
    "\n",
    "fig, axs = plt.subplots(1, top_k, figsize=(13, 4))\n",
    "for i, idx in enumerate(rnd_idx):\n",
    "    k = (-np.abs(perfo_direction)).argsort()[idx]\n",
    "    \n",
    "    # axs[i].set_title(r'$s_{' + str(k) + '}$')\n",
    "    axs[i].hist(s_wellclassified[:, k].cpu().numpy(), bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "    axs[i].hist(s_misclassified[:, k].cpu().numpy(), bins=20, edgecolor='none', alpha=0.8, label='mis-classified')\n",
    "    axs[i].set_ylabel('number of samples')\n",
    "    axs[i].set_xlabel(r'value for dimension $s_{' + str(k) + '}$')\n",
    "    # axs[i].axvline(style_min_vec[k].cpu().numpy(), color='k', ls='--')\n",
    "    # plt.text(1.1*style_min_vec[k].cpu().numpy(), 100, 'empirical min',rotation=90)\n",
    "    # axs[i].axvline(style_max_vec[k].cpu().numpy(), color='k', ls='--')\n",
    "    # plt.text(1.1*style_max_vec[k].cpu().numpy(), 100, 'empirical max',rotation=90)\n",
    "    axs[i].legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_results / 'figures' / 'hist_random_dims')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image manipulation: visualize corruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "strength = 5\n",
    "n_images = 8\n",
    "\n",
    "# Top perfo directions \n",
    "## NORMALIZED\n",
    "if DIRECTION == 'normalized difference':\n",
    "    perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "## ORIGINAL\n",
    "# perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "## STYLESPACE\n",
    "# s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "# perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "## GRADIENT\n",
    "elif DIRECTION == 'gradient':\n",
    "    perfo_direction = (- s_grads_all.mean(axis=0) * style_std_vec).cpu().numpy()\n",
    "\n",
    "top_k_dims = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "# ORIGINAL IMAGES\n",
    "imgs_orig = generate_img_from_s(s_wellclassified[:n_images])\n",
    "imgs_orig = postprocess_images(imgs_orig)\n",
    "digits_pred = classifier_digits(imgs_orig)\n",
    "class_logit = digits_pred[torch.arange(n_images), digits_wellclassified[:n_images]]\n",
    "class_softmax_orig = F.softmax(digits_pred, dim=1)[torch.arange(n_images), digits_wellclassified[:n_images]]\n",
    "\n",
    "# CORRUPT\n",
    "s_wellclassified_shifted = s_wellclassified[:n_images].clone()\n",
    "for k in top_k_dims:\n",
    "    positive_direction = perfo_direction[k] >= 0\n",
    "    # positive_direction = not(positive_direction)\n",
    "    d = 1 if positive_direction else -1\n",
    "    weight_shift = d * strength * style_std_vec[k]\n",
    "    s_wellclassified_shifted[:, k] += weight_shift\n",
    "\n",
    "imgs_corr = generate_img_from_s(s_wellclassified_shifted)\n",
    "imgs_corr = postprocess_images(imgs_corr)\n",
    "digits_pred = classifier_digits(imgs_corr)\n",
    "class_logit = digits_pred[torch.arange(n_images), digits_wellclassified[:n_images]]\n",
    "class_softmax_corr = F.softmax(digits_pred, dim=1)[torch.arange(n_images), digits_wellclassified[:n_images]]\n",
    "\n",
    "# CLEAN\n",
    "s_wellclassified_shifted = s_wellclassified[:n_images].clone()\n",
    "for k in top_k_dims:\n",
    "    positive_direction = perfo_direction[k] >= 0\n",
    "    positive_direction = not(positive_direction)\n",
    "    d = 1 if positive_direction else -1\n",
    "    weight_shift = d * strength * style_std_vec[k]\n",
    "    s_wellclassified_shifted[:, k] += weight_shift\n",
    "\n",
    "imgs_clean = generate_img_from_s(s_wellclassified_shifted)\n",
    "imgs_clean = postprocess_images(imgs_clean)\n",
    "digits_pred = classifier_digits(imgs_clean)\n",
    "class_logit = digits_pred[torch.arange(n_images), digits_wellclassified[:n_images]]\n",
    "class_softmax_clean = F.softmax(digits_pred, dim=1)[torch.arange(n_images), digits_wellclassified[:n_images]]\n",
    "\n",
    "# PLOT\n",
    "imgs_orig = imgs_orig * 255\n",
    "imgs_orig = imgs_orig.to(torch.uint8).cpu()\n",
    "imgs_corr = imgs_corr * 255\n",
    "imgs_corr = imgs_corr.to(torch.uint8).cpu()\n",
    "imgs_clean = imgs_clean * 255\n",
    "imgs_clean = imgs_clean.to(torch.uint8).cpu()\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 5))\n",
    "axs[0].imshow(vutils.make_grid(imgs_orig, pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('well-classified samples')\n",
    "axs[1].imshow(vutils.make_grid(imgs_corr, pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "axs[1].set_title(f'after corruption (top_k={top_k}; strength={strength})')\n",
    "axs[1].axis('off')\n",
    "axs[2].imshow(vutils.make_grid(imgs_clean, pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "axs[2].set_title(f'after cleaning (top_k={top_k}; strength={strength})')\n",
    "axs[2].axis('off')\n",
    "# plt.tight_layout()\n",
    "plt.savefig(path_results / 'figures' / 'image_manipulation', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(class_softmax_orig)\n",
    "print(class_softmax_corr)\n",
    "print(class_softmax_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy vs. distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = 0.7\n",
    "top_k_list = [10, 50, 100, s_all.shape[1]]\n",
    "# center = 'global'\n",
    "center = 'wellclassified'\n",
    "# center = 'misclassified'\n",
    "save_array = True\n",
    "\n",
    "if save_array:\n",
    "    top_k_list = [100]\n",
    "\n",
    "if center == 'global':\n",
    "    s_center = s_all.mean(0).unsqueeze(0)\n",
    "elif center == 'wellclassified':\n",
    "    s_center = s_wellclassified.mean(0).unsqueeze(0)\n",
    "elif center == 'misclassified':\n",
    "    s_center = s_misclassified.mean(0).unsqueeze(0)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel(f'(normalized) distance from {center} center')\n",
    "plt.ylabel('classifer accuracy [%]')\n",
    "\n",
    "for top_k in top_k_list:\n",
    "    # Top perfo directions \n",
    "    ## NORMALIZED\n",
    "    if DIRECTION == 'normalized difference':\n",
    "        perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "    ## ORIGINAL\n",
    "    # perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "    ## STYLESPACE\n",
    "    # s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "    # perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "    ## GRADIENT\n",
    "    elif DIRECTION == 'gradient':\n",
    "        perfo_direction = (- s_grads_all.mean(axis=0) * style_std_vec).cpu().numpy()\n",
    "\n",
    "    top_k_dims = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "\n",
    "    dist_wellclassified2center = torch.cdist(s_wellclassified[:, top_k_dims], s_center[:, top_k_dims]).cpu().numpy()\n",
    "    dist_misclassified2center = torch.cdist(s_misclassified[:, top_k_dims], s_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "    # Normalize\n",
    "    dist_misclassified2center -= dist_wellclassified2center.min()\n",
    "    dist_wellclassified2center -= dist_wellclassified2center.min()\n",
    "    dist_misclassified2center /= cut * dist_wellclassified2center.max()\n",
    "    dist_wellclassified2center /= cut * dist_wellclassified2center.max()\n",
    "\n",
    "    hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "    hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "    distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "    accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "    plt.plot(distance[distance<1], accuracy[distance<1], label=f'using top {top_k} dimensions')\n",
    "    \n",
    "plt.legend()\n",
    "plt.savefig(path_results / 'figures' / f'accuracy_vs_distance_{center}_center')\n",
    "\n",
    "if save_array:\n",
    "    with open(path_results / 'figures' / f'distance_accuracy.classif_{CLASSIFIER}.class_{CLASS_SELECTED}.topk{top_k}.center_{center}.npy', 'wb') as f:\n",
    "        np.savez(f, distance=distance, accuracy=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD RESULTS AND PLOT\n",
    "\n",
    "npzfile_clean = np.load(path_results / 'figures' / 'distance_accuracy.classif_clean.class_all.topk100.center_wellclassified.npy')\n",
    "npzfile_corrupt = np.load(path_results / 'figures' / 'distance_accuracy.classif_corrupt.class_all.topk100.center_wellclassified.npy')\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel(f'(normalized) distance from {center} center (using top 100 dimensions)')\n",
    "plt.ylabel('classifier accuracy [%]')\n",
    "\n",
    "distance = npzfile_clean['distance']\n",
    "accuracy = npzfile_clean['accuracy']\n",
    "plt.plot(distance[distance<1], accuracy[distance<1], label=f'classifier trained on clean data')\n",
    "\n",
    "distance = npzfile_corrupt['distance']\n",
    "accuracy = npzfile_corrupt['accuracy']\n",
    "plt.plot(distance[distance<1], accuracy[distance<1], label=f'classifier trained on corrupted data')\n",
    "\n",
    "plt.legend(fontsize='large')\n",
    "plt.savefig(path_results / 'figures' / f'accuracy_vs_distance_compare_classifiers')\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel(f'(normalized) distance from {center} center (using top 100 dimensions)')\n",
    "plt.ylabel('classifier accuracy [%]')\n",
    "\n",
    "for class_ in ['all', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "    npzfile = np.load(path_results / 'figures' / f'distance_accuracy.classif_corrupt.class_{class_}.topk100.center_wellclassified.npy')\n",
    "    distance = npzfile['distance']\n",
    "    accuracy = npzfile['accuracy']\n",
    "    plt.plot(distance[distance<1], accuracy[distance<1], label=f'{class_}', c=('k' if class_ == 'all' else None))\n",
    "\n",
    "plt.legend(title='Class')\n",
    "plt.savefig(path_results / 'figures' / f'accuracy_vs_distance_compare_classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### illustrate accuracy vs distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "distances_list = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "n_images = 8\n",
    "\n",
    "top_k_dims = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "dist_all2center = torch.cdist(s_all[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "dist_all2center -= dist_all2center.min()\n",
    "dist_all2center /= cut * dist_all2center.max()\n",
    "\n",
    "imgs_plot = torch.empty((len(distances_list), n_images, 1, 28, 28))\n",
    "distances = []\n",
    "confidences = []\n",
    "labels = []\n",
    "for i, d in enumerate(distances_list):\n",
    "\n",
    "    idx = (np.abs(dist_all2center - d) <= 0.01).squeeze()\n",
    "    s = s_all[idx][:n_images]\n",
    "    digit = digits[idx][:n_images]\n",
    "\n",
    "    imgs = generate_img_from_s(s)\n",
    "    imgs = postprocess_images(imgs)\n",
    "\n",
    "    digits_pred = classifier_digits(imgs)\n",
    "    class_logit = digits_pred[torch.arange(n_images), digit]\n",
    "    class_softmax = F.softmax(digits_pred, dim=1)[torch.arange(n_images), digit]\n",
    "    \n",
    "    imgs_plot[i] = imgs\n",
    "    distances.append(d)\n",
    "    confidences.append(class_softmax.detach().cpu().numpy())\n",
    "    labels.append(digit)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(n_images, imgs_plot.shape[0], figsize=(12, 20))\n",
    "for i in range(n_images): # for each image\n",
    "    for j in range(imgs_plot.shape[0]):\n",
    "        ax = axs[i, j]\n",
    "        ax.imshow(imgs_plot[j, i].squeeze(), vmin=0, vmax=1, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.grid(False)\n",
    "        title = 'd={:.1f} ; p({})={:.0f}%'.format(distances[j], labels[j][i], 100*confidences[j][i])\n",
    "        ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_list = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "n_images = 8\n",
    "\n",
    "\n",
    "for i, d in enumerate(distances_list):\n",
    "\n",
    "    idx = (np.abs(dist_all2center - d) <= 0.05).squeeze()\n",
    "    s = s_all[idx]\n",
    "    digit = digits[idx]\n",
    "\n",
    "    acc = []\n",
    "    for s_batch, digit_batch in zip(DataLoader(s, 32), DataLoader(digit, 32)):\n",
    "        imgs = generate_img_from_s(s_batch)\n",
    "        imgs = postprocess_images(imgs)\n",
    "\n",
    "        digits_pred = classifier_digits(imgs)\n",
    "        class_softmax = F.softmax(digits_pred, dim=1)[torch.arange(len(digits_pred)), digit_batch]\n",
    "        acc.append((class_softmax>0.5).sum()/len(class_softmax))\n",
    "\n",
    "    print(d, torch.mean(torch.tensor(acc)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = generate_img_from_s(wellclassified_center)\n",
    "img = postprocess_images(img)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img.squeeze().cpu().numpy(), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "misclassified_center = s_misclassified.mean(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "img = generate_img_from_s(misclassified_center)\n",
    "img = postprocess_images(img)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img.squeeze().cpu().numpy(), cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted images vs distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 10\n",
    "top_k = 100\n",
    "\n",
    "\n",
    "# Top perfo directions \n",
    "## NORMALIZED\n",
    "if DIRECTION == 'normalized difference':\n",
    "    perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "## ORIGINAL\n",
    "# perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "## STYLESPACE\n",
    "# s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "# perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "## GRADIENT\n",
    "elif DIRECTION == 'gradient':\n",
    "    perfo_direction = (- s_grads_all.mean(axis=0) * style_std_vec).cpu().numpy()\n",
    "\n",
    "top_k_dims = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "dist_wellclassified2center = torch.cdist(s_wellclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "dist_misclassified2center = torch.cdist(s_misclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "axs[0].hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('distance from well-classified center using top perfo dimensions')\n",
    "axs[1].plot(distance, accuracy)\n",
    "axs[1].set_xlabel('distance from well-classified center using top perfo dimensions')\n",
    "axs[1].set_ylabel('accuracy [%]')\n",
    "\n",
    "\n",
    "\n",
    "# SAMPLE IMAGES\n",
    "distances_low2high = dist_wellclassified2center.squeeze().argsort()\n",
    "idx_images = distances_low2high[np.random.randint(1000, size=n_images)]\n",
    "\n",
    "s = s_wellclassified[idx_images, :]\n",
    "s_digits = digits_wellclassified[idx_images]\n",
    "\n",
    "\n",
    "strengths = np.linspace(0, 3, num=6)\n",
    "imgs_plot = torch.empty((len(strengths), n_images, 1, 28, 28))\n",
    "distances = []\n",
    "confidences = []\n",
    "\n",
    "for i, strength in enumerate(strengths):\n",
    "    # manipulate images\n",
    "    s_shifted = s.clone()\n",
    "    for k in top_k_dims:\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_shifted[:, k] += weight_shift\n",
    "\n",
    "    imgs = generate_img_from_s(s_shifted)\n",
    "    imgs = postprocess_images(imgs)\n",
    "\n",
    "    digits_pred = classifier_digits(imgs)\n",
    "    class_logit = digits_pred[torch.arange(n_images), s_digits]\n",
    "    class_softmax = F.softmax(digits_pred, dim=1)[torch.arange(n_images), s_digits]\n",
    "\n",
    "    imgs_plot[i] = imgs\n",
    "    distances.append(torch.cdist(s_shifted[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy())\n",
    "    confidences.append(class_softmax.detach().cpu().numpy())\n",
    "\n",
    "fig, axs = plt.subplots(n_images, imgs_plot.shape[0], figsize=(12, 20))\n",
    "for i in range(n_images): # for each image\n",
    "    for j in range(imgs_plot.shape[0]):\n",
    "        ax = axs[i, j]\n",
    "        ax.imshow(imgs_plot[j, i].squeeze(), vmin=0, vmax=1, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.grid(False)\n",
    "        title = 'd={:.1f} ; c={:.0f}%'.format(distances[j][i, 0], 100*confidences[j][i])\n",
    "        ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(np.arange(len(perfo_direction)), np.abs(perfo_direction[(-np.abs(perfo_direction)).argsort()]))\n",
    "plt.xlabel('dimension')\n",
    "plt.ylabel('absolute difference')\n",
    "\n",
    "top_k = 5\n",
    "fig, axs = plt.subplots(1, top_k, figsize=(20, 5))\n",
    "for i in range(top_k):\n",
    "    k = (-np.abs(perfo_direction)).argsort()[i]\n",
    "    \n",
    "    axs[i].set_title(r'$s_{' + str(k) + '}$')\n",
    "    axs[i].hist(s_wellclassified[:, k].cpu().numpy(), bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "    axs[i].hist(s_misclassified[:, k].cpu().numpy(), bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "    axs[i].axvline(style_min_vec[k].cpu().numpy(), color='k', ls='--')\n",
    "    # plt.text(1.1*style_min_vec[k].cpu().numpy(), 100, 'empirical min',rotation=90)\n",
    "    axs[i].axvline(style_max_vec[k].cpu().numpy(), color='k', ls='--')\n",
    "    # plt.text(1.1*style_max_vec[k].cpu().numpy(), 100, 'empirical max',rotation=90)\n",
    "    axs[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 50\n",
    "top_k_idxs = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "strength = 10\n",
    "\n",
    "s_wellclassified_shifted = s_wellclassified.clone()\n",
    "for k in top_k_idxs:\n",
    "    positive_direction = perfo_direction[k] >= 0\n",
    "    # positive_direction = not(positive_direction)\n",
    "    d = 1 if positive_direction else -1\n",
    "    weight_shift = d * strength * style_std_vec[k]\n",
    "    s_wellclassified_shifted[:, k] += weight_shift\n",
    "\n",
    "\n",
    "n_images = 8\n",
    "imgs_orig = generate_img_from_s(s_wellclassified[:n_images])\n",
    "imgs_orig = postprocess_images(imgs_orig)\n",
    "imgs_orig = imgs_orig * 255\n",
    "imgs_orig = imgs_orig.to(torch.uint8).cpu()\n",
    "\n",
    "imgs_corr = generate_img_from_s(s_wellclassified_shifted[:n_images])\n",
    "imgs_corr = postprocess_images(imgs_corr)\n",
    "imgs_corr = imgs_corr * 255\n",
    "imgs_corr = imgs_corr.to(torch.uint8).cpu()\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15, 4))\n",
    "axs[0].imshow(vutils.make_grid(imgs_orig, pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('well-classified samples')\n",
    "axs[1].imshow(vutils.make_grid(imgs_corr, pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "axs[1].set_title('same samples after corruption')\n",
    "axs[1].axis('off')\n",
    "\n",
    "\n",
    "s_misclassified_shifted = s_misclassified.clone()\n",
    "for k in top_k_idxs:\n",
    "    positive_direction = perfo_direction[k] >= 0\n",
    "    positive_direction = not(positive_direction)\n",
    "    d = 1 if positive_direction else -1\n",
    "    weight_shift = d * strength * style_std_vec[k]\n",
    "    s_misclassified_shifted[:, k] += weight_shift\n",
    "\n",
    "\n",
    "imgs_orig = generate_img_from_s(s_misclassified[:n_images])\n",
    "imgs_orig = postprocess_images(imgs_orig)\n",
    "imgs_orig = imgs_orig * 255\n",
    "imgs_orig = imgs_orig.to(torch.uint8).cpu()\n",
    "\n",
    "imgs_clean = generate_img_from_s(s_misclassified_shifted[:n_images])\n",
    "imgs_clean = postprocess_images(imgs_clean)\n",
    "imgs_clean = imgs_clean * 255\n",
    "imgs_clean = imgs_clean.to(torch.uint8).cpu()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15, 4))\n",
    "axs[0].imshow(vutils.make_grid(imgs_orig, pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('misclassified samples')\n",
    "axs[1].imshow(vutils.make_grid(imgs_clean, pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "axs[1].set_title('same samples after cleaning')\n",
    "axs[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strength = 10\n",
    "top_k_list = [1, 10, 20, 50, 100, 150, 250, 500, 1000, 2000, 3000, s_wellclassified.shape[1]]\n",
    "\n",
    "df = pd.Series(index=top_k_list, dtype='float64')\n",
    "show_n_images = 8\n",
    "imgs_visualize = {}\n",
    "\n",
    "for top_k in top_k_list:\n",
    "\n",
    "    top_k_idxs = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "    s_wellclassified_shifted = s_wellclassified.clone()\n",
    "    for k in top_k_idxs:\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_wellclassified_shifted[:, k] += weight_shift\n",
    "\n",
    "\n",
    "    # get misclassifications nb resulting from the shift\n",
    "    nb_misclassifications = 0\n",
    "    for s, labels in zip(DataLoader(s_wellclassified_shifted, batch_size), DataLoader(digits[class_predicted == digits], batch_size)):\n",
    "\n",
    "        imgs = generate_img_from_s(s)\n",
    "        imgs = postprocess_images(imgs)\n",
    "        imgs_visualize[top_k] = imgs[:show_n_images]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            digits_pred = classifier_digits(imgs)\n",
    "            class_pred = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "            nb_misclassifications += (class_pred != labels).sum()\n",
    "            \n",
    "    accuracy = 100 - 100 * nb_misclassifications / s_wellclassified_shifted.shape[0]\n",
    "    df[top_k] = accuracy\n",
    "    print('Accuracy: {:.2f}% ; {}/{} misclassified samples'.format(accuracy, nb_misclassifications, s_wellclassified_shifted.shape[0]))\n",
    "\n",
    "plt.figure()\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strength = 5\n",
    "# top_k_list = [1, 10, 20, 50, 100, 150, 250, 500, 1000, 2000, 3000, s_wellclassified.shape[1]]\n",
    "top_k_list = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "df = pd.Series(index=top_k_list, dtype='float64')\n",
    "show_n_images = 8\n",
    "imgs_visualize = {}\n",
    "\n",
    "for top_k in top_k_list:\n",
    "\n",
    "    # shift images\n",
    "    top_k_idxs = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "    s_shifted_all = s_all.clone()\n",
    "    for k in top_k_idxs:\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_shifted_all[:, k] += weight_shift\n",
    "\n",
    "\n",
    "    # get classifications after the shift\n",
    "    class_predicted_after_shift = None\n",
    "    for s, labels in zip(DataLoader(s_shifted_all, batch_size), DataLoader(digits, batch_size)):\n",
    "\n",
    "        imgs = generate_img_from_s(s)\n",
    "        imgs = postprocess_images(imgs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            digits_pred = classifier_digits(imgs)\n",
    "            class_pred_t = F.softmax(digits_pred, dim=1).max(axis=1).indices\n",
    "\n",
    "        if class_predicted_after_shift is None: imgs_visualize[top_k] = imgs[:show_n_images] # save from 1st batch\n",
    "        class_predicted_after_shift = class_pred_t if class_predicted_after_shift is None else torch.cat((class_predicted_after_shift, class_pred_t))\n",
    "\n",
    "    s_shifted_wellclassified = s_shifted_all[class_predicted_after_shift == digits]\n",
    "    s_shifted_misclassified = s_shifted_all[class_predicted_after_shift != digits]\n",
    "    accuracy = 100 * s_shifted_wellclassified.shape[0] / s_shifted_all.shape[0]\n",
    "    df[top_k] = accuracy\n",
    "    print('Accuracy: {:.2f}% ; {}/{} misclassified samples'.format(accuracy, s_shifted_misclassified.shape[0], s_shifted_all.shape[0]))\n",
    "\n",
    "    # Top perfo directions \n",
    "    # NORMALIZED\n",
    "    perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "    # ORIGINAL\n",
    "    # perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "    # STYLESPACE\n",
    "    # s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "    # perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "    top_k_dims = (-np.abs(perfo_direction)).argsort()[:top_k]\n",
    "\n",
    "    dist_wellclassified2center = torch.cdist(s_shifted_wellclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "    dist_misclassified2center = torch.cdist(s_shifted_misclassified[:, top_k_dims], wellclassified_center[:, top_k_dims]).cpu().numpy()\n",
    "\n",
    "    hist_well, bins = np.histogram(dist_wellclassified2center, bins=20)\n",
    "    hist_mis, _ = np.histogram(dist_misclassified2center, bins=bins)\n",
    "    distance = np.array([(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)])\n",
    "    accuracy = 100 * hist_well / (hist_well+hist_mis)\n",
    "\n",
    "\n",
    "    # PLOT\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n",
    "    axs[0].hist(dist_wellclassified2center, bins=20, edgecolor='none', alpha=0.8, label='well-classified')\n",
    "    axs[0].hist(dist_misclassified2center, bins=20, edgecolor='none', alpha=0.8, label='misclassified')\n",
    "    axs[0].legend()\n",
    "    axs[0].set_xlabel(f'distance from well-classified center using top {top_k} perfo dimensions')\n",
    "    axs[0].set_ylabel('number of values')\n",
    "    axs[1].plot(distance, accuracy)\n",
    "    axs[1].set_xlabel(f'distance from well-classified center using top {top_k} perfo dimensions')\n",
    "    axs[1].set_ylabel('accuracy [%]')\n",
    "\n",
    "    plot_images(imgs_visualize[top_k])\n",
    "    plt.title(top_k)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at individual styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_norm_diff = (s_wellclassified - s_misclassified.mean(0)) / s_misclassified.std(0)\n",
    "relevance = s_norm_diff.mean(0) / s_norm_diff.std(0)\n",
    "perfo_direction = relevance.cpu().numpy()\n",
    "top_k_idxs = (-np.abs(relevance.cpu().numpy())).argsort()[:top_k]\n",
    "top_k_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "relevance = s_norm_diff.mean(0) / s_norm_diff.std(0)\n",
    "perfo_direction = relevance.cpu().numpy()\n",
    "top_k_idxs = (-np.abs(relevance.cpu().numpy())).argsort()[:top_k]\n",
    "top_k_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_norm_diff = (s_misclassified - s_all.mean(0)) / s_all.std(0)\n",
    "relevance = s_norm_diff.mean(0) / s_norm_diff.std(0)\n",
    "perfo_direction = relevance.cpu().numpy()\n",
    "top_k_idxs = (-np.abs(relevance.cpu().numpy())).argsort()[:top_k]\n",
    "top_k_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 10\n",
    "\n",
    "\n",
    "# ORIGINAL\n",
    "# perfo_direction = (s_misclassified.mean(0) - s_wellclassified.mean(0)).cpu().numpy()\n",
    "\n",
    "# NORMALIZED\n",
    "perfo_direction = (((s_misclassified - s_all.mean(0)) / s_all.std(0)).mean(0) - ((s_wellclassified - s_all.mean(0)) / s_all.std(0)).mean(0)).cpu().numpy()\n",
    "\n",
    "# STYLESPACE\n",
    "# s_norm_diff = (s_misclassified - s_wellclassified.mean(0)) / s_wellclassified.std(0)\n",
    "# perfo_direction = (s_norm_diff.mean(0) / s_norm_diff.std(0)).cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "top_k_list = [0, 1, 3, 6, 9, 12, 15, 20]\n",
    "imgs_plot = torch.empty((top_k+1, n_images, 1, 28, 28))\n",
    "class_softmax_all = torch.empty((top_k+1, n_images))\n",
    "\n",
    "for i, top_k_subset in enumerate(top_k_list):\n",
    "\n",
    "    top_k_idxs = (-np.abs(perfo_direction)).argsort()[:top_k_subset]\n",
    "    strength = 10\n",
    "\n",
    "    # manipulate images\n",
    "    s_wellclassified_shifted = s_wellclassified[:n_images].clone()\n",
    "    for k in top_k_idxs:\n",
    "        positive_direction = perfo_direction[k] >= 0\n",
    "        # positive_direction = not(positive_direction)\n",
    "        d = 1 if positive_direction else -1\n",
    "        weight_shift = d * strength * style_std_vec[k]\n",
    "        s_wellclassified_shifted[:, k] += weight_shift\n",
    "\n",
    "    imgs_newStyle = generate_img_from_s(s_wellclassified_shifted)\n",
    "    imgs_newStyle = postprocess_images(imgs_newStyle)\n",
    "    if type(CLASS_SELECTED) == int:\n",
    "        with torch.no_grad():\n",
    "            digits_pred_newStyle = classifier_digits(imgs_newStyle)\n",
    "        class_softmax_newStyle = F.softmax(digits_pred_newStyle, dim=1)[0, CLASS_SELECTED]\n",
    "\n",
    "    # record variables\n",
    "    imgs_plot[i] = imgs_newStyle.cpu()\n",
    "    if type(CLASS_SELECTED) == int: class_softmax_all[i] = class_softmax_newStyle.cpu()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(n_images, len(top_k_list), figsize=(12, 20))\n",
    "for i in range(n_images): # for each image\n",
    "    for j, nb_k in enumerate(top_k_list): # for each style\n",
    "        ax = axs[i, j]\n",
    "        ax.imshow(imgs_plot[j, i].squeeze(), vmin=0, vmax=1, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.grid(False)\n",
    "        title = 'original' if j == 0 else f'nb top_k: {nb_k}'\n",
    "        if type(CLASS_SELECTED) == int: title += '\\n{}: {:.0f}%'.format(CLASS_SELECTED, 100*class_softmax_all[j, i])\n",
    "        ax.set_title(title)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48bb13f6dc1e8ce33d9feddcd1695f81303b36fa9b096227160ee2e101e0653c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('alc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
