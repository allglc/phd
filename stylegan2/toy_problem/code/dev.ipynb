{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import time\n",
    "from pathlib import Path\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "path_results = Path.cwd().parent / 'results'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_moons(n_samples=10000, noise=0.3, random_state=1)\n",
    "colors = ['C0' if y == 0 else 'C1' for y in data[1]]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(data[0][:,0], data[0][:,1], c=colors, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoonsDataset(Dataset):\n",
    "    def __init__(self, n_samples=10000, noise=None):\n",
    "        x, y = make_moons(n_samples=n_samples, noise=noise, random_state=1)\n",
    "        self.x, self.y = torch.from_numpy(x).float(), torch.from_numpy(y).float()\n",
    "        # self.x = torch.zeros_like(self.x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class MoonsDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, n_samples=10000, noise=None):\n",
    "        super().__init__()\n",
    "        self.n_samples = n_samples\n",
    "        self.noise = noise\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        data = MoonsDataset(n_samples=10000, noise=self.noise)\n",
    "        self.data_train, self.data_val = torch.utils.data.random_split(data, [8000, 2000])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.data_train, batch_size=32, shuffle=True, num_workers=4)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.data_val, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.unsqueeze(1)\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        acc = (torch.sigmoid(y_hat).round() == y).sum() / y.shape[0]\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.unsqueeze(1)\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        acc = (torch.sigmoid(y_hat).round() == y).sum() / y.shape[0]\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc)\n",
    "        return loss        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MoonsDataModule(n_samples=10000, noise=0.3)\n",
    "model = Classifier()\n",
    "\n",
    "timestamp = time.strftime('%Y-%m-%d_%H%M', time.localtime())\n",
    "path_results_exp = path_results / 'classifier' / timestamp\n",
    "if not path_results_exp.exists(): path_results_exp.mkdir(parents=True)\n",
    "logger = TensorBoardLogger(save_dir=path_results_exp, name='', version='')\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"auto\", devices=1, max_epochs=30, logger=logger)\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, data_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 10),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(10, data_dim))\n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = self.model(z)\n",
    "        return x\n",
    "    \n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, latent_dim, data_dim):\n",
    "#         super().__init__()\n",
    "#         self.mapping = nn.Sequential(\n",
    "#             nn.Linear(latent_dim, 10),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(10, latent_dim),\n",
    "#             nn.LeakyReLU(0.2))\n",
    "#         self.synthesis = nn.Sequential(\n",
    "#             nn.Linear(latent_dim, 10),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(10, 10),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(10, 10),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(10, data_dim))\n",
    "        \n",
    "#     def forward(self, z):\n",
    "#         w = self.mapping(z)\n",
    "#         x = self.synthesis(w)\n",
    "#         return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, data_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(data_dim, 10),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(10, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.model(x)\n",
    "        return y\n",
    "    \n",
    "    def _get_gradient_penalty(self, crit, real, fake, epsilon):\n",
    "        '''\n",
    "        Return the gradient of the critic's scores with respect to mixes of real and fake images.\n",
    "        Parameters:\n",
    "            crit: the critic model\n",
    "            real: a batch of real images\n",
    "            fake: a batch of fake images\n",
    "            epsilon: a vector of the uniformly random proportions of real/fake per mixed image\n",
    "        Returns:\n",
    "            gradient: the gradient of the critic's scores, with respect to the mixed image\n",
    "        '''\n",
    "        # Mix the images together\n",
    "        mixed_images = real * epsilon + fake * (1 - epsilon)\n",
    "\n",
    "        # Calculate the critic's scores on the mixed images\n",
    "        mixed_scores = crit(mixed_images)\n",
    "        \n",
    "        # Take the gradient of the scores with respect to the images\n",
    "        gradient = torch.autograd.grad(\n",
    "            # Note: You need to take the gradient of outputs with respect to inputs.\n",
    "            # This documentation may be useful, but it should not be necessary:\n",
    "            # https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n",
    "            #### START CODE HERE ####\n",
    "            inputs=mixed_images,\n",
    "            outputs=mixed_scores,\n",
    "            #### END CODE HERE ####\n",
    "            # These other parameters have to do with the pytorch autograd engine works\n",
    "            grad_outputs=torch.ones_like(mixed_scores), \n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "\n",
    "        gradient = gradient.view(len(gradient), -1)\n",
    "\n",
    "        # Calculate the magnitude of every row\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        \n",
    "        # Penalize the mean squared distance of the gradient norms from 1\n",
    "        penalty = torch.pow(torch.mean(gradient_norm - 1), 2)\n",
    "\n",
    "        return penalty\n",
    "\n",
    "\n",
    "\n",
    "class GAN(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, latent_dim, data_dim, data_module, c_dim=0, loss='standard', lambda_gp=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.generator = Generator(latent_dim + c_dim, data_dim)\n",
    "        self.discriminator = Discriminator(data_dim + c_dim)\n",
    "        self.dm = data_module\n",
    "        self.c_dim = c_dim # class dimensionality, 0=unconditional\n",
    "        self.loss = loss\n",
    "        self.lambda_gp = lambda_gp # gradient penalty weight for wasserstein loss\n",
    "        \n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "    \n",
    "    def wasserstein_loss_gp(self, mode, pred_fake, pred_real=None, x_real=None, x_fake=None, lambda_gp=None):\n",
    "        if mode == 'generator':\n",
    "            loss = -pred_fake.mean()\n",
    "        elif mode == 'discriminator':\n",
    "            epsilon = torch.rand(x_real.shape[0], 1, device=self.device, requires_grad=True)\n",
    "            gp = self.discriminator._get_gradient_penalty(self.discriminator, x_real, x_fake.detach(), epsilon)\n",
    "            loss = torch.mean(pred_fake - pred_real) + lambda_gp*gp\n",
    "        return loss\n",
    "        \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        x_real, y_real = batch\n",
    "        z = torch.randn(x_real.shape[0], self.latent_dim, device=self.device)\n",
    "        if self.c_dim > 0:\n",
    "            rnd_label = torch.randint(self.c_dim, size=(z.shape[0],), device=self.device)\n",
    "            c = F.one_hot(rnd_label, num_classes=self.c_dim)\n",
    "            z = torch.cat([z, c], dim=1)\n",
    "\n",
    "        # generator\n",
    "        if optimizer_idx == 0:\n",
    "            x_fake = self.generator(z)\n",
    "            if self.c_dim > 0: \n",
    "                x_fake = torch.cat([x_fake, c], dim=1)\n",
    "            pred_fake = self.discriminator(x_fake)\n",
    "            if self.loss == 'standard':\n",
    "                loss_gen = self.adversarial_loss(pred_fake, torch.ones_like(pred_fake, device=self.device)) # fakes should be predicted as true\n",
    "            elif self.loss == 'wasserstein':\n",
    "                loss_gen = self.wasserstein_loss_gp('generator', pred_fake)\n",
    "            self.log('loss_gen', loss_gen)\n",
    "            return loss_gen\n",
    "        \n",
    "        # discriminator\n",
    "        elif optimizer_idx == 1:\n",
    "            x_fake = self.generator(z).detach()\n",
    "            if self.c_dim > 0: \n",
    "                x_fake = torch.cat([x_fake, c], dim=1)\n",
    "                c_real = F.one_hot(y_real.long(), num_classes=self.c_dim)\n",
    "                x_real = torch.cat([x_real, c_real], dim=1)\n",
    "            pred_real = self.discriminator(x_real)\n",
    "            pred_fake = self.discriminator(x_fake)\n",
    "            if self.loss == 'standard':\n",
    "                loss_disc = 0.5 * (self.adversarial_loss(pred_fake, torch.zeros_like(pred_fake, device=self.device)) # fakes should be predicted as false\n",
    "                                   + self.adversarial_loss(pred_real, torch.ones_like(pred_real, device=self.device))) # reals should be predicted as true\n",
    "            elif self.loss == 'wasserstein':\n",
    "                loss_disc = self.wasserstein_loss_gp('discriminator', pred_fake, pred_real, x_real, x_fake, lambda_gp=self.lambda_gp)\n",
    "            self.log('loss_disc', loss_disc)\n",
    "            return loss_disc\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optim_gen = torch.optim.Adam(self.generator.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "        optim_disc = torch.optim.Adam(self.discriminator.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "        return optim_gen, optim_disc\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # log images\n",
    "        z = torch.randn(1000, self.latent_dim, device=self.device)\n",
    "        if self.c_dim > 0:\n",
    "            rnd_label = torch.randint(self.c_dim, size=(z.shape[0],), device=self.device)\n",
    "            c = F.one_hot(rnd_label, num_classes=self.c_dim)\n",
    "            z = torch.cat([z, c], dim=1)\n",
    "        x_fake = self.generator(z).detach().cpu().numpy()\n",
    "        plt.scatter(self.dm.data_train[:1000][0][:, 0], self.dm.data_train[:1000][0][:, 1], alpha=0.5, c=['C0' if y == 0 else 'C1' for y in self.dm.data_train[:1000][1]])\n",
    "        color = ['C2' if y == 0 else 'C3' for y in rnd_label] if self.c_dim > 0 else 'k'\n",
    "        plt.scatter(x_fake[:, 0], x_fake[:, 1], c=color)\n",
    "        tensorboard_logger = self.logger.experiment\n",
    "        tensorboard_logger.add_figure(\"generated_images\", plt.gcf(), self.current_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'standard_GAN_cond_latent10'\n",
    "\n",
    "dm = MoonsDataModule(n_samples=10000, noise=0.1)\n",
    "model = GAN(10, 2, dm, c_dim=2, loss='standard', lambda_gp=10)\n",
    "\n",
    "timestamp = time.strftime('%Y-%m-%d_%H%M', time.localtime())\n",
    "path_results_exp = path_results / 'GAN' / timestamp\n",
    "if not path_results_exp.exists(): path_results_exp.mkdir(parents=True)\n",
    "logger = TensorBoardLogger(save_dir=path_results_exp, name=experiment_name, version='')\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"auto\", devices=1, max_epochs=100, logger=logger)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'wasserstein_GAN_cond_latent10'\n",
    "\n",
    "dm = MoonsDataModule(n_samples=10000, noise=0.1)\n",
    "model = GAN(10, 2, dm, c_dim=2, loss='wasserstein', lambda_gp=10)\n",
    "\n",
    "timestamp = time.strftime('%Y-%m-%d_%H%M', time.localtime())\n",
    "path_results_exp = path_results / 'GAN' / timestamp\n",
    "if not path_results_exp.exists(): path_results_exp.mkdir(parents=True)\n",
    "logger = TensorBoardLogger(save_dir=path_results_exp, name=experiment_name, version='')\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"auto\", devices=1, max_epochs=100, logger=logger)\n",
    "trainer.fit(model, dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d98911e8e1829f7f6c5e31f61dda5e143049f52824dab13c60371001bf774251"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
