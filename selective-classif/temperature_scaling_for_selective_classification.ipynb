{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1316549/1808983860.py:32: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import nn\n",
    "from torchvision.models import (\n",
    "    resnet18, ResNet18_Weights,\n",
    "    resnet34, ResNet34_Weights,\n",
    "    resnet50, ResNet50_Weights,\n",
    "    resnet101, ResNet101_Weights,\n",
    "    vit_b_16, ViT_B_16_Weights,\n",
    "    vit_b_32, ViT_B_32_Weights,\n",
    "    vgg16, VGG16_Weights, \n",
    "    vgg16_bn, VGG16_BN_Weights,\n",
    "    convnext_tiny, ConvNeXt_Tiny_Weights,\n",
    "    convnext_base, ConvNeXt_Base_Weights,\n",
    "    efficientnet_v2_s, EfficientNet_V2_S_Weights,\n",
    "    efficientnet_v2_m, EfficientNet_V2_M_Weights\n",
    ")\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import Compose, Resize, Lambda, ToTensor, Grayscale, ToPILImage\n",
    "import timm\n",
    "from timm.data import resolve_data_config, create_transform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import PIL\n",
    "\n",
    "import sys\n",
    "sys.path.append('./benchmarking-uncertainty-estimation-performance-main/utils')\n",
    "from temperature_scaling import _ECELoss\n",
    "from uncertainty_metrics import AUROC, ECE_calc\n",
    "from general_calibration_error import gce\n",
    "from df_posthoc_calibration.calibration import HB_binary, HB_toplabel\n",
    "\n",
    "from calibrators import ModelWithVectorScaling, ModelWithTemperatureOriginal, ModelWithTemperature\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "path_results = os.path.dirname(os.getcwd()) + '/results'\n",
    "\n",
    "for p in [\n",
    "    Path('/scratchf/IMAGENET'), # DeepLab\n",
    "    Path(os.path.expandvars('$DSDIR/imagenet'))]: # Jean Zay\n",
    "    if os.path.exists(p):\n",
    "        path_dataset = p\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MODEL = 'ResNet50_V2' \n",
    "\n",
    "# TORCHVISION\n",
    "models_and_weights_torchvision = {\n",
    "    'ResNet18': (resnet18, ResNet18_Weights.IMAGENET1K_V1), # same/worse\n",
    "    'ResNet34': (resnet34, ResNet34_Weights.IMAGENET1K_V1), # same/worse\n",
    "    'ResNet50': (resnet50, ResNet50_Weights.IMAGENET1K_V1), # same/worse\n",
    "    'ResNet50_V2': (resnet50, ResNet50_Weights.IMAGENET1K_V2), # better\n",
    "    'ResNet101': (resnet101, ResNet101_Weights.IMAGENET1K_V1), # same/worse\n",
    "    'ResNet101_V2': (resnet101, ResNet101_Weights.IMAGENET1K_V2), # better\n",
    "    'ViT_B_16': (vit_b_16, ViT_B_16_Weights.IMAGENET1K_V1), # better\n",
    "    'ViT_B_16_SWAG_E2E': (vit_b_16, ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1), # same/worse\n",
    "    'ViT_B_16_SWAG_LINEAR': (vit_b_16, ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1), # same/worse\n",
    "    'ViT_B_32': (vit_b_32, ViT_B_32_Weights.IMAGENET1K_V1), # better\n",
    "    'VGG16': (vgg16, VGG16_Weights.IMAGENET1K_V1), # same/worse\n",
    "    'VGG16_BN': (vgg16_bn, VGG16_BN_Weights.IMAGENET1K_V1), # same/worse\n",
    "    'ConvNeXt_Tiny': (convnext_tiny, ConvNeXt_Tiny_Weights.IMAGENET1K_V1), # better\n",
    "    'ConvNeXt_Base': (convnext_base, ConvNeXt_Base_Weights.IMAGENET1K_V1), # better\n",
    "    'EfficientNet_V2_S': (efficientnet_v2_s, EfficientNet_V2_S_Weights.IMAGENET1K_V1), # better\n",
    "    'EfficientNet_V2_M': (efficientnet_v2_m, EfficientNet_V2_M_Weights.IMAGENET1K_V1) # better\n",
    "}\n",
    "\n",
    "# TIMM\n",
    "models_timm = [\n",
    "    'vit_base_patch16_224', # same/worse\n",
    "    'vit_base_patch16_224_in21k', # NEED TO CONVERT IN21K PREDICTIONS TO IN1K\n",
    "    'vit_base_patch16_224_miil', # same/worse\n",
    "    'vit_base_patch16_224_miil_in21k', # NEED TO CONVERT IN21K PREDICTIONS TO IN1K\n",
    "    'vit_base_patch16_384', # same/worse\n",
    "    'vit_base_patch32_224', # same/worse\n",
    "    'vit_base_patch32_224_in21k', # NEED TO CONVERT IN21K PREDICTIONS TO IN1K\n",
    "    'vit_base_patch32_384', # same/worse\n",
    "    ]\n",
    "\n",
    "if MODEL in models_and_weights_torchvision.keys():\n",
    "    TORCHVISION_OR_TIMM = 'torchvision'\n",
    "elif MODEL in models_timm:\n",
    "    TORCHVISION_OR_TIMM = 'timm'\n",
    "else:\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CLASSIFIER\n",
    "if TORCHVISION_OR_TIMM == 'timm':\n",
    "    classifier = timm.create_model(MODEL, pretrained=True).eval().to(device)\n",
    "    transforms = timm.data.create_transform(**timm.data.resolve_data_config({}, model=classifier))\n",
    "\n",
    "elif TORCHVISION_OR_TIMM == 'torchvision':\n",
    "    model, weights = models_and_weights_torchvision[MODEL]\n",
    "    classifier = model(weights=weights).eval().to(device)\n",
    "    classifier = torch.compile(classifier)\n",
    "    transforms = weights.transforms()\n",
    "\n",
    "\n",
    "# LOAD DATA\n",
    "# dataset_train = ImageFolder(path_dataset+'/train', transform=transforms)\n",
    "dataset_val = ImageFolder(path_dataset/'val', transform=transforms)\n",
    "\n",
    "# dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, shuffle=True) \n",
    "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, shuffle=True) \n",
    "# id_to_idx = {}\n",
    "# idx_to_label = {}\n",
    "# with open(path_imagenet_labels) as f:\n",
    "#     for i, line in enumerate(f):\n",
    "#         id_to_idx[line[:9]] = i\n",
    "#         idx_to_label[i] = line[10:-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing temperature scaling\n"
     ]
    }
   ],
   "source": [
    "valid_size = 5000\n",
    "num_classes = 1000\n",
    "\n",
    "test_indices, valid_indices = sklearn.model_selection.train_test_split(np.arange(len(dataset_val)),\n",
    "                                                                        train_size=len(dataset_val) - valid_size,\n",
    "                                                                        stratify=dataset_val.targets)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset_val, pin_memory=True, batch_size=BATCH_SIZE,\n",
    "                                            sampler=SubsetRandomSampler(valid_indices), num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_val, pin_memory=True, batch_size=BATCH_SIZE,\n",
    "                                            sampler=SubsetRandomSampler(test_indices), num_workers=4)\n",
    "\n",
    "\n",
    "model = ModelWithTemperatureOriginal(classifier)\n",
    "print(f'Performing temperature scaling')\n",
    "model.set_temperature(valid_loader)\n",
    "baseline_temp = model.temperature.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWithTemperature(classifier, binary_loss=True)\n",
    "model.set_temperature(valid_loader)\n",
    "optimal_temp = model.temperature.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits_test = torch.zeros((len(test_indices), 1000))\n",
    "all_labels_test = torch.zeros(len(test_indices), dtype=int)\n",
    "i = 0\n",
    "for x, y in test_loader:\n",
    "    with torch.no_grad():\n",
    "        logits = classifier(x.to(device))\n",
    "    all_logits_test[i:i+logits.shape[0], :] = logits.cpu()\n",
    "    all_labels_test[i:i+logits.shape[0]] = y.cpu()\n",
    "    i += logits.shape[0]\n",
    "\n",
    "\n",
    "all_logits_valid = torch.zeros((len(valid_indices), 1000))\n",
    "all_labels_valid = torch.zeros(len(valid_indices), dtype=int)\n",
    "i = 0\n",
    "for x, y in valid_loader:\n",
    "    with torch.no_grad():\n",
    "        logits = classifier(x.to(device))\n",
    "    all_logits_valid[i:i+logits.shape[0], :] = logits.cpu()\n",
    "    all_labels_valid[i:i+logits.shape[0]] = y.cpu()\n",
    "    i += logits.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimize TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focal_calibration.Losses.brier_score import BrierScore\n",
    "\n",
    "class ModelWithTemperature(nn.Module):\n",
    "    \"\"\"\n",
    "    A thin decorator, which wraps a model with temperature scaling\n",
    "    model (nn.Module):\n",
    "        A classification neural network\n",
    "        NB: Output of the neural network should be the classification logits,\n",
    "            NOT the softmax (or log softmax)!\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super(ModelWithTemperature, self).__init__()\n",
    "        self.model = model.eval()\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "    def forward(self, input):\n",
    "        logits = self.model(input)\n",
    "        return self.temperature_scale(logits)\n",
    "\n",
    "    def temperature_scale(self, logits):\n",
    "        \"\"\"\n",
    "        Perform temperature scaling on logits\n",
    "        \"\"\"\n",
    "        # Expand temperature to match the size of logits\n",
    "        temperature = self.temperature.unsqueeze(1).expand(logits.size(0), logits.size(1))\n",
    "        return logits / temperature\n",
    "\n",
    "    # This function probably should live outside of this class, but whatever\n",
    "    def set_temperature(self, valid_loader):\n",
    "        \"\"\"\n",
    "        Tune the tempearature of the model (using the validation set).\n",
    "        We're going to set it to optimize NLL.\n",
    "        valid_loader (DataLoader): validation set loader\n",
    "        \"\"\"\n",
    "        self.cuda()\n",
    "        nll_criterion = BrierScore().cuda()\n",
    "\n",
    "        # First: collect all the logits and labels for the validation set\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "        with torch.no_grad():\n",
    "            for input, label in valid_loader:\n",
    "                input = input.cuda()\n",
    "                logits = self.model(input)\n",
    "                logits_list.append(logits)\n",
    "                labels_list.append(label)\n",
    "            logits = torch.cat(logits_list).cuda()\n",
    "            labels = torch.cat(labels_list).cuda()\n",
    "\n",
    "        # Next: optimize the temperature w.r.t. NLL\n",
    "        optimizer = torch.optim.LBFGS([self.temperature], lr=0.01, max_iter=5000)\n",
    "\n",
    "        def eval():\n",
    "            optimizer.zero_grad()\n",
    "            loss = nll_criterion(self.temperature_scale(logits), labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(eval)\n",
    "\n",
    "        return self\n",
    "\n",
    "model = ModelWithTemperature(classifier)\n",
    "print(f'Performing temperature scaling')\n",
    "model.set_temperature(valid_loader)\n",
    "baseline_temp2 = model.temperature.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focal_calibration.Losses.brier_score import BrierScore\n",
    "\n",
    "\n",
    "class ModelWithTemperatureCustom(nn.Module):\n",
    "    \"\"\"\n",
    "    A thin decorator, which wraps a model with temperature scaling\n",
    "    model (nn.Module):\n",
    "        A classification neural network\n",
    "        NB: Output of the neural network should be the classification logits,\n",
    "            NOT the softmax (or log softmax)!\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super(ModelWithTemperatureCustom, self).__init__()\n",
    "        self.model = model.eval()\n",
    "        self.temperature = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        logits = self.model(input)\n",
    "        return self.temperature_scale(logits)\n",
    "\n",
    "    def temperature_scale(self, logits):\n",
    "        \"\"\"\n",
    "        Perform temperature scaling on logits\n",
    "        \"\"\"\n",
    "        # Expand temperature to match the size of logits\n",
    "        temperature = self.temperature.unsqueeze(1).expand(logits.size(0), logits.size(1))\n",
    "        return logits / temperature\n",
    "\n",
    "    # This function probably should live outside of this class, but whatever\n",
    "    def set_temperature(self, valid_loader):\n",
    "        \"\"\"\n",
    "        Tune the tempearature of the model (using the validation set).\n",
    "        We're going to set it to optimize NLL.\n",
    "        valid_loader (DataLoader): validation set loader\n",
    "        \"\"\"\n",
    "        self.cuda()\n",
    "        nll_criterion = nn.BCELoss().cuda()\n",
    "\n",
    "        # First: collect all the logits and labels for the validation set\n",
    "        logits_list = []\n",
    "        probs_list = []\n",
    "        labels_list = []\n",
    "        train_labels_list = []\n",
    "        with torch.no_grad():\n",
    "            for input, label in valid_loader:\n",
    "                input = input.cuda()\n",
    "                label = label.cuda()\n",
    "                logits = self.model(input)\n",
    "                probs = torch.softmax(logits, axis=1)\n",
    "                y_pred = probs.argmax(axis=1)\n",
    "                correct = (y_pred == label).float()\n",
    "                logits_list.append(logits)\n",
    "                probs_list.append(probs)\n",
    "                train_labels_list.append(correct)\n",
    "                labels_list.append(label)\n",
    "            logits = torch.cat(logits_list).cuda()\n",
    "            train_probas = torch.max(torch.cat(probs_list).cuda(), 1, keepdim=True).values\n",
    "            labels = torch.cat(labels_list).cuda()\n",
    "            train_labels = torch.cat(train_labels_list).cuda().unsqueeze(1)\n",
    "\n",
    "        # Next: optimize the temperature w.r.t. NLL\n",
    "        optimizer = torch.optim.LBFGS([self.temperature], lr=0.01, max_iter=5000)\n",
    "\n",
    "        def eval():\n",
    "            optimizer.zero_grad()\n",
    "            scaled_logits = self.temperature_scale(logits)\n",
    "            probas = torch.softmax(scaled_logits, axis=1)\n",
    "            max_probas = torch.max(probas, 1, keepdim=True).values\n",
    "            # loss = nll_criterion(max_probas, train_labels)\n",
    "            loss = ((max_probas - train_labels) ** 2).mean()\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(eval)\n",
    "\n",
    "        return self\n",
    "\n",
    "model = ModelWithTemperatureCustom(classifier)\n",
    "model.set_temperature(valid_loader)\n",
    "optimal_temp = model.temperature.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "# ece, _ = ECE_calc(samples_certainties)\n",
    "ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('original')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "t = baseline_temp\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "# ece, _ = ECE_calc(samples_certainties)\n",
    "ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nbaseline')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "t = optimal_temp_base\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "# ece, _ = ECE_calc(samples_certainties)\n",
    "ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\noptimal')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "t = optimal_temp\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "# ece, _ = ECE_calc(samples_certainties)\n",
    "ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\noptimal')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "t = baseline_temp2\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "# ece, _ = ECE_calc(samples_certainties)\n",
    "ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nbaseline2')\n",
    "print(ece)\n",
    "print(auroc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters for vector scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = valid_loader\n",
    "\n",
    "all_logits = torch.zeros((len(loader.sampler), num_classes))\n",
    "all_labels = torch.zeros(len(loader.sampler), dtype=int)\n",
    "i = 0\n",
    "for x, y in loader:\n",
    "    with torch.no_grad():\n",
    "        logits = classifier(x.to(device))\n",
    "    all_logits[i:i+logits.shape[0], :] = logits.cpu()\n",
    "    all_labels[i:i+logits.shape[0]] = y.cpu()\n",
    "    i += logits.shape[0]\n",
    "probs = torch.softmax(all_logits, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels\n",
    "\n",
    "\n",
    "acc_per_class = np.array([correct[all_labels == l].float().mean().item() for l in range(num_classes)])\n",
    "avg_confid_per_class = np.array([certainties[all_labels == l].mean().item() for l in range(num_classes)])\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].hist(acc_per_class)\n",
    "axs[0].set_xlabel('accuracy per class')\n",
    "axs[1].hist(avg_confid_per_class)\n",
    "axs[1].set_xlabel('average confid per class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adaptive_bins(predictions, num_bins):\n",
    "  \"\"\"Returns upper edges for binning an equal number of datapoints per bin.\"\"\"\n",
    "  if np.size(predictions) == 0:\n",
    "    return np.linspace(0, 1, num_bins+1)[:-1]\n",
    "\n",
    "  edge_indices = np.linspace(0, len(predictions), num_bins, endpoint=False)\n",
    "\n",
    "  # Round into integers for indexing. If num_bins does not evenly divide\n",
    "  # len(predictions), this means that bin sizes will alternate between SIZE and\n",
    "  # SIZE+1.\n",
    "  edge_indices = np.round(edge_indices).astype(int)\n",
    "\n",
    "  # If there are many more bins than data points, some indices will be\n",
    "  # out-of-bounds by one. Set them to be within bounds:\n",
    "  edge_indices = np.minimum(edge_indices, len(predictions) - 1)\n",
    "\n",
    "  # Obtain the edge values:\n",
    "  edges = np.sort(predictions)[edge_indices]\n",
    "\n",
    "  # Following the convention of numpy.digitize, we do not include the leftmost\n",
    "  # edge (i.e. return the upper bin edges):\n",
    "  return edges[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = torch.linspace(avg_confid_per_class.min()-1e-6, avg_confid_per_class.max(), n_clusters+1) # -1e-6 to include min value in first bucket\n",
    "boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = torch.bucketize(torch.tensor(avg_confid_per_class), boundaries)\n",
    "buckets.unique()\n",
    "for i, b in enumerate(buckets.unique()):\n",
    "    print(i, (buckets == b).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = get_adaptive_bins(avg_confid_per_class, 5)\n",
    "boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = torch.bucketize(torch.tensor(avg_confid_per_class), torch.tensor(boundaries))\n",
    "buckets.unique()\n",
    "for i, b in enumerate(buckets.unique()):\n",
    "    print(i, (buckets == b).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focal_calibration.Losses.brier_score import BrierScore\n",
    "\n",
    "class ModelWithVectorScaling(nn.Module):\n",
    "    \"\"\"\n",
    "    A thin decorator, which wraps a model with vector scaling\n",
    "    model (nn.Module):\n",
    "        A classification neural network\n",
    "        NB: Output of the neural network should be the classification logits,\n",
    "            NOT the softmax (or log softmax)!\n",
    "    \"\"\"\n",
    "    def __init__(self, model, n_classes, binary_loss=False, n_clusters=None):\n",
    "        super(ModelWithVectorScaling, self).__init__()\n",
    "        self.model = model.eval()\n",
    "        self.n_classes = n_classes\n",
    "        self.binary_loss = binary_loss\n",
    "        self.n_clusters = n_clusters\n",
    "        if self.n_clusters is None:\n",
    "            self.vector_params = nn.Parameter(torch.ones(self.n_classes) * 1.5)\n",
    "        else:\n",
    "            self.vector_params = nn.Parameter(torch.ones(self.n_clusters) * 1.5)\n",
    "\n",
    "    def forward(self, input):\n",
    "        logits = self.model(input)\n",
    "        return self.scale(logits)\n",
    "    \n",
    "    def get_adaptive_bins(self, predictions, num_bins):\n",
    "        \"\"\"Returns upper edges for binning an equal number of datapoints per bin.\"\"\"\n",
    "        if np.size(predictions) == 0:\n",
    "            return np.linspace(0, 1, num_bins+1)[:-1]\n",
    "\n",
    "        edge_indices = np.linspace(0, len(predictions), num_bins, endpoint=False)\n",
    "\n",
    "        # Round into integers for indexing. If num_bins does not evenly divide\n",
    "        # len(predictions), this means that bin sizes will alternate between SIZE and\n",
    "        # SIZE+1.\n",
    "        edge_indices = np.round(edge_indices).astype(int)\n",
    "\n",
    "        # If there are many more bins than data points, some indices will be\n",
    "        # out-of-bounds by one. Set them to be within bounds:\n",
    "        edge_indices = np.minimum(edge_indices, len(predictions) - 1)\n",
    "\n",
    "        # Obtain the edge values:\n",
    "        edges = np.sort(predictions)[edge_indices]\n",
    "\n",
    "        # Following the convention of numpy.digitize, we do not include the leftmost\n",
    "        # edge (i.e. return the upper bin edges):\n",
    "        return edges[1:]\n",
    "\n",
    "    def get_vector(self):\n",
    "        if self.n_clusters is None:\n",
    "            vector = self.vector_params\n",
    "        else:\n",
    "            vector = torch.zeros(self.n_classes).cuda()\n",
    "            for i, b in enumerate(self.buckets.unique()):\n",
    "                vector[self.buckets==b] = self.vector_params[i]\n",
    "        return vector\n",
    "\n",
    "    def scale(self, logits):\n",
    "        \"\"\"\n",
    "        Perform scaling on logits\n",
    "        \"\"\"\n",
    "        vector = self.get_vector()\n",
    "        return logits * vector\n",
    "\n",
    "    def set_vector(self, valid_loader):\n",
    "        \"\"\"\n",
    "        Tune the vector (using the validation set).\n",
    "        We're going to set it to optimize NLL.\n",
    "        valid_loader (DataLoader): validation set loader\n",
    "        \"\"\"\n",
    "\n",
    "        self.cuda()\n",
    "        nll_criterion = nn.BCELoss().cuda() if self.binary_loss else nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        # First: collect all the logits and labels for the validation set\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "        binary_labels_list = []\n",
    "        with torch.no_grad():\n",
    "            for input, label in valid_loader:\n",
    "                input = input.cuda()\n",
    "                label = label.cuda()\n",
    "                logits = self.model(input)\n",
    "                logits_list.append(logits)\n",
    "                labels_list.append(label)\n",
    "                if self.binary_loss:\n",
    "                    probs = torch.softmax(logits, axis=1)\n",
    "                    y_pred = probs.argmax(axis=1)\n",
    "                    correct = (y_pred == label).float()\n",
    "                    binary_labels_list.append(correct) # binary label\n",
    "            logits = torch.cat(logits_list).cuda()\n",
    "            labels = torch.cat(labels_list).cuda()\n",
    "            if self.binary_loss:\n",
    "                binary_labels = torch.cat(binary_labels_list).cuda().unsqueeze(1)\n",
    "\n",
    "        if self.n_clusters is not None:\n",
    "            probs = torch.softmax(logits, axis=1)\n",
    "            certainties, y_pred = probs.max(axis=1)\n",
    "            correct = (y_pred == labels)\n",
    "            avg_confid_per_class = torch.tensor([certainties[labels == l].mean().item() for l in range(self.n_classes)])\n",
    "            # boundaries = torch.linspace(avg_confid_per_class.min()-1e-6, avg_confid_per_class.max(), self.n_clusters+1) # -1e-6 to include min value in first bucket\n",
    "            boundaries = torch.tensor(self.get_adaptive_bins(avg_confid_per_class.numpy(), self.n_clusters))\n",
    "            self.buckets = torch.bucketize(avg_confid_per_class, boundaries)\n",
    "            # acc_per_class = torch.tensor([correct[labels == l].float().mean().item() for l in range(self.n_classes)])\n",
    "            # boundaries = torch.linspace(acc_per_class.min()-1e-6, acc_per_class.max(), self.n_clusters+1) # -1e-6 to include min value in first bucket\n",
    "            # self.buckets = torch.bucketize(acc_per_class, boundaries)\n",
    "\n",
    "        # Next: optimize the temperature w.r.t. NLL\n",
    "        optimizer = torch.optim.LBFGS([self.vector_params], lr=0.01, max_iter=5000)\n",
    "\n",
    "        def eval():\n",
    "            optimizer.zero_grad()\n",
    "            if self.binary_loss:\n",
    "                scaled_logits = self.scale(logits)\n",
    "                probas = torch.softmax(scaled_logits, axis=1)\n",
    "                max_probas = torch.max(probas, 1, keepdim=True).values\n",
    "                # loss = nll_criterion(max_probas, binary_labels)\n",
    "                loss = ((max_probas - binary_labels) ** 2).mean()\n",
    "            else:\n",
    "                # loss = nll_criterion(self.scale(logits), labels)\n",
    "                loss = BrierScore()(self.scale(logits), labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(eval)\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWithVectorScaling(classifier, 1000)\n",
    "model.set_vector(valid_loader)\n",
    "baseline_VS = model.get_vector().detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWithVectorScaling(classifier, 1000, True)\n",
    "model.set_vector(valid_loader)\n",
    "BVS = model.get_vector().detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWithVectorScaling(classifier, 1000, n_clusters=n_clusters)\n",
    "model.set_vector(valid_loader)\n",
    "VS_clustered = model.get_vector().detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWithVectorScaling(classifier, 1000, True, n_clusters=n_clusters)\n",
    "model.set_vector(valid_loader)\n",
    "BVS_clustered = model.get_vector().detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('original')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "t = baseline_temp\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nbaseline')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "t = optimal_temp\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\noptimal')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "probs = torch.softmax(all_logits_test*baseline_VS, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nbaseline VS')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test*BVS, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nBVS')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test*VS_clustered, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nVS clustered')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test*BVS_clustered, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nBVS clustered')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "# probs = torch.softmax(all_logits_test*optimal_VS, axis=1)\n",
    "# certainties, y_pred = probs.max(axis=1)\n",
    "# correct = y_pred == all_labels_test\n",
    "# samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "# ece, _ = ECE_calc(samples_certainties)\n",
    "# auroc = AUROC(samples_certainties)\n",
    "# print('\\noptimal VS')\n",
    "# print(ece)\n",
    "# print(auroc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('original')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "t = baseline_temp\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nbaseline')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "t = optimal_temp\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\noptimal')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "probs = torch.softmax(all_logits_test*baseline_VS, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nbaseline VS')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test*BVS, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nBVS')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test*VS_clustered, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nVS clustered')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test*BVS_clustered, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nBVS clustered')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "# probs = torch.softmax(all_logits_test*optimal_VS, axis=1)\n",
    "# certainties, y_pred = probs.max(axis=1)\n",
    "# correct = y_pred == all_labels_test\n",
    "# samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "# ece, _ = ECE_calc(samples_certainties)\n",
    "# auroc = AUROC(samples_certainties)\n",
    "# print('\\noptimal VS')\n",
    "# print(ece)\n",
    "# print(auroc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('original')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "t = baseline_temp\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nbaseline')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "t = optimal_temp\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\noptimal')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "probs = torch.softmax(all_logits_test*baseline_VS, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nbaseline VS')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test*BVS, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nBVS')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test*VS_clustered, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nVS clustered')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test*BVS_clustered, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nBVS clustered')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "# probs = torch.softmax(all_logits_test*optimal_VS, axis=1)\n",
    "# certainties, y_pred = probs.max(axis=1)\n",
    "# correct = y_pred == all_labels_test\n",
    "# samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "# ece, _ = ECE_calc(samples_certainties)\n",
    "# auroc = AUROC(samples_certainties)\n",
    "# print('\\noptimal VS')\n",
    "# print(ece)\n",
    "# print(auroc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS to optimize selective classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWithTemperatureCustom(classifier)\n",
    "model.set_temperature(valid_loader)\n",
    "optimal_temp = model.temperature.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithVectorScalingCustom(nn.Module):\n",
    "    \"\"\"\n",
    "    A thin decorator, which wraps a model with temperature scaling\n",
    "    model (nn.Module):\n",
    "        A classification neural network\n",
    "        NB: Output of the neural network should be the classification logits,\n",
    "            NOT the softmax (or log softmax)!\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super(ModelWithVectorScalingCustom, self).__init__()\n",
    "        self.model = model.eval()\n",
    "        self.vector = nn.Parameter(torch.ones(1000) * 1.5)\n",
    "\n",
    "    def forward(self, input):\n",
    "        logits = self.model(input)\n",
    "        return self.temperature_scale(logits)\n",
    "\n",
    "    def temperature_scale(self, logits):\n",
    "        \"\"\"\n",
    "        Perform temperature scaling on logits\n",
    "        \"\"\"\n",
    "        # Expand temperature to match the size of logits\n",
    "        return logits * self.vector\n",
    "\n",
    "    # This function probably should live outside of this class, but whatever\n",
    "    def set_temperature(self, valid_loader):\n",
    "        \"\"\"\n",
    "        Tune the tempearature of the model (using the validation set).\n",
    "        We're going to set it to optimize NLL.\n",
    "        valid_loader (DataLoader): validation set loader\n",
    "        \"\"\"\n",
    "        self.cuda()\n",
    "        nll_criterion = nn.BCELoss().cuda()\n",
    "        ece_criterion = _ECELoss().cuda()\n",
    "\n",
    "        # First: collect all the logits and labels for the validation set\n",
    "        logits_list = []\n",
    "        probs_list = []\n",
    "        labels_list = []\n",
    "        train_labels_list = []\n",
    "        with torch.no_grad():\n",
    "            for input, label in valid_loader:\n",
    "                input = input.cuda()\n",
    "                label = label.cuda()\n",
    "                logits = self.model(input)\n",
    "                probs = torch.softmax(logits, axis=1)\n",
    "                y_pred = probs.argmax(axis=1)\n",
    "                correct = (y_pred == label).float()\n",
    "                logits_list.append(logits)\n",
    "                probs_list.append(probs)\n",
    "                train_labels_list.append(correct)\n",
    "                labels_list.append(label)\n",
    "            logits = torch.cat(logits_list).cuda()\n",
    "            train_probas = torch.max(torch.cat(probs_list).cuda(), 1, keepdim=True).values\n",
    "            labels = torch.cat(labels_list).cuda()\n",
    "            train_labels = torch.cat(train_labels_list).cuda().unsqueeze(1)\n",
    "\n",
    "        # Calculate NLL and ECE before temperature scaling\n",
    "        before_temperature_nll = nll_criterion(train_probas, train_labels).item()\n",
    "        before_temperature_ece = ece_criterion(logits, labels).item()\n",
    "        print('Before temperature - NLL: %.3f, ECE: %.3f' % (before_temperature_nll, before_temperature_ece))\n",
    "\n",
    "        # Next: optimize the temperature w.r.t. NLL\n",
    "        optimizer = torch.optim.LBFGS([self.vector], lr=0.01, max_iter=5000)\n",
    "\n",
    "        def eval():\n",
    "            optimizer.zero_grad()\n",
    "            scaled_logits = self.temperature_scale(logits)\n",
    "            probas = torch.softmax(scaled_logits, axis=1)\n",
    "            max_probas = torch.max(probas, 1, keepdim=True).values\n",
    "            loss = nll_criterion(max_probas, train_labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(eval)\n",
    "        # Calculate NLL and ECE after temperature scaling\n",
    "        scaled_logits = self.temperature_scale(logits)\n",
    "        probas = torch.softmax(scaled_logits, axis=1)\n",
    "        max_probas = torch.max(probas, 1, keepdim=True).values\n",
    "        after_temperature_nll = nll_criterion(max_probas, train_labels).item()\n",
    "        after_temperature_ece = ece_criterion(self.temperature_scale(logits), labels).item()\n",
    "        # print('Optimal temperature: %.3f' % self.vector)\n",
    "        print('After temperature - NLL: %.3f, ECE: %.3f' % (after_temperature_nll, after_temperature_ece))\n",
    "\n",
    "        return self\n",
    "\n",
    "model = ModelWithVectorScalingCustom(classifier)\n",
    "model.set_temperature(valid_loader)\n",
    "optimal_VS = model.vector.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithProbaScaling(nn.Module):\n",
    "    def __init__(self, model, validation_dataloader):\n",
    "        super(ModelWithProbaScaling, self).__init__()\n",
    "        self.model = model.eval().cuda()\n",
    "        self.coefficient = self.compute_coefficient(validation_dataloader)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        logits = self.model(input)\n",
    "        probas = torch.softmax(logits, axis=1)\n",
    "        probas = torch.clamp(probas * self.coefficient, 0, 1)\n",
    "        return probas\n",
    "    \n",
    "    def compute_coefficient(self, dataloader):\n",
    "        probs_list = []\n",
    "        labels_list = []\n",
    "        for input, label in dataloader:\n",
    "            input = input.cuda()\n",
    "            label = label.cuda()\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(input)\n",
    "            probs = torch.softmax(logits, axis=1)\n",
    "            probs_list.append(probs)\n",
    "            labels_list.append(label)\n",
    "        all_probas = torch.cat(probs_list).cuda()\n",
    "        all_labels = torch.cat(labels_list).cuda()    \n",
    "        certainties, y_pred = all_probas.max(axis=1)\n",
    "        correct = (y_pred == all_labels)\n",
    "        accuracy = correct.float().mean().item()\n",
    "        avg_confidence = certainties.mean().item()\n",
    "        coefficient = accuracy / avg_confidence\n",
    "        print(f'Accuracy: {accuracy:.3f}, Avg confidence: {avg_confidence:.3f}')\n",
    "        return coefficient\n",
    "    \n",
    "model_scaled_proba = ModelWithProbaScaling(classifier, valid_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits = torch.zeros((len(test_indices), 1000))\n",
    "all_labels = torch.zeros(len(test_indices), dtype=int)\n",
    "i = 0\n",
    "for x, y in test_loader:\n",
    "    with torch.no_grad():\n",
    "        logits = classifier(x.to(device))\n",
    "    all_logits[i:i+logits.shape[0], :] = logits.cpu()\n",
    "    all_labels[i:i+logits.shape[0]] = y.cpu()\n",
    "    i += logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "probs = torch.softmax(all_logits/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('original')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "t = baseline_temp\n",
    "probs = torch.softmax(all_logits/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nbaseline')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "# t = optimal_temp\n",
    "# probs = torch.softmax(all_logits/t, axis=1)\n",
    "# certainties, y_pred = probs.max(axis=1)\n",
    "# correct = y_pred == all_labels\n",
    "# samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "# ece, _ = ECE_calc(samples_certainties)\n",
    "# auroc = AUROC(samples_certainties)\n",
    "# print('\\noptimal')\n",
    "# print(ece)\n",
    "# print(auroc)\n",
    "\n",
    "\n",
    "probs = torch.softmax(all_logits*baseline_VS, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nbaseline VS')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "# probs = torch.softmax(all_logits*optimal_VS, axis=1)\n",
    "# certainties, y_pred = probs.max(axis=1)\n",
    "# correct = y_pred == all_labels\n",
    "# samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "# ece, _ = ECE_calc(samples_certainties)\n",
    "# auroc = AUROC(samples_certainties)\n",
    "# print('\\noptimal VS')\n",
    "# print(ece)\n",
    "# print(auroc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_probas = torch.zeros((len(test_indices), 1000))\n",
    "all_labels = torch.zeros(len(test_indices), dtype=int)\n",
    "i = 0\n",
    "for x, y in test_loader:\n",
    "    with torch.no_grad():\n",
    "        probas = model_scaled_proba(x.to(device))\n",
    "    all_probas[i:i+probas.shape[0], :] = probas.cpu()\n",
    "    all_labels[i:i+probas.shape[0]] = y.cpu()\n",
    "    i += probas.shape[0]\n",
    "    \n",
    "certainties, y_pred = all_probas.max(axis=1)\n",
    "correct = y_pred == all_labels\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('\\nscaled proba')\n",
    "print(ece)\n",
    "print(auroc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# netcal library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netcal.scaling import TemperatureScaling, LogisticCalibration\n",
    "from netcal.metrics import ECE, MMCE\n",
    "from netcal.presentation import ReliabilityDiagram\n",
    "\n",
    "n_bins = 15\n",
    "\n",
    "ece = ECE(n_bins)\n",
    "diagram = ReliabilityDiagram(n_bins)\n",
    "temperature = TemperatureScaling()\n",
    "\n",
    "all_logits_valid = torch.zeros((len(valid_indices), 1000))\n",
    "all_labels_valid = torch.zeros(len(valid_indices), dtype=int)\n",
    "i = 0\n",
    "for x, y in valid_loader:\n",
    "    with torch.no_grad():\n",
    "        logits = classifier(x.to(device))\n",
    "    all_logits_valid[i:i+logits.shape[0], :] = logits.cpu()\n",
    "    all_labels_valid[i:i+logits.shape[0]] = y.cpu()\n",
    "    i += logits.shape[0]\n",
    "confidences_valid = torch.softmax(all_logits_valid, axis=1).numpy()\n",
    "ground_truth_valid = all_labels_valid.numpy()\n",
    "confidences_test = torch.softmax(all_logits, axis=1).numpy()\n",
    "ground_truth_test = all_labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECE(n_bins).measure(confidences_test, ground_truth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netcal.metrics import ECE, MMCE\n",
    "MMCE().measure(confidences_test, ground_truth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram.plot(confidences_test, ground_truth_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = TemperatureScaling(method='mle')\n",
    "temperature.fit(confidences_valid, ground_truth_valid)\n",
    "calibrated = temperature.transform(confidences_test)\n",
    "ece.measure(calibrated, ground_truth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = TemperatureScaling(method='mle', use_cuda=True)\n",
    "temperature.fit(confidences_valid, ground_truth_valid)\n",
    "calibrated = temperature.transform(confidences_test)\n",
    "ece.measure(calibrated, ground_truth_test)\n",
    "temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LogisticCalibration()\n",
    "lc.fit(confidences_valid, ground_truth_valid)\n",
    "calibrated = lc.transform(confidences_test)\n",
    "ece.measure(calibrated, ground_truth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = TemperatureScaling()\n",
    "temperature.fit(np.array([[0.8, 0.2], [0.2, 0.8]]), np.array([0, 1])).temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = TemperatureScaling()\n",
    "temperature.fit(confidences_valid, np.ones(5000)).temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LogisticCalibration()\n",
    "lc.fit(np.array([[0.8, 0.2], [0.2, 0.8]]), np.array([0, 1]))\n",
    "# calibrated = lc.transform(confidences_test)\n",
    "# ece.measure(calibrated, ground_truth_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focal_calibration.Net.resnet import resnet50 as cifar_resnet50, resnet110 as cifar_resnet110\n",
    "from focal_calibration.Data import cifar100 as cifar100_loader, cifar10 as cifar10_loader\n",
    "from df_posthoc_calibration.assessment import toplabel_ece\n",
    "\n",
    "\n",
    "\n",
    "models_and_weights_path_cifar = {\n",
    "    'CIFAR10_resnet50': (cifar_resnet50, 'CIFAR10/resnet50_brier_score_350.model'),\n",
    "    'CIFAR10_resnet110': (cifar_resnet110, 'CIFAR10/resnet110_brier_score_350.model'),\n",
    "\n",
    "    'CIFAR100_resnet50': (cifar_resnet50, 'CIFAR100/resnet50_brier_score_350.model'),\n",
    "    'CIFAR100_resnet110': (cifar_resnet110, 'CIFAR100/resnet110_brier_score_430.model')\n",
    "}\n",
    "\n",
    "def convert_state_dict(state_dict):\n",
    "    # https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/4\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    return new_state_dict\n",
    "    \n",
    "model_name = 'CIFAR10_resnet50'\n",
    "num_classes = 10\n",
    "architecture, weights_path = models_and_weights_path_cifar[model_name]\n",
    "classifier = architecture(num_classes=num_classes, temp=1.0).eval().cuda()\n",
    "classifier.load_state_dict(convert_state_dict(torch.load('./focal_calibration_models/' + weights_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader, valid_loader = cifar100_loader.get_train_valid_loader(batch_size=256, augment=False, random_seed=1, data_dir=Path(os.path.expandvars('$DSDIR/'))) # seed=1 in original github\n",
    "# test_loader = cifar100_loader.get_test_loader(batch_size=256, data_dir=Path(os.path.expandvars('$DSDIR/')))\n",
    "\n",
    "train_loader, valid_loader = cifar10_loader.get_train_valid_loader(batch_size=256, augment=False, random_seed=1, data_dir=Path(os.path.expandvars('$DSDIR/'))) # seed=1 in original github\n",
    "test_loader = cifar10_loader.get_test_loader(batch_size=256, data_dir=Path(os.path.expandvars('$DSDIR/')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = test_loader\n",
    "\n",
    "all_logits = torch.zeros((len(loader.sampler), num_classes))\n",
    "all_labels = torch.zeros(len(loader.sampler), dtype=int)\n",
    "i = 0\n",
    "for x, y in loader:\n",
    "    with torch.no_grad():\n",
    "        logits = classifier(x.to(device))\n",
    "    all_logits[i:i+logits.shape[0], :] = logits.cpu()\n",
    "    all_labels[i:i+logits.shape[0]] = y.cpu()\n",
    "    i += logits.shape[0]\n",
    "    \n",
    "t = 1\n",
    "probs = torch.softmax(all_logits/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('original')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "t = optimal_temp\n",
    "probs = torch.softmax(all_logits/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('optimal')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "t = baseline_temp\n",
    "probs = torch.softmax(all_logits/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "auroc = AUROC(samples_certainties)\n",
    "print('baseline')\n",
    "print(ece)\n",
    "print(auroc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test histogram binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "class Multi_HB_binary():\n",
    "    def __init__(self, n_clusters, n_bins=15):\n",
    "        self.n_bins = n_bins\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_classes = 1000\n",
    "\n",
    "    def get_adaptive_bins(self, predictions, num_bins):\n",
    "        \"\"\"Returns upper edges for binning an equal number of datapoints per bin.\"\"\"\n",
    "        if np.size(predictions) == 0:\n",
    "            return np.linspace(0, 1, num_bins+1)[:-1]\n",
    "\n",
    "        edge_indices = np.linspace(0, len(predictions), num_bins, endpoint=False)\n",
    "\n",
    "        # Round into integers for indexing. If num_bins does not evenly divide\n",
    "        # len(predictions), this means that bin sizes will alternate between SIZE and\n",
    "        # SIZE+1.\n",
    "        edge_indices = np.round(edge_indices).astype(int)\n",
    "\n",
    "        # If there are many more bins than data points, some indices will be\n",
    "        # out-of-bounds by one. Set them to be within bounds:\n",
    "        edge_indices = np.minimum(edge_indices, len(predictions) - 1)\n",
    "\n",
    "        # Obtain the edge values:\n",
    "        edges = np.sort(predictions)[edge_indices]\n",
    "\n",
    "        # Following the convention of numpy.digitize, we do not include the leftmost\n",
    "        # edge (i.e. return the upper bin edges):\n",
    "        return edges[1:]\n",
    "\n",
    "    def fit(self, logits, labels):\n",
    "        probs = torch.softmax(logits, axis=1)\n",
    "        certainties, y_pred = probs.max(axis=1)\n",
    "        correct = (y_pred == labels)\n",
    "        avg_confid_per_class = torch.tensor([certainties[labels == l].mean().item() for l in range(self.n_classes)])\n",
    "        boundaries = torch.linspace(avg_confid_per_class.min()-1e-6, avg_confid_per_class.max(), self.n_clusters+1) # -1e-6 to include min value in first bucket\n",
    "        # boundaries = torch.tensor(self.get_adaptive_bins(avg_confid_per_class.numpy(), self.n_clusters))\n",
    "        self.buckets = torch.bucketize(avg_confid_per_class, boundaries)\n",
    "        # acc_per_class = torch.tensor([correct[labels == l].float().mean().item() for l in range(self.n_classes)])\n",
    "        # boundaries = torch.linspace(acc_per_class.min()-1e-6, acc_per_class.max(), self.n_clusters+1) # -1e-6 to include min value in first bucket\n",
    "        # self.buckets = torch.bucketize(acc_per_class, boundaries)\n",
    "\n",
    "        samples_buckets = torch.tensor([self.buckets[l] for l in y_pred])\n",
    "        self.hb_dict = {}\n",
    "        for b in self.buckets.unique().tolist():\n",
    "            hb = HB_binary(n_bins=self.n_bins)\n",
    "            print(len(certainties[samples_buckets == b]))\n",
    "            hb.fit(certainties[samples_buckets == b].numpy(), correct[samples_buckets == b].numpy())\n",
    "            self.hb_dict[b] = hb\n",
    "    \n",
    "    def predict_proba(self, logits):\n",
    "        probs = torch.softmax(logits, axis=1)\n",
    "        certainties, y_pred = probs.max(axis=1)\n",
    "\n",
    "        samples_buckets = torch.tensor([self.buckets[l] for l in y_pred])\n",
    "\n",
    "        all_probs_calib = torch.zeros_like(certainties)\n",
    "        for i in range(len(certainties)):\n",
    "            hb = self.hb_dict[samples_buckets[i].item()]\n",
    "            all_probs_calib[i] = torch.tensor(hb.predict_proba(certainties[i].cpu().numpy()))\n",
    "        \n",
    "        return all_probs_calib\n",
    "\n",
    "mhb = Multi_HB_binary(n_clusters=1)\n",
    "mhb.fit(all_logits_valid, all_labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit HB on full val set\n",
    "probs = torch.softmax(all_logits_valid, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_valid\n",
    "hb = HB_binary()\n",
    "hb.fit(certainties.numpy(), correct.numpy())\n",
    "\n",
    "# fit HB on partial val set\n",
    "probs = torch.softmax(all_logits_valid, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_valid\n",
    "labels_selected = torch.randperm(1000)[:980]\n",
    "idx = torch.isin(y_pred, labels_selected)\n",
    "hb_p = HB_binary()\n",
    "hb_p.fit(certainties[idx].numpy(), correct[idx].numpy())\n",
    "\n",
    "# EVAL\n",
    "probs = torch.softmax(all_logits_test, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "idx = torch.isin(y_pred, labels_selected)\n",
    "\n",
    "probs = torch.softmax(all_logits_test[idx], axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = torch.tensor(hb.predict_proba(certainties.cpu().numpy()))\n",
    "correct = y_pred == all_labels_test[idx]\n",
    "samples_certainties = torch.stack((certainties, correct), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nHB')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test[idx], axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = torch.tensor(hb_p.predict_proba(certainties.cpu().numpy()))\n",
    "correct = y_pred == all_labels_test[idx]\n",
    "samples_certainties = torch.stack((certainties, correct), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nHBP')\n",
    "print(ece)\n",
    "print(auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HB on full\n",
      "tensor(0.0096, dtype=torch.float64)\n",
      "0.799363718461624\n",
      "\n",
      "HBP on full\n",
      "tensor(0.0093, dtype=torch.float64)\n",
      "0.796778330984703\n",
      "\n",
      "HB on partial\n",
      "tensor(0.0093, dtype=torch.float64)\n",
      "0.7985101709779999\n",
      "\n",
      "HBP on partial\n",
      "tensor(0.0096, dtype=torch.float64)\n",
      "0.7956449808694515\n"
     ]
    }
   ],
   "source": [
    "# fit HB on full val set\n",
    "probs = torch.softmax(all_logits_valid, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_valid\n",
    "hb = HB_binary()\n",
    "hb.fit(certainties.numpy(), correct.numpy())\n",
    "\n",
    "# fit HB on partial val set\n",
    "probs = torch.softmax(all_logits_valid, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_valid\n",
    "labels_selected = torch.randperm(1000)[:900]\n",
    "idx = torch.isin(y_pred, labels_selected)\n",
    "hb_p = HB_binary()\n",
    "hb_p.fit(certainties[idx].numpy(), correct[idx].numpy())\n",
    "\n",
    "# EVAL\n",
    "probs = torch.softmax(all_logits_test, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = torch.tensor(hb.predict_proba(certainties.cpu().numpy()))\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties, correct), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nHB on full')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = torch.tensor(hb_p.predict_proba(certainties.cpu().numpy()))\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties, correct), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nHBP on full')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "idx = torch.isin(y_pred, labels_selected)\n",
    "\n",
    "probs = torch.softmax(all_logits_test[idx], axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = torch.tensor(hb.predict_proba(certainties.cpu().numpy()))\n",
    "correct = y_pred == all_labels_test[idx]\n",
    "samples_certainties = torch.stack((certainties, correct), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nHB on partial')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test[idx], axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = torch.tensor(hb_p.predict_proba(certainties.cpu().numpy()))\n",
    "correct = y_pred == all_labels_test[idx]\n",
    "samples_certainties = torch.stack((certainties, correct), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nHBP on partial')\n",
    "print(ece)\n",
    "print(auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "\n",
      "HB\n",
      "tensor(0.0096, dtype=torch.float64)\n",
      "0.799363718461624\n",
      "\n",
      "HBP\n",
      "tensor(0.0152, dtype=torch.float64)\n",
      "0.7999666069537394\n",
      "\n",
      "HBB\n",
      "tensor(0.0096, dtype=torch.float64)\n",
      "0.7991581363340243\n",
      "\n",
      "MHB\n",
      "tensor(0.0096)\n",
      "0.7988653676384246\n"
     ]
    }
   ],
   "source": [
    "# fit HB on full val set\n",
    "probs = torch.softmax(all_logits_valid, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_valid\n",
    "hb = HB_binary()\n",
    "hb.fit(certainties.numpy(), correct.numpy())\n",
    "\n",
    "# fit HB on partial val set\n",
    "probs = torch.softmax(all_logits_valid, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_valid\n",
    "idx = torch.randperm(certainties.size(0))[:3000]\n",
    "hb_p = HB_binary()\n",
    "hb_p.fit(certainties[idx].numpy(), correct[idx].numpy())\n",
    "\n",
    "# compute clusters and fit MHB on full val set\n",
    "mhb = Multi_HB_binary(n_clusters=1)\n",
    "mhb.fit(all_logits_valid, all_labels_valid)\n",
    "samples_buckets = torch.tensor([mhb.buckets[l] for l in y_pred])\n",
    "\n",
    "# fit HB on one bucket of val set\n",
    "b = 1\n",
    "hb_b = HB_binary()\n",
    "hb_b.fit(certainties[samples_buckets==b].numpy(), correct[samples_buckets==b].numpy())\n",
    "\n",
    "\n",
    "# EVAL\n",
    "probs = torch.softmax(all_logits_test, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "samples_buckets = torch.tensor([mhb.buckets[l] for l in y_pred])\n",
    "\n",
    "\n",
    "probs = torch.softmax(all_logits_test[samples_buckets==b], axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = torch.tensor(hb.predict_proba(certainties.cpu().numpy()))\n",
    "correct = y_pred == all_labels_test[samples_buckets==b]\n",
    "samples_certainties = torch.stack((certainties, correct), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nHB')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test[samples_buckets==b], axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = torch.tensor(hb_p.predict_proba(certainties.cpu().numpy()))\n",
    "correct = y_pred == all_labels_test[samples_buckets==b]\n",
    "samples_certainties = torch.stack((certainties, correct), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nHBP')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test[samples_buckets==b], axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = torch.tensor(hb_b.predict_proba(certainties.cpu().numpy()))\n",
    "correct = y_pred == all_labels_test[samples_buckets==b]\n",
    "samples_certainties = torch.stack((certainties, correct), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nHBB')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test[samples_buckets==b], axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = mhb.predict_proba(all_logits_test[samples_buckets==b])\n",
    "correct = y_pred == all_labels_test[samples_buckets==b]\n",
    "samples_certainties = torch.stack((certainties, correct), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nMHB')\n",
    "print(ece)\n",
    "print(auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.softmax(all_logits_test, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_buckets = torch.tensor([mhb.buckets[l] for l in y_pred])\n",
    "b = 0\n",
    "\n",
    "probs = torch.softmax(all_logits_test, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = torch.tensor(hb.predict_proba(certainties[samples_buckets==b].cpu().numpy()))\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties, correct[samples_buckets==b]), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nHB')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "\n",
    "probs = torch.softmax(all_logits_valid, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_valid\n",
    "samples_buckets_valid = torch.tensor([mhb.buckets[l] for l in y_pred])\n",
    "hb_b = HB_binary()\n",
    "hb_b.fit(certainties[samples_buckets_valid==b].numpy(), correct[samples_buckets_valid==b].numpy())\n",
    "\n",
    "probs = torch.softmax(all_logits_test, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = torch.tensor(hb_b.predict_proba(certainties[samples_buckets==b].cpu().numpy()))\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties, correct[samples_buckets==b]), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nHB on bucket')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = mhb.predict_proba(all_logits_test[samples_buckets==b])\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties, correct[samples_buckets==b]), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nMHB')\n",
    "print(ece)\n",
    "print(auroc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from df_posthoc_calibration.calibration import HB_binary, HB_toplabel\n",
    "\n",
    "all_logits_valid = torch.zeros((len(valid_indices), 1000))\n",
    "all_labels_valid = torch.zeros(len(valid_indices), dtype=int)\n",
    "i = 0\n",
    "for x, y in valid_loader:\n",
    "    with torch.no_grad():\n",
    "        logits = classifier(x.to(device))\n",
    "    all_logits_valid[i:i+logits.shape[0], :] = logits.cpu()\n",
    "    all_labels_valid[i:i+logits.shape[0]] = y.cpu()\n",
    "    i += logits.shape[0]\n",
    "\n",
    "probs = torch.softmax(all_logits_valid, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_valid\n",
    "\n",
    "hb = HB_binary()\n",
    "hb.fit(certainties.numpy(), correct.numpy())\n",
    "\n",
    "# hb_tl = HB_toplabel()\n",
    "# hb_tl.fit(probs.numpy(), all_labels_valid.numpy()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('original')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "t = baseline_temp\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nbaseline')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "t = optimal_temp\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\noptimal')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = torch.tensor(hb.predict_proba(certainties.cpu().numpy()))\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties, correct), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nHB')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = mhb.predict_proba(all_logits_test)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties, correct), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nMHB')\n",
    "print(ece)\n",
    "print(auroc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.softmax(all_logits_test, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "certainties = torch.tensor(hb.predict_proba(certainties.cpu().numpy()))\n",
    "certainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.softmax(all_logits_valid, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "\n",
    "hb = HB_binary()\n",
    "hb.fit(certainties.numpy(), all_labels_valid.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb.bin_upper_edges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test TS + HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.softmax(all_logits_valid, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_valid\n",
    "hb = HB_binary()\n",
    "hb.fit(certainties.numpy(), correct.numpy())\n",
    "\n",
    "t = baseline_temp\n",
    "probs = torch.softmax(all_logits_valid/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_valid\n",
    "hb_ts = HB_binary()\n",
    "hb_ts.fit(certainties.numpy(), correct.numpy())\n",
    "\n",
    "t = optimal_temp\n",
    "probs = torch.softmax(all_logits_valid/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_valid\n",
    "hb_bts = HB_binary()\n",
    "hb_bts.fit(certainties.numpy(), correct.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\n",
      "tensor(0.4117)\n",
      "0.8063290870659501\n",
      "\n",
      "baseline\n",
      "tensor(0.0297)\n",
      "0.8604652882588941\n",
      "\n",
      "optimal\n",
      "tensor(0.0137)\n",
      "0.8581556791927044\n",
      "\n",
      "HB\n",
      "tensor(0.0096, dtype=torch.float64)\n",
      "0.7991581363340243\n",
      "\n",
      "HB_TS\n",
      "tensor(0.0059, dtype=torch.float64)\n",
      "0.8574424065344745\n",
      "\n",
      "HB_BTS\n",
      "tensor(0.0069, dtype=torch.float64)\n",
      "0.8553329513898562\n"
     ]
    }
   ],
   "source": [
    "t = 1\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('original')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "t = baseline_temp\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nbaseline')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "\n",
    "t = optimal_temp\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties.cpu(), correct.cpu()), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\noptimal')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "probs = torch.softmax(all_logits_test, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = torch.tensor(hb.predict_proba(certainties.cpu().numpy()))\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties, correct), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nHB')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "t = baseline_temp\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = torch.tensor(hb_ts.predict_proba(certainties.cpu().numpy()))\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties, correct), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nHB_TS')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n",
    "t = optimal_temp\n",
    "probs = torch.softmax(all_logits_test/t, axis=1)\n",
    "certainties, y_pred = probs.max(axis=1)\n",
    "certainties = torch.tensor(hb_bts.predict_proba(certainties.cpu().numpy()))\n",
    "correct = y_pred == all_labels_test\n",
    "samples_certainties = torch.stack((certainties, correct), dim=1)\n",
    "auroc = AUROC(samples_certainties)\n",
    "ece, _ = ECE_calc(samples_certainties)\n",
    "# ece = gce(all_labels_test, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "print('\\nHB_BTS')\n",
    "print(ece)\n",
    "print(auroc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.0.0+py3.10.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
