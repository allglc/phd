{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./stylegan') \n",
    "sys.path.append('./stylegan/stylegan2')\n",
    " \n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from models import CNN_MNIST\n",
    "from temperature_scaling import ModelWithTemperature, _ECELoss\n",
    "from stylegan2.training.dataset import ImageFolderDataset\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "for p in [\n",
    "    Path('/d/alecoz/projects'), # DeepLab\n",
    "    Path(os.path.expandvars('$WORK')), # Jean Zay\n",
    "    Path('w:/')]: # local\n",
    "    if os.path.exists(p):\n",
    "        path_main = p\n",
    "# path_results = path_main / 'uncertainty-conditioned-gan/results'\n",
    "path_data = path_main / 'DATA'\n",
    "path_models = Path.cwd().parent / 'models' / 'MNIST'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data, classifer, and GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CORRUPTED\n",
    "path_model = path_models / 'stylegan2-training-runs' / '00004-mnist_stylegan2_blur_noise_maxSeverity5_proba100-cond-cifar-classifCond'\n",
    "path_classifier = path_models / 'classifier' / 'CNN_mnist_stylegan2_blur_noise_maxSeverity5_proba100_20230525_1128.pth'\n",
    "dataset_train = 'mnist_stylegan2_blur_noise_maxSeverity5_proba50'\n",
    "dataset_test = 'mnistTest_stylegan2_blur_noise_maxSeverity5_proba50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_images(images):\n",
    "    assert images.dim() == 4, \"Expected 4D (B x C x H x W) image tensor, got {}D\".format(images.dim())\n",
    "    images = ((images + 1) / 2).clamp(0, 1) # scale\n",
    "    images = images[:, :, 2:30, 2:30] # remove padding\n",
    "    return images\n",
    "\n",
    "def plot_images_grid(images, title=''):\n",
    "    images = images * 255\n",
    "    images = images.to(torch.uint8)\n",
    "    plt.figure()\n",
    "    plt.imshow(vutils.make_grid(images.cpu(), pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.grid(False)\n",
    "    plt.title(title)\n",
    "    \n",
    "def generate_random_images(n_images=5):\n",
    "    z = torch.randn((n_images, G.z_dim), device=device)\n",
    "    labels = torch.randint(0, n_classes, (n_images,), device=device)\n",
    "    c = F.one_hot(labels, n_classes)\n",
    "    ws = G.mapping(z, c, truncation_psi=1)\n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "    img = postprocess_images(img)\n",
    "    return img\n",
    "\n",
    "def get_classifier_MSP(logits):\n",
    "    max_softmax_proba = torch.max(torch.softmax(logits, axis=1), axis=1).values\n",
    "    return max_softmax_proba\n",
    "\n",
    "def get_classifier_TCP(logits, labels):\n",
    "    y_as_idx = F.one_hot(labels.long(), num_classes=10).bool()\n",
    "    probas = torch.softmax(logits, dim=1)\n",
    "    true_class_proba = probas[y_as_idx]\n",
    "    return true_class_proba\n",
    "\n",
    "def mahalanobis(U, v):\n",
    "    ''' Compute the Mahalanobis distance between each row of U and v'''\n",
    "    cov_inv = torch.linalg.inv(torch.cov(U.T))\n",
    "    m = torch.zeros(U.shape[0])\n",
    "    for i in range(U.shape[0]):\n",
    "        delta = U[i, :] - v\n",
    "        m[i] = torch.sqrt(torch.matmul(torch.matmul(delta, cov_inv), delta.T))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD GENERATOR\n",
    "if not str(path_model).endswith('pkl'):\n",
    "    # find best model in folder\n",
    "    with open(path_model / 'metric-fid50k_full.jsonl', 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    best_fid = 1e6\n",
    "    for json_str in json_list:\n",
    "        json_line = json.loads(json_str)\n",
    "        if json_line['results']['fid50k_full'] < best_fid:\n",
    "            best_fid = json_line['results']['fid50k_full']\n",
    "            best_model = json_line['snapshot_pkl']\n",
    "    print('Best FID: {:.2f} ; best model : {}'.format(best_fid, best_model))\n",
    "    path_model = path_model / best_model\n",
    "    with open(path_model, 'rb') as f:\n",
    "        G = pickle.load(f)['G_ema'].eval().to(device)  # torch.nn.Module\n",
    "else:\n",
    "    with dnnlib.util.open_url(path_model) as f:\n",
    "        G = legacy.load_network_pkl(f)['G_ema'].eval().requires_grad_(False).to(device)\n",
    "        \n",
    "\n",
    "# LOAD CLASSIFIER\n",
    "classifier = CNN_MNIST()\n",
    "classifier.load_state_dict(torch.load(path_classifier, map_location=device))\n",
    "classifier = classifier.eval().requires_grad_(False).to(device)\n",
    "\n",
    "\n",
    "# LOAD DATASET\n",
    "path_dataset = path_data / 'MNIST' / f'{dataset_train}.zip'\n",
    "train_data = ImageFolderDataset(path_dataset, use_labels=True)\n",
    "train_dataloader = DataLoader(train_data, batch_size=128)\n",
    "\n",
    "path_dataset = path_data / 'MNIST' / f'{dataset_test}.zip'\n",
    "test_data = ImageFolderDataset(path_dataset, use_labels=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for x, y in train_dataloader:\n",
    "    x = (x / 255)[:, :, 2:30, 2:30]\n",
    "    y = y.argmax(1)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = classifier(x)\n",
    "        pred = torch.max(logits, axis=1).indices\n",
    "        correct += (pred == y).sum().item()\n",
    "accuracy = correct / len(train_data)\n",
    "print('Accuracy on training set: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "correct = 0\n",
    "for x, y in test_dataloader:\n",
    "    x = (x / 255)[:, :, 2:30, 2:30]\n",
    "    y = y.argmax(1)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = classifier(x)\n",
    "        pred = torch.max(logits, axis=1).indices\n",
    "        correct += (pred == y).sum().item()\n",
    "accuracy = correct / len(test_data)\n",
    "\n",
    "print('Accuracy on test set: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dataloader:\n",
    "    x = (x / 255)[:, :, 2:30, 2:30]\n",
    "    y = y.argmax(1)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    break\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, figsize=(6, 6))\n",
    "for i in range(20): # for each image\n",
    "    ax = axs.flatten()[i]\n",
    "    img = x[i]\n",
    "    \n",
    "    logits = classifier(img.unsqueeze(0))\n",
    "    probas = torch.softmax(logits, axis=1)\n",
    "    msp, class_pred = torch.max(probas, axis=1)\n",
    "    \n",
    "    ax.imshow(img.cpu().numpy().squeeze(), vmin=0, vmax=1, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "    title = 'p({})={:.2f}'.format(class_pred.item(), msp.item())\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MSP_TCP(dataloader, classifier, device):\n",
    "\n",
    "    classifier.eval()\n",
    "    msp = torch.zeros((len(dataloader.dataset)))\n",
    "    tcp = torch.zeros((len(dataloader.dataset)))\n",
    "    idx = 0\n",
    "    for X, y in dataloader:\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        X = (X / 255)[:, :, 2:30, 2:30]\n",
    "        y = y.argmax(1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = classifier(X)\n",
    "        msp[idx:idx+batch_size] = get_classifier_MSP(logits)\n",
    "        tcp[idx:idx+batch_size] = get_classifier_TCP(logits, y)\n",
    "        idx += batch_size\n",
    "\n",
    "    return msp, tcp\n",
    "\n",
    "\n",
    "\n",
    "msp_train, tcp_train = get_MSP_TCP(train_dataloader, classifier, device)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.set_xlabel('MSP value')\n",
    "ax.hist(msp_train, alpha=0.5, bins=50, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions for calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ECELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates the Expected Calibration Error of a model.\n",
    "    (This isn't necessary for temperature scaling, just a cool metric).\n",
    "\n",
    "    The input to this loss is the logits of a model, NOT the softmax scores.\n",
    "\n",
    "    This divides the confidence outputs into equally-sized interval bins.\n",
    "    In each bin, we compute the confidence gap:\n",
    "\n",
    "    bin_gap = | avg_confidence_in_bin - accuracy_in_bin |\n",
    "\n",
    "    We then return a weighted average of the gaps, based on the number\n",
    "    of samples in each bin\n",
    "\n",
    "    See: Naeini, Mahdi Pakdaman, Gregory F. Cooper, and Milos Hauskrecht.\n",
    "    \"Obtaining Well Calibrated Probabilities Using Bayesian Binning.\" AAAI.\n",
    "    2015.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_bins=15):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(_ECELoss, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        if logits.shape[1] == 1: # binary classif\n",
    "            probas_class1 = torch.sigmoid(logits)\n",
    "            probas_class0 = 1 - probas_class1\n",
    "            softmaxes = torch.cat((probas_class0, probas_class1), dim=1)\n",
    "        else:\n",
    "            softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        if labels.shape[1] > 1: # one-hot embedding\n",
    "            labels = labels.argmax(1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "\n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ece_from_dataloader(model, dataloader):\n",
    "    # First: collect all the logits and labels for the validation set\n",
    "    logits_list = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for input, label in dataloader:\n",
    "            input = input.cuda()\n",
    "            input = (input / 255)[:, :, 2:30, 2:30]\n",
    "            logits = model(input)\n",
    "            logits_list.append(logits)\n",
    "            labels_list.append(label)\n",
    "        logits = torch.cat(logits_list).cuda()\n",
    "        labels = torch.cat(labels_list).cuda()\n",
    "    ece = _ECELoss()(logits, labels)\n",
    "\n",
    "    return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = path_data / 'MNIST' / f'{dataset_test}.zip'\n",
    "dataset = ImageFolderDataset(path_dataset, use_labels=True)\n",
    "dataset = test_data\n",
    "valid_size = 1000\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration data from real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ece_calib_before_TS = {}\n",
    "all_ece_test_before_TS = {}\n",
    "all_ece_calib_after_TS = {}\n",
    "all_ece_test_after_TS = {}\n",
    "\n",
    "for valid_size in np.geomspace(100, 5000, 5, dtype=int):\n",
    "    \n",
    "    valid_indices, test_indices = sklearn.model_selection.train_test_split(np.arange(len(dataset)), train_size=valid_size)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(valid_indices))\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(test_indices))\n",
    "\n",
    "    print(f'Calibration set size: {valid_size}')\n",
    "    all_ece_calib_before_TS[valid_size] = []\n",
    "    all_ece_test_before_TS[valid_size] = []\n",
    "    all_ece_calib_after_TS[valid_size] = []\n",
    "    all_ece_test_after_TS[valid_size] = []\n",
    "\n",
    "    ece_calib_before_TS = ece_from_dataloader(classifier, valid_loader)\n",
    "    ece_test_before_TS = ece_from_dataloader(classifier, test_loader)\n",
    "    \n",
    "    # Performing temperature scaling\n",
    "    model = ModelWithTemperature(classifier).to(device)\n",
    "    model.set_temperature(valid_loader)\n",
    "\n",
    "    ece_calib_after_TS = ece_from_dataloader(model, valid_loader)\n",
    "    ece_test_after_TS = ece_from_dataloader(model, test_loader)\n",
    "\n",
    "    all_ece_calib_before_TS[valid_size] += [ece_calib_before_TS.item()]\n",
    "    all_ece_test_before_TS[valid_size] += [ece_test_before_TS.item()]\n",
    "    all_ece_calib_after_TS[valid_size] += [ece_calib_after_TS.item()]\n",
    "    all_ece_test_after_TS[valid_size] += [ece_test_after_TS.item()]\n",
    "\n",
    "\n",
    "# PLOT\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "means = np.array([np.mean(v) for v in all_ece_calib_before_TS.values()])\n",
    "stds = np.array([np.std(v) for v in all_ece_calib_before_TS.values()])\n",
    "ax.plot(all_ece_calib_before_TS.keys(), means, label='ECE calib before TS')\n",
    "ax.fill_between(all_ece_calib_before_TS.keys(), means-stds, means+stds, alpha=0.5)\n",
    "\n",
    "means = np.array([np.mean(v) for v in all_ece_calib_after_TS.values()])\n",
    "stds = np.array([np.std(v) for v in all_ece_calib_after_TS.values()])\n",
    "ax.plot(all_ece_calib_after_TS.keys(), means, label='ECE calib after TS')\n",
    "ax.fill_between(all_ece_calib_after_TS.keys(), means-stds, means+stds, alpha=0.5)\n",
    "\n",
    "means = np.array([np.mean(v) for v in all_ece_test_before_TS.values()])\n",
    "stds = np.array([np.std(v) for v in all_ece_test_before_TS.values()])\n",
    "ax.plot(all_ece_test_before_TS.keys(), means, label='ECE test before TS')\n",
    "ax.fill_between(all_ece_test_before_TS.keys(), means-stds, means+stds, alpha=0.5)\n",
    "\n",
    "means = np.array([np.mean(v) for v in all_ece_test_after_TS.values()])\n",
    "stds = np.array([np.std(v) for v in all_ece_test_after_TS.values()])\n",
    "ax.plot(all_ece_test_after_TS.keys(), means, label='ECE test after TS')\n",
    "ax.fill_between(all_ece_test_after_TS.keys(), means-stds, means+stds, alpha=0.5)\n",
    "ax.set_xlabel('Calibration set size')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ece_test_after_TS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration data from synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset(n_samples=20000):\n",
    "\n",
    "    z = torch.randn(n_samples, gan.latent_dim, device=gan.device)\n",
    "    # if gan.condition_dim > 0:\n",
    "    rnd_label = torch.randint(2, size=(z.shape[0],), device=gan.device)\n",
    "    c = F.one_hot(rnd_label, num_classes=2)\n",
    "    confidence = get_MSP_correct(DataLoader(MoonsDataset(n_samples=n_samples, noise=noise, random_state=None), 1000), classifier.to(device), device)[0].unsqueeze(1) # confidence from real distrib\n",
    "    # confidence = 0.999*torch.ones((n_samples, 1), device=gan.device) # fixed confidence\n",
    "    # confidence = 0.5 + 0.5*torch.rand((n_samples, 1), device=gan.device) # uniform confidence\n",
    "    if gan.classifier_conditioning is not None:\n",
    "        c = torch.cat([c, confidence], dim=1)\n",
    "    z = torch.cat([z, c], dim=1)\n",
    "    with torch.no_grad():\n",
    "        w = gan.generator.mapping(z)\n",
    "        x_fake = gan.generator.synthesis(w).detach().cpu().numpy()\n",
    "\n",
    "    class SyntheticDataset(Dataset):\n",
    "\n",
    "        def __init__(self, x, y):\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.x)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "    synthetic_data = SyntheticDataset(x_fake, rnd_label.cpu().numpy().astype(float))\n",
    "\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "all_ece_calib_before_TS = {}\n",
    "all_ece_test_before_TS = {}\n",
    "all_ece_calib_after_TS = {}\n",
    "all_ece_test_after_TS = {}\n",
    "\n",
    "test_loader = DataLoader(MoonsDataset(n_samples=20000, noise=noise, random_state=2), batch_size=batch_size)\n",
    "\n",
    "for valid_size in np.linspace(100, 10000, 3, dtype=int):\n",
    "    print(f'Calibration set size: {valid_size}')\n",
    "    all_ece_calib_before_TS[valid_size] = []\n",
    "    all_ece_test_before_TS[valid_size] = []\n",
    "    all_ece_calib_after_TS[valid_size] = []\n",
    "    all_ece_test_after_TS[valid_size] = []\n",
    "\n",
    "    for trial in range(10):\n",
    "        synthetic_data = create_synthetic_dataset(valid_size)\n",
    "        valid_loader = DataLoader(synthetic_data, batch_size=batch_size)\n",
    "        ece_calib_before_TS = ece_from_dataloader(classifier, valid_loader)\n",
    "        ece_test_before_TS = ece_from_dataloader(classifier, test_loader)\n",
    "\n",
    "        # Performing temperature scaling\n",
    "        model = ModelWithTemperature(classifier).to(device)\n",
    "        model.set_temperature(valid_loader)\n",
    "\n",
    "        ece_calib_after_TS = ece_from_dataloader(model, valid_loader)\n",
    "        ece_test_after_TS = ece_from_dataloader(model, test_loader)\n",
    "\n",
    "        all_ece_calib_before_TS[valid_size] += [ece_calib_before_TS.item()]\n",
    "        all_ece_test_before_TS[valid_size] += [ece_test_before_TS.item()]\n",
    "        all_ece_calib_after_TS[valid_size] += [ece_calib_after_TS.item()]\n",
    "        all_ece_test_after_TS[valid_size] += [ece_test_after_TS.item()]\n",
    "\n",
    "\n",
    "# PLOT\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "means = np.array([np.mean(v) for v in all_ece_calib_before_TS.values()])\n",
    "stds = np.array([np.std(v) for v in all_ece_calib_before_TS.values()])\n",
    "ax.plot(all_ece_calib_before_TS.keys(), means, label='ECE calib before TS')\n",
    "ax.fill_between(all_ece_calib_before_TS.keys(), means-stds, means+stds, alpha=0.5)\n",
    "\n",
    "means = np.array([np.mean(v) for v in all_ece_calib_after_TS.values()])\n",
    "stds = np.array([np.std(v) for v in all_ece_calib_after_TS.values()])\n",
    "ax.plot(all_ece_calib_after_TS.keys(), means, label='ECE calib after TS')\n",
    "ax.fill_between(all_ece_calib_after_TS.keys(), means-stds, means+stds, alpha=0.5)\n",
    "\n",
    "means = np.array([np.mean(v) for v in all_ece_test_before_TS.values()])\n",
    "stds = np.array([np.std(v) for v in all_ece_test_before_TS.values()])\n",
    "ax.plot(all_ece_test_before_TS.keys(), means, label='ECE test before TS')\n",
    "ax.fill_between(all_ece_test_before_TS.keys(), means-stds, means+stds, alpha=0.5)\n",
    "\n",
    "means = np.array([np.mean(v) for v in all_ece_test_after_TS.values()])\n",
    "stds = np.array([np.std(v) for v in all_ece_test_after_TS.values()])\n",
    "ax.plot(all_ece_test_after_TS.keys(), means, label='ECE test after TS')\n",
    "ax.fill_between(all_ece_test_after_TS.keys(), means-stds, means+stds, alpha=0.5)\n",
    "ax.set_xlabel('Calibration set size')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ece_test_after_TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-1.10.0_py3.9.7",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-1.10.0_py3.9.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c522a5618233109531e8d8fb7f5c3011399924e76f9423af51315557ed1c4c11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
