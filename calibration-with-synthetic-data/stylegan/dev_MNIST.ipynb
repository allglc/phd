{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.utils as vutils\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from stylegan2.training.dataset import ImageFolderDataset\n",
    "import stylegan2.dnnlib\n",
    "import stylegan2.legacy\n",
    "from models import CNN_MNIST\n",
    "\n",
    "for p in [\n",
    "    Path('/d/alecoz/projects'), # DeepLab\n",
    "    Path(os.path.expandvars('$WORK')), # Jean Zay\n",
    "    Path('w:/')]: # local\n",
    "    if os.path.exists(p):\n",
    "        path_main = p\n",
    "path_results = path_main / 'uncertainty-conditioned-gan/results'\n",
    "path_data = path_main / 'DATA'\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_selected = 9\n",
    "class_selected = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CLEAN DATA\n",
    "# # path_model = path_results/ 'stylegan2-training-runs' / '00003-mnist_stylegan2-cond-cifar'\n",
    "# path_model = path_results/ 'stylegan2-training-runs' / '00005-mnist_stylegan2-cond-cifar-classifCond' # classifCond\n",
    "# path_classifier = path_results / 'classifiers' / 'CNN_mnist_clean_20230525_1119.pth'\n",
    "# dataset_train = 'mnist_stylegan2'\n",
    "# dataset_test = 'mnistTest_stylegan2'\n",
    "\n",
    "# DATA CORRUPTED\n",
    "path_model = path_results / 'stylegan2-training-runs' / '00006-mnist_stylegan2_blur_noise_maxSeverity5_proba50-cond-cifar-classifCond'\n",
    "path_classifier = path_results / 'classifiers' / 'CNN_mnist_stylegan2_blur_noise_maxSeverity5_proba50_20230525_1124.pth'\n",
    "dataset_train = 'mnist_stylegan2_blur_noise_maxSeverity5_proba50'\n",
    "dataset_test = 'mnistTest_stylegan2_blur_noise_maxSeverity5_proba50'\n",
    "\n",
    "# # DATA CORRUPTED (MORE)\n",
    "# path_model = path_results / 'stylegan2-training-runs' / '00007-mnist_stylegan2_blur_noise_maxSeverity5_proba100-cond-cifar-classifCond'\n",
    "# path_classifier = path_results / 'classifiers' / 'CNN_mnist_stylegan2_blur_noise_maxSeverity5_proba100_20230525_1128.pth'\n",
    "# dataset_train = 'mnist_stylegan2_blur_noise_maxSeverity5_proba100'\n",
    "# dataset_test = 'mnistTest_stylegan2_blur_noise_maxSeverity5_proba100'\n",
    "\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_images(images):\n",
    "    assert images.dim() == 4, \"Expected 4D (B x C x H x W) image tensor, got {}D\".format(images.dim())\n",
    "    images = ((images + 1) / 2).clamp(0, 1) # scale\n",
    "    images = images[:, :, 2:30, 2:30] # remove padding\n",
    "    return images\n",
    "\n",
    "def plot_images_grid(images, title=''):\n",
    "    images = images * 255\n",
    "    images = images.to(torch.uint8)\n",
    "    plt.figure()\n",
    "    plt.imshow(vutils.make_grid(images.cpu(), pad_value=255).permute(1,2,0), vmin=0, vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.grid(False)\n",
    "    plt.title(title)\n",
    "    \n",
    "def generate_random_images(n_images=5):\n",
    "    z = torch.randn((n_images, G.z_dim), device=device)\n",
    "    labels = torch.randint(0, n_classes, (n_images,), device=device)\n",
    "    c = F.one_hot(labels, n_classes)\n",
    "    ws = G.mapping(z, c, truncation_psi=1)\n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "    img = postprocess_images(img)\n",
    "    return img\n",
    "\n",
    "def get_classifier_MSP(logits):\n",
    "    max_softmax_proba = torch.max(torch.softmax(logits, axis=1), axis=1).values\n",
    "    return max_softmax_proba\n",
    "\n",
    "def get_classifier_TCP(logits, labels):\n",
    "    y_as_idx = F.one_hot(labels.long(), num_classes=10).bool()\n",
    "    probas = torch.softmax(logits, dim=1)\n",
    "    true_class_proba = probas[y_as_idx]\n",
    "    return true_class_proba\n",
    "\n",
    "def mahalanobis(U, v):\n",
    "    ''' Compute the Mahalanobis distance between each row of U and v'''\n",
    "    cov_inv = torch.linalg.inv(torch.cov(U.T))\n",
    "    m = torch.zeros(U.shape[0])\n",
    "    for i in range(U.shape[0]):\n",
    "        delta = U[i, :] - v\n",
    "        m[i] = torch.sqrt(torch.matmul(torch.matmul(delta, cov_inv), delta.T))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD GENERATOR\n",
    "if not str(path_model).endswith('pkl'):\n",
    "    # find best model in folder\n",
    "    with open(path_model / 'metric-fid50k_full.jsonl', 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "    best_fid = 1e6\n",
    "    for json_str in json_list:\n",
    "        json_line = json.loads(json_str)\n",
    "        if json_line['results']['fid50k_full'] < best_fid:\n",
    "            best_fid = json_line['results']['fid50k_full']\n",
    "            best_model = json_line['snapshot_pkl']\n",
    "    print('Best FID: {:.2f} ; best model : {}'.format(best_fid, best_model))\n",
    "    path_model = path_model / best_model\n",
    "    with open(path_model, 'rb') as f:\n",
    "        G = pickle.load(f)['G_ema'].eval().to(device)  # torch.nn.Module\n",
    "else:\n",
    "    with dnnlib.util.open_url(path_model) as f:\n",
    "        G = legacy.load_network_pkl(f)['G_ema'].eval().requires_grad_(False).to(device)\n",
    "        \n",
    "\n",
    "# LOAD CLASSIFIER\n",
    "classifier = CNN_MNIST()\n",
    "classifier.load_state_dict(torch.load(path_classifier, map_location=device))\n",
    "classifier = classifier.eval().requires_grad_(False).to(device)\n",
    "\n",
    "\n",
    "# LOAD DATASET\n",
    "path_dataset = path_data / 'MNIST' / f'{dataset_train}.zip'\n",
    "train_data = ImageFolderDataset(path_dataset, use_labels=True)\n",
    "train_dataloader = DataLoader(train_data, batch_size=128)\n",
    "\n",
    "path_dataset = path_data / 'MNIST' / f'{dataset_test}.zip'\n",
    "test_data = ImageFolderDataset(path_dataset, use_labels=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for x, y in train_dataloader:\n",
    "    x = (x / 255)[:, :, 2:30, 2:30]\n",
    "    y = y.argmax(1)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = classifier(x)\n",
    "        pred = torch.max(logits, axis=1).indices\n",
    "        correct += (pred == y).sum().item()\n",
    "accuracy = correct / len(train_data)\n",
    "print('Accuracy on training set: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "correct = 0\n",
    "for x, y in test_dataloader:\n",
    "    x = (x / 255)[:, :, 2:30, 2:30]\n",
    "    y = y.argmax(1)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = classifier(x)\n",
    "        pred = torch.max(logits, axis=1).indices\n",
    "        correct += (pred == y).sum().item()\n",
    "accuracy = correct / len(test_data)\n",
    "\n",
    "print('Accuracy on test set: {:.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dataloader:\n",
    "    x = (x / 255)[:, :, 2:30, 2:30]\n",
    "    y = y.argmax(1)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    break\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, figsize=(6, 6))\n",
    "for i in range(20): # for each image\n",
    "    ax = axs.flatten()[i]\n",
    "    img = x[i]\n",
    "    \n",
    "    logits = classifier(img.unsqueeze(0))\n",
    "    probas = torch.softmax(logits, axis=1)\n",
    "    msp, class_pred = torch.max(probas, axis=1)\n",
    "    \n",
    "    ax.imshow(img.cpu().numpy().squeeze(), vmin=0, vmax=1, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "    title = 'p({})={:.2f}'.format(class_pred.item(), msp.item())\n",
    "    ax.set_title(title)\n",
    "    \n",
    "plt.savefig('results_mnist_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MSP_TCP(dataloader, classifier, device):\n",
    "\n",
    "    classifier.eval()\n",
    "    msp = torch.zeros((len(dataloader.dataset)))\n",
    "    tcp = torch.zeros((len(dataloader.dataset)))\n",
    "    idx = 0\n",
    "    for X, y in dataloader:\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        X = (X / 255)[:, :, 2:30, 2:30]\n",
    "        y = y.argmax(1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = classifier(X)\n",
    "        msp[idx:idx+batch_size] = get_classifier_MSP(logits)\n",
    "        tcp[idx:idx+batch_size] = get_classifier_TCP(logits, y)\n",
    "        idx += batch_size\n",
    "\n",
    "    return msp, tcp\n",
    "\n",
    "\n",
    "\n",
    "msp_train, tcp_train = get_MSP_TCP(train_dataloader, classifier, device)\n",
    "# msp_test, tcp_test = get_MSP_TCP(test_dataloader, classifier, device)\n",
    "\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# ax1.set_xlabel('train')\n",
    "# ax1.hist(msp_train, alpha=0.5, bins=50, label='MSP')\n",
    "# ax1.hist(tcp_train, alpha=0.5, bins=50, label='TCP')\n",
    "# ax1.legend()\n",
    "\n",
    "# ax2.set_xlabel('test')\n",
    "# ax2.hist(msp_test, alpha=0.5, bins=50, label='MSP')\n",
    "# ax2.hist(tcp_test, alpha=0.5, bins=50, label='TCP')\n",
    "# ax2.legend()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.set_xlabel('MSP value')\n",
    "ax.hist(msp_train, alpha=0.5, bins=50, log=True)\n",
    "\n",
    "plt.savefig('results_mnist_classifier_MSP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_low_TCP(dataloader, classifier, device, n_images):\n",
    "    classifier.eval()\n",
    "    images = torch.zeros((n_images, 1, 28, 28))\n",
    "    idx = 0\n",
    "    for X, y in dataloader:\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        X = (X / 255)[:, :, 2:30, 2:30]\n",
    "        y = y.argmax(1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = classifier(X)\n",
    "        tcp = get_classifier_TCP(logits, y)\n",
    "        idx_low_tcp = tcp < 0.05\n",
    "        n_images_low_tcp = idx_low_tcp.int().sum()\n",
    "        print(n_images_low_tcp)\n",
    "        if idx+n_images_low_tcp <= n_images:\n",
    "            images[idx:idx+n_images_low_tcp] = X[idx_low_tcp]\n",
    "            idx += n_images_low_tcp\n",
    "        else:\n",
    "            images[idx:] = X[idx_low_tcp][:n_images - (idx+n_images_low_tcp)]\n",
    "            break\n",
    " \n",
    "    return images\n",
    "\n",
    "\n",
    "img = get_images_low_TCP(train_dataloader, classifier, device, 18)\n",
    "\n",
    "plot_images_grid(img, title=f'Generated images, low TCP')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = generate_random_images(n_images=5)\n",
    "# plot_images_grid(img, title='Generated images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if G.c_dim == 11:\n",
    "    n_images = 8\n",
    "\n",
    "    z = torch.randn((n_images, G.z_dim), device=device)\n",
    "    if class_selected == 'all':\n",
    "        labels = torch.randint(0, n_classes, (n_images,), device=device)\n",
    "    else:\n",
    "        labels = class_selected * torch.ones((n_images,), dtype=torch.int64, device=device)\n",
    "    c_labels = F.one_hot(labels, n_classes)\n",
    "    c_classif = torch.ones((n_images, 1), device=device)\n",
    "    c = torch.cat((c_labels, c_classif), dim=1)\n",
    "\n",
    "    ws = G.mapping(z, c, truncation_psi=1)\n",
    "    imgs = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "    imgs = postprocess_images(imgs)\n",
    "    \n",
    "    # plot\n",
    "    fig, axs = plt.subplots(1, 8, figsize=(10, 2))\n",
    "    fig.suptitle('Generated images, MSP condition=1')\n",
    "    for i in range(8): # for each image\n",
    "        ax = axs.flatten()[i]\n",
    "        img = imgs[i]\n",
    "\n",
    "        logits = classifier(img.unsqueeze(0))\n",
    "        probas = torch.softmax(logits, axis=1)\n",
    "        msp, class_pred = torch.max(probas, axis=1)\n",
    "\n",
    "        ax.imshow(img.cpu().numpy().squeeze(), vmin=0, vmax=1, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.grid(False)\n",
    "        title = 'p({})={:.2f}'.format(class_pred.item(), msp.item())\n",
    "        ax.set_title(title)\n",
    "    plt.savefig('results_mnist_generator_confid1')\n",
    "\n",
    "    # change only classif condition\n",
    "    proba = 0.7\n",
    "    c_classif = proba * torch.ones((n_images, 1), device=device)\n",
    "    c = torch.cat((c_labels, c_classif), dim=1)\n",
    "    \n",
    "    ws = G.mapping(z, c, truncation_psi=1)\n",
    "    imgs = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "    imgs = postprocess_images(imgs)\n",
    "    \n",
    "    # plot\n",
    "    fig, axs = plt.subplots(1, 8, figsize=(10, 2))\n",
    "    fig.suptitle('Generated images, MSP condition=0.7')\n",
    "    for i in range(8): # for each image\n",
    "        ax = axs.flatten()[i]\n",
    "        img = imgs[i]\n",
    "\n",
    "        logits = classifier(img.unsqueeze(0))\n",
    "        probas = torch.softmax(logits, axis=1)\n",
    "        msp, class_pred = torch.max(probas, axis=1)\n",
    "\n",
    "        ax.imshow(img.cpu().numpy().squeeze(), vmin=0, vmax=1, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.grid(False)\n",
    "        title = 'p({})={:.2f}'.format(class_pred.item(), msp.item())\n",
    "        ax.set_title(title)\n",
    "    plt.savefig('results_mnist_generator_confid07')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if G.c_dim == 11:\n",
    "    n_images = 3000\n",
    "    \n",
    "    if class_selected == 'all':\n",
    "        labels = torch.randint(0, n_classes, (n_images,), device=device)\n",
    "    else:\n",
    "        labels = class_selected * torch.ones((n_images,), dtype=torch.int64, device=device)\n",
    "    c_labels = F.one_hot(labels, n_classes)\n",
    "    # uniform confidence\n",
    "    confid_in = torch.linspace(0.1, 1, n_images, device=device).unsqueeze(1)\n",
    "    c = torch.cat((c_labels, confid_in), dim=1)\n",
    "    \n",
    "    confid_out = torch.empty(0, device=device)\n",
    "    for c_i in DataLoader(c, 100):\n",
    "        z = torch.randn((c_i.shape[0], G.z_dim), device=device)\n",
    "\n",
    "        ws = G.mapping(z, c_i, truncation_psi=1)\n",
    "        img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "        img = postprocess_images(img)\n",
    "        confid_out = torch.cat((confid_out, F.softmax(classifier(img), dim=1).max(dim=1).values))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot([0.1, 1], [0.1, 1])\n",
    "    plt.scatter(confid_in.cpu(), confid_out.cpu(), alpha=0.5)\n",
    "    plt.xlabel('confidence in')\n",
    "    plt.ylabel('confidence out')\n",
    "    \n",
    "    plt.savefig('results_mnist_generator_MSP')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSP direction in W\n",
    "\n",
    "tester ajouter direction seulement a w_7 ou w_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if G.c_dim == 11:\n",
    "    n_images = 10000\n",
    "    confidence_max = 1\n",
    "    confidence_min = 0.9\n",
    "\n",
    "    z = torch.randn((n_images, G.z_dim), device=device)\n",
    "    if class_selected == 'all':\n",
    "        labels = torch.randint(0, n_classes, (n_images,), device=device)\n",
    "    else:\n",
    "        labels = class_selected * torch.ones((n_images,), dtype=torch.int64, device=device)\n",
    "    c_labels = F.one_hot(labels, n_classes)\n",
    "    c_classif = confidence_max*torch.ones((n_images, 1), device=device)\n",
    "    c = torch.cat((c_labels, c_classif), dim=1)\n",
    "    w_max = G.mapping(z, c, truncation_psi=1)[:,0,:].mean(dim=0)\n",
    "\n",
    "    # WITH NEW Z\n",
    "    # z = torch.randn((n_images, G.z_dim), device=device)\n",
    "    # labels = torch.randint(0, n_classes, (n_images,), device=device)\n",
    "    # c_labels = F.one_hot(labels, n_classes)\n",
    "    # c_classif = confidence_min*torch.ones((n_images, 1), device=device)\n",
    "    # c = torch.cat((c_labels, c_classif), dim=1)\n",
    "    # w_min = G.mapping(z, c, truncation_psi=1)[:,0,:].mean(dim=0)\n",
    "\n",
    "    # WITHOUT NEW Z\n",
    "    c_classif = confidence_min*torch.ones((n_images, 1), device=device)\n",
    "    c = torch.cat((c_labels, c_classif), dim=1)\n",
    "    w_min = G.mapping(z, c, truncation_psi=1)[:,0,:].mean(dim=0)\n",
    "\n",
    "    w_dir_msp = (w_max - w_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if G.c_dim == 11:\n",
    "    n_images = 8\n",
    "    lambda_ = 2\n",
    "\n",
    "    z = torch.randn((n_images, G.z_dim), device=device)\n",
    "    if class_selected == 'all':\n",
    "        labels = torch.randint(0, n_classes, (n_images,), device=device)\n",
    "    else:\n",
    "        labels = class_selected * torch.ones((n_images,), dtype=torch.int64, device=device)\n",
    "    c_labels = F.one_hot(labels, n_classes)\n",
    "    c_classif = torch.ones((n_images, 1), device=device)\n",
    "    c = torch.cat((c_labels, c_classif), dim=1)\n",
    "\n",
    "    ws = G.mapping(z, c, truncation_psi=1)\n",
    "    img = G.synthesis(ws, noise_mode='const', force_fp32=True)\n",
    "    img = postprocess_images(img)\n",
    "    plot_images_grid(img, title='Generated images, confidence=1')\n",
    "    plt.show()\n",
    "    print('confidences:', F.softmax(classifier(img), dim=1).max(dim=1).values)\n",
    "\n",
    "    # move along w_dir_msp\n",
    "\n",
    "    ws_moved = ws - lambda_ * w_dir_msp\n",
    "    img = G.synthesis(ws_moved, noise_mode='const', force_fp32=True)\n",
    "    img = postprocess_images(img)\n",
    "    plot_images_grid(img, title=f'Generated images, edited along w_dir_msp')\n",
    "    plt.show()\n",
    "    print('confidences:', F.softmax(classifier(img), dim=1).max(dim=1).values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_from_ws(ws, labels=None):\n",
    "    dataloader = DataLoader(TensorDataset(ws, labels), 128)\n",
    "    msp = torch.empty(ws.shape[0])\n",
    "    tcp = torch.empty(ws.shape[0])\n",
    "    idx_batch = 0\n",
    "    for ws_, y in dataloader:\n",
    "        batch_size = ws_.shape[0]\n",
    "        img = G.synthesis(ws_, noise_mode='const')\n",
    "        img = postprocess_images(img)\n",
    "        logits = classifier(img)\n",
    "        msp[idx_batch:idx_batch+batch_size] = get_classifier_MSP(logits).cpu()\n",
    "        tcp[idx_batch:idx_batch+batch_size] = get_classifier_TCP(logits, y).cpu()\n",
    "        idx_batch += batch_size\n",
    "    well_classified = msp == tcp\n",
    "    return msp, tcp, well_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "if class_selected == 'all':\n",
    "    n_images_train = 60000\n",
    "    labels_train = torch.randint(0, n_classes, (n_images_train,), device=device)\n",
    "else:\n",
    "    n_images_train = 60000\n",
    "    labels_train = class_selected * torch.ones((n_images_train,), dtype=torch.int64, device=device)\n",
    "z_train = torch.randn((n_images_train, G.z_dim), device=device)\n",
    "c_labels_train = F.one_hot(labels_train, n_classes)\n",
    "if G.c_dim == 11:\n",
    "    c_classif_train = 0.8 + 0.2 * torch.rand((n_images_train, 1), device=device)\n",
    "    c_train = torch.cat((c_labels_train, c_classif_train), dim=1)\n",
    "elif G.c_dim == 10:\n",
    "    c_train = c_labels_train\n",
    "\n",
    "ws_train = G.mapping(z_train, c_train, truncation_psi=1)\n",
    "w_train = ws_train[:, 0, :]\n",
    "msp_train, tcp_train, well_classified_train = classify_from_ws(ws_train, labels_train)\n",
    "\n",
    "# VAL\n",
    "if class_selected == 'all':\n",
    "    n_images_val = 10000\n",
    "    labels_val = torch.randint(0, n_classes, (n_images_val,), device=device)\n",
    "else:\n",
    "    n_images_val = 10000\n",
    "    labels_val = class_selected * torch.ones((n_images_val,), dtype=torch.int64, device=device)\n",
    "z_val = torch.randn((n_images_val, G.z_dim), device=device)\n",
    "c_labels_val = F.one_hot(labels_val, n_classes)\n",
    "if G.c_dim == 11:\n",
    "    c_classif_val = 0.8 + 0.2 * torch.rand((n_images_val, 1), device=device)\n",
    "    c_val = torch.cat((c_labels_val, c_classif_val), dim=1)\n",
    "elif G.c_dim == 10:\n",
    "    c_val = c_labels_val\n",
    "\n",
    "ws_val = G.mapping(z_val, c_val, truncation_psi=1)\n",
    "w_val = ws_val[:, 0, :]\n",
    "msp_val, tcp_val, well_classified_val = classify_from_ws(ws_val, labels_val)\n",
    "\n",
    "\n",
    "perfo_direction = (((w_train[well_classified_train.logical_not()] - w_train.mean(0)) / w_train.std(0)).mean(0)\n",
    "                       - ((w_train[well_classified_train] - w_train.mean(0)) / w_train.std(0)).mean(0))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "w_embedded = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(w_train[:n_samples, :].cpu().numpy())\n",
    "\n",
    "top_k = 10\n",
    "top_k_dims = (-np.abs(perfo_direction.cpu())).argsort()[:top_k]\n",
    "w_embedded_topk = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(w_train[:n_samples, top_k_dims].cpu().numpy())\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(10, 10))\n",
    "for i, w_emb in enumerate([w_embedded, w_embedded_topk]):\n",
    "    sc0 = axs[0, i].scatter(w_emb[:, 0], w_emb[:, 1], c=well_classified_train[:n_samples].cpu(), alpha=0.2, cmap='viridis')\n",
    "    sc1 = axs[1, i].scatter(w_emb[:, 0], w_emb[:, 1], c=msp_train[:n_samples].cpu(), alpha=0.2, cmap='viridis')\n",
    "    if G.c_dim == 11:\n",
    "        sc2 = axs[2, i].scatter(w_emb[:, 0], w_emb[:, 1], c=c_classif_train[:n_samples].cpu(), alpha=0.2, cmap='viridis')\n",
    "        fig.colorbar(sc2, ax=axs[2, i], label='MSP input')\n",
    "    fig.colorbar(sc0, ax=axs[0, i], label='well-classified')\n",
    "    fig.colorbar(sc1, ax=axs[1, i], label='MSP')\n",
    "fig.tight_layout()\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "axs[0, 0].set_title('W - All dimensions')\n",
    "axs[0, 1].set_title(f'W - Top {top_k} dimensions')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain defined by MSP inputidx_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline: max softmax\n",
    "domain_cutoff_baseline = np.linspace(0, 1, 1000)\n",
    "coverage_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "acc_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "for i, cut in enumerate(domain_cutoff_baseline):\n",
    "    idx_domain = msp_train > cut\n",
    "    coverage_baseline[i] = idx_domain.float().mean()\n",
    "    acc_baseline[i] = well_classified_train[idx_domain].float().mean()\n",
    "    \n",
    "# msp input of generator\n",
    "domain_cutoff_mspIn = np.linspace(0, 1, 1000)\n",
    "coverage_mspIn = np.zeros_like(domain_cutoff_mspIn)\n",
    "acc_mspIn = np.zeros_like(domain_cutoff_mspIn)\n",
    "for i, cut in enumerate(domain_cutoff_mspIn):\n",
    "    idx_domain = c_classif_train.squeeze().cpu() > cut\n",
    "    coverage_mspIn[i] = idx_domain.float().mean()\n",
    "    acc_mspIn[i] = well_classified_train[idx_domain].float().mean()\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.set_title('coverage vs. accuracy')\n",
    "ax.plot(coverage_baseline, acc_baseline, c='k', label='baseline (max softmax)')\n",
    "ax.plot(coverage_mspIn, acc_mspIn, label='MSP input of GAN')\n",
    "ax.legend()\n",
    "ax.set_xlabel('coverage')\n",
    "ax.set_ylabel('accuracy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain as distance from MSP=1 center\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_center_msp1 = w_train[(msp_train > 0.99), :].mean(dim=0, keepdims=True)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\n",
    "fig.suptitle('coverage vs. accuracy (distance from MSP>0.99 center)')\n",
    "ax1.set_title('cosine similarity')\n",
    "ax2.set_title('euclidian distance')\n",
    "ax3.set_title('Mahalanobis distance')\n",
    "\n",
    "for top_k in [2, 10, 20, 30, 50, 100, 200, w_train.shape[-1]]:\n",
    "    perfo_direction = (((w_train[well_classified_train.logical_not()] - w_train.mean(0)) / w_train.std(0)).mean(0)\n",
    "                       - ((w_train[well_classified_train] - w_train.mean(0)) / w_train.std(0)).mean(0))\n",
    "    top_k_dims = (-np.abs(perfo_direction.cpu())).argsort()[:top_k]\n",
    "\n",
    "    mahalanobis_dist_train = mahalanobis(w_train[:, top_k_dims], w_center_msp1[:, top_k_dims])\n",
    "    euclidian_dist_train = torch.cdist(w_train[:, top_k_dims], w_center_msp1[:, top_k_dims]).squeeze().cpu()\n",
    "    cosine_sim_train = torch.nn.functional.cosine_similarity(w_train[:, top_k_dims], w_center_msp1[:, top_k_dims]).cpu()\n",
    "    c_classif = c_classif.squeeze().cpu()\n",
    "\n",
    "    # cosine similarity\n",
    "    domain_cutoff_cosineSim = np.linspace(0, 1, 1000)\n",
    "    coverage_cosineSim = np.zeros_like(domain_cutoff_cosineSim)\n",
    "    acc_cosineSim = np.zeros_like(domain_cutoff_cosineSim)\n",
    "    for i, cut in enumerate(domain_cutoff_cosineSim):\n",
    "        idx_domain = cosine_sim_train > cut\n",
    "        coverage_cosineSim[i] = idx_domain.float().mean()\n",
    "        acc_cosineSim[i] = well_classified_train[idx_domain].float().mean()\n",
    "    ax1.plot(coverage_cosineSim, acc_cosineSim, label=f'cosine similarity - top {top_k} dims')\n",
    "\n",
    "    # euclidian distance\n",
    "    domain_cutoff_eucliDist = np.linspace(0, 1, 1000)\n",
    "    coverage_eucliDist = np.zeros_like(domain_cutoff_eucliDist)\n",
    "    acc_eucliDist = np.zeros_like(domain_cutoff_eucliDist)\n",
    "    for i, cut in enumerate(domain_cutoff_eucliDist):\n",
    "        idx_domain = (euclidian_dist_train / euclidian_dist_train.max()) < cut\n",
    "        coverage_eucliDist[i] = idx_domain.float().mean()\n",
    "        acc_eucliDist[i] = well_classified_train[idx_domain].float().mean()\n",
    "    ax2.plot(coverage_eucliDist, acc_eucliDist, label=f'euclidian distance - top {top_k} dims')\n",
    "    \n",
    "    # mahalanobis distance\n",
    "    domain_cutoff_Maha = np.linspace(0, 1, 1000)\n",
    "    coverage_Maha = np.zeros_like(domain_cutoff_Maha)\n",
    "    acc_Maha = np.zeros_like(domain_cutoff_Maha)\n",
    "    for i, cut in enumerate(domain_cutoff_Maha):\n",
    "        idx_domain = (mahalanobis_dist_train / mahalanobis_dist_train.max()) < cut\n",
    "        coverage_Maha[i] = idx_domain.float().mean()\n",
    "        acc_Maha[i] = well_classified_train[idx_domain].float().mean()\n",
    "    ax3.plot(coverage_Maha, acc_Maha, label=f'euclidian distance - top {top_k} dims')\n",
    "\n",
    "# baseline: max softmax\n",
    "domain_cutoff_baseline = np.linspace(0, 1, 1000)\n",
    "coverage_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "acc_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "for i, cut in enumerate(domain_cutoff_baseline):\n",
    "    idx_domain = msp_train > cut\n",
    "    coverage_baseline[i] = idx_domain.float().mean()\n",
    "    acc_baseline[i] = well_classified_train[idx_domain].float().mean()\n",
    "\n",
    "for ax in [ax1, ax2, ax3]:\n",
    "    ax.plot(coverage_baseline, acc_baseline, c='k', label='baseline (max softmax)')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('coverage')\n",
    "    ax.set_ylabel('accuracy')\n",
    "    ax.set_ylim(0.99*np.nanmin(acc_baseline), 1.01*np.nanmax(acc_baseline))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain as distance from well-classified center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_center_wellclassified = w_train[well_classified_train, :].mean(dim=0, keepdims=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('coverage vs. accuracy (distance from well-classified center)')\n",
    "ax1.set_title('cosine similarity')\n",
    "ax2.set_title('euclidian distance')\n",
    "\n",
    "for top_k in [1, 10, 20, 50, 100, w_train.shape[-1]]:\n",
    "    perfo_direction = (((w_train[well_classified_train.logical_not()] - w_train.mean(0)) / w_train.std(0)).mean(0)\n",
    "                       - ((w_train[well_classified_train] - w_train.mean(0)) / w_train.std(0)).mean(0))\n",
    "    top_k_dims = (-np.abs(perfo_direction.cpu())).argsort()[:top_k]\n",
    "\n",
    "    euclidian_dist_train = torch.cdist(w_train[:, top_k_dims], w_center_wellclassified[:, top_k_dims]).squeeze().cpu()\n",
    "    cosine_sim_train = torch.nn.functional.cosine_similarity(w_train[:, top_k_dims], w_center_wellclassified[:, top_k_dims]).cpu()\n",
    "    c_classif = c_classif.squeeze().cpu()\n",
    "\n",
    "    # cosine similarity\n",
    "    domain_cutoff_cosineSim = np.linspace(0, 1, 1000)\n",
    "    coverage_cosineSim = np.zeros_like(domain_cutoff_cosineSim)\n",
    "    acc_cosineSim = np.zeros_like(domain_cutoff_cosineSim)\n",
    "    for i, cut in enumerate(domain_cutoff_cosineSim):\n",
    "        idx_domain = cosine_sim_train > cut\n",
    "        coverage_cosineSim[i] = idx_domain.float().mean()\n",
    "        acc_cosineSim[i] = well_classified_train[idx_domain].float().mean()\n",
    "    ax1.plot(coverage_cosineSim, acc_cosineSim, label=f'cosine similarity - top {top_k} dims')\n",
    "\n",
    "    # euclidian distance\n",
    "    domain_cutoff_eucliDist = np.linspace(0, 1, 1000)\n",
    "    coverage_eucliDist = np.zeros_like(domain_cutoff_eucliDist)\n",
    "    acc_eucliDist = np.zeros_like(domain_cutoff_eucliDist)\n",
    "    for i, cut in enumerate(domain_cutoff_eucliDist):\n",
    "        idx_domain = (euclidian_dist_train / euclidian_dist_train.max()) < cut\n",
    "        coverage_eucliDist[i] = idx_domain.float().mean()\n",
    "        acc_eucliDist[i] = well_classified_train[idx_domain].float().mean()\n",
    "    ax2.plot(coverage_eucliDist, acc_eucliDist, label=f'euclidian distance - top {top_k} dims')\n",
    "\n",
    "# baseline: max softmax\n",
    "domain_cutoff_baseline = np.linspace(0, 1, 1000)\n",
    "coverage_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "acc_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "for i, cut in enumerate(domain_cutoff_baseline):\n",
    "    idx_domain = msp_train > cut\n",
    "    coverage_baseline[i] = idx_domain.float().mean()\n",
    "    acc_baseline[i] = well_classified_train[idx_domain].float().mean()\n",
    "ax1.plot(coverage_baseline, acc_baseline, c='k', label='baseline (max softmax)')\n",
    "ax2.plot(coverage_baseline, acc_baseline, c='k', label='baseline (max softmax)')\n",
    "\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('coverage')\n",
    "    ax.set_ylabel('accuracy')\n",
    "    ax.set_ylim(np.nanmin(acc_baseline), np.nanmax(acc_baseline))\n",
    "    ax.set_ylim(0.99*np.nanmin(acc_baseline), 1.01*np.nanmax(acc_baseline))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(c_classif_train[well_classified_train].cpu(), msp_train[well_classified_train], c='C0', label='well-classified', alpha=0.1)\n",
    "plt.scatter(c_classif_train[well_classified_train.logical_not()].cpu(), msp_train[well_classified_train.logical_not()], c='C1', label='miscalssified', alpha=0.1)\n",
    "plt.legend()\n",
    "plt.xlabel('MSP input')\n",
    "plt.ylabel('MSP')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(euclidian_dist_train, msp_train, alpha=0.1)\n",
    "plt.xlabel('distance to center of MSP=1')\n",
    "plt.ylabel('MSP')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(cosine_sim_train, msp_train, alpha=0.1)\n",
    "plt.xlabel('cosine simi to center of MSP=1')\n",
    "plt.ylabel('MSP')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(c_classif_train.cpu(), tcp_train, alpha=0.1)\n",
    "plt.xlabel('MSP input')\n",
    "plt.ylabel('TCP')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(euclidian_dist_train, tcp_train, alpha=0.1)\n",
    "plt.xlabel('distance to center of MSP=1')\n",
    "plt.ylabel('TCP')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(cosine_sim_train, tcp_train, alpha=0.1)\n",
    "plt.xlabel('cosine simi to center of MSP=1')\n",
    "plt.ylabel('TCP')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans: clusters of well classified\n",
    "Distance to these clusters not very discriminative between well and misclassified points (still a little bit). Better with more clusters, but normal ? If all well classified are a cluster, then they are close to the closest cluster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "fig, ((ax11, ax12), (ax21, ax22)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.set_tight_layout(True)\n",
    "fig.suptitle('coverage vs. accuracy (based on cosine similarity threshold)')\n",
    "ax11.set_title('Cosine similarity - Train')\n",
    "ax21.set_title('Cosine similarity - Val')\n",
    "ax12.set_title('Euclidian distance - Train')\n",
    "ax22.set_title('Euclidian distance - Val')\n",
    "\n",
    "n_clusters = [16, 32, 64, 128, 256, 512]\n",
    "features = w_train[well_classified_train, :].cpu()\n",
    "for i, n_clust in enumerate(n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clust, random_state=0).fit(features)\n",
    "    euclidian_dist = kmeans.transform(w_train.cpu()).min(axis=1)\n",
    "    cosine_sim = cosine_similarity(w_train.cpu(), kmeans.cluster_centers_).max(axis=1)\n",
    "    \n",
    "    print(f'n_clusters: {n_clust} ; euclidian_well_mean: {euclidian_dist[well_classified_train].mean():.2f} ; euclidian_mis_mean: {euclidian_dist[well_classified_train.logical_not()].mean():.2f} ; cosine_well_mean: {cosine_sim[well_classified_train].mean():.2f} ; cosine_mis_mean: {cosine_sim[well_classified_train.logical_not()].mean():.2f}')\n",
    "    \n",
    "    # cosine similarity Train\n",
    "    domain_cutoff_cosineSim = np.linspace(0, 1, 1000)\n",
    "    coverage_cosineSim = np.zeros_like(domain_cutoff_cosineSim)\n",
    "    acc_cosineSim = np.zeros_like(domain_cutoff_cosineSim)\n",
    "    for i, cut in enumerate(domain_cutoff_cosineSim):\n",
    "        idx_domain = cosine_sim > cut\n",
    "        coverage_cosineSim[i] = idx_domain.astype(float).mean()\n",
    "        acc_cosineSim[i] = well_classified_train[idx_domain].float().mean()\n",
    "    ax11.plot(coverage_cosineSim, acc_cosineSim, label=f'{n_clust} clusters')\n",
    "    \n",
    "    # cosine similarity Val\n",
    "    cosine_sim_val = cosine_similarity(ws_val[:, 0, :].cpu(), kmeans.cluster_centers_).max(axis=1)\n",
    "    domain_cutoff_cosineSim = np.linspace(0, 1, 1000)\n",
    "    coverage_cosineSim = np.zeros_like(domain_cutoff_cosineSim)\n",
    "    acc_cosineSim = np.zeros_like(domain_cutoff_cosineSim)\n",
    "    for i, cut in enumerate(domain_cutoff_cosineSim):\n",
    "        idx_domain = cosine_sim_val > cut\n",
    "        coverage_cosineSim[i] = idx_domain.astype(float).mean()\n",
    "        acc_cosineSim[i] = well_classified_val[idx_domain].float().mean()\n",
    "    ax21.plot(coverage_cosineSim, acc_cosineSim, label=f'{n_clust} clusters')\n",
    "    \n",
    "    # euclidian distance Train\n",
    "    domain_cutoff_eucliDist = np.linspace(0, 1, 1000)\n",
    "    coverage_eucliDist = np.zeros_like(domain_cutoff_eucliDist)\n",
    "    acc_eucliDist = np.zeros_like(domain_cutoff_eucliDist)\n",
    "    for i, cut in enumerate(domain_cutoff_eucliDist):\n",
    "        idx_domain = (euclidian_dist / euclidian_dist.max()) < cut\n",
    "        coverage_eucliDist[i] = idx_domain.astype(float).mean()\n",
    "        acc_eucliDist[i] = well_classified_train[idx_domain].float().mean()\n",
    "    ax12.plot(coverage_eucliDist, acc_eucliDist, label=f'{n_clust} clusters')\n",
    "    \n",
    "    # euclidian distance Val\n",
    "    euclidian_dist_val = kmeans.transform(ws_val[:, 0, :].cpu()).min(axis=1)\n",
    "    domain_cutoff_eucliDist = np.linspace(0, 1, 1000)\n",
    "    coverage_eucliDist = np.zeros_like(domain_cutoff_eucliDist)\n",
    "    acc_eucliDist = np.zeros_like(domain_cutoff_eucliDist)\n",
    "    for i, cut in enumerate(domain_cutoff_eucliDist):\n",
    "        idx_domain = (euclidian_dist_val / euclidian_dist_val.max()) < cut\n",
    "        coverage_eucliDist[i] = idx_domain.astype(float).mean()\n",
    "        acc_eucliDist[i] = well_classified_val[idx_domain].float().mean()\n",
    "    ax22.plot(coverage_eucliDist, acc_eucliDist, label=f'{n_clust} clusters')\n",
    "\n",
    "# baseline: max softmax\n",
    "domain_cutoff_baseline = np.linspace(0, 1, 1000)\n",
    "coverage_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "acc_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "for i, cut in enumerate(domain_cutoff_baseline):\n",
    "    idx_domain = msp_train > cut\n",
    "    coverage_baseline[i] = idx_domain.float().mean()\n",
    "    acc_baseline[i] = well_classified_train[idx_domain].float().mean()\n",
    "\n",
    "for ax in [ax11, ax12, ax21, ax22]:\n",
    "    ax.plot(coverage_baseline, acc_baseline, c='k', label='baseline (max softmax)')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('coverage')\n",
    "    ax.set_ylabel('accuracy')\n",
    "    ax.set_ylim(0.99*np.nanmin(acc_baseline), 1.01*np.nanmax(acc_baseline))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain as distance from well-classified center - OT to find top directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_center_wellclassified = w_train[well_classified_train, :].mean(dim=0, keepdims=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('coverage vs. accuracy (distance from well-classified center, OT to find top dimensions)')\n",
    "ax1.set_title('cosine similarity')\n",
    "ax2.set_title('euclidian distance')\n",
    "\n",
    "for top_k in [1, 10, 20, 50, 100, w_train.shape[-1]]:\n",
    "    q_wellClassified = torch.quantile(w_train[well_classified_train, :], q=torch.tensor([x/100 for x in range(5, 100, 5)], device=device), dim=0)\n",
    "    q_misClassified = torch.quantile(w_train[well_classified_train.logical_not(), :], q=torch.tensor([x/100 for x in range(5, 100, 5)], device=device), dim=0)\n",
    "    ot_dist = ((q_wellClassified - q_misClassified)**2).sum(0)\n",
    "    top_k_dims = (-ot_dist).argsort()[:top_k]\n",
    "\n",
    "    euclidian_dist_train = torch.cdist(w_train[:, top_k_dims], w_center_wellclassified[:, top_k_dims]).squeeze().cpu()\n",
    "    cosine_sim_train = torch.nn.functional.cosine_similarity(w_train[:, top_k_dims], w_center_wellclassified[:, top_k_dims]).cpu()\n",
    "    c_classif = c_classif.squeeze().cpu()\n",
    "\n",
    "    # cosine similarity\n",
    "    domain_cutoff_cosineSim = np.linspace(0, 1, 1000)\n",
    "    coverage_cosineSim = np.zeros_like(domain_cutoff_cosineSim)\n",
    "    acc_cosineSim = np.zeros_like(domain_cutoff_cosineSim)\n",
    "    for i, cut in enumerate(domain_cutoff_cosineSim):\n",
    "        idx_domain = cosine_sim_train > cut\n",
    "        coverage_cosineSim[i] = idx_domain.float().mean()\n",
    "        acc_cosineSim[i] = well_classified_train[idx_domain].float().mean()\n",
    "    ax1.plot(coverage_cosineSim, acc_cosineSim, label=f'cosine similarity - top {top_k} dims')\n",
    "\n",
    "    # euclidian distance\n",
    "    domain_cutoff_eucliDist = np.linspace(0, 1, 1000)\n",
    "    coverage_eucliDist = np.zeros_like(domain_cutoff_eucliDist)\n",
    "    acc_eucliDist = np.zeros_like(domain_cutoff_eucliDist)\n",
    "    for i, cut in enumerate(domain_cutoff_eucliDist):\n",
    "        idx_domain = (euclidian_dist_train / euclidian_dist_train.max()) < cut\n",
    "        coverage_eucliDist[i] = idx_domain.float().mean()\n",
    "        acc_eucliDist[i] = well_classified_train[idx_domain].float().mean()\n",
    "    ax2.plot(coverage_eucliDist, acc_eucliDist, label=f'euclidian distance - top {top_k} dims')\n",
    "\n",
    "# baseline: max softmax\n",
    "domain_cutoff_baseline = np.linspace(0, 1, 1000)\n",
    "coverage_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "acc_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "for i, cut in enumerate(domain_cutoff_baseline):\n",
    "    idx_domain = msp_train > cut\n",
    "    coverage_baseline[i] = idx_domain.float().mean()\n",
    "    acc_baseline[i] = well_classified_train[idx_domain].float().mean()\n",
    "ax1.plot(coverage_baseline, acc_baseline, c='k', label='baseline (max softmax)')\n",
    "ax2.plot(coverage_baseline, acc_baseline, c='k', label='baseline (max softmax)')\n",
    "\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('coverage')\n",
    "    ax.set_ylabel('accuracy')\n",
    "    ax.set_ylim(np.nanmin(acc_baseline), np.nanmax(acc_baseline))\n",
    "    ax.set_ylim(0.99*np.nanmin(acc_baseline), 1.01*np.nanmax(acc_baseline))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "Regarder vecteur: w - w_center et sa projection sur direction well-classified - misclassified (direction moyenne ou prototypes)\n",
    "\n",
    "entrainer MLP a predict well classif or MSP or TCP from w\n",
    "\n",
    "t-SNE W well/misclassif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    stop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['classifier'])\n",
    "        \n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "        self.classifier\n",
    "        self.generator\n",
    "        self.encoder\n",
    "        \n",
    "        url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n",
    "        with dnnlib.util.open_url(url) as f:\n",
    "            self.vgg16 = torch.jit.load(f).eval().to(device)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizers = []\n",
    "        \n",
    "        optim_encoder = torch.optim.SGD(self.encoder.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "        optimizers.append(optim_encoder)\n",
    "        \n",
    "        return optimizers\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, y = batch\n",
    "        \n",
    "        optim_encoder = self.optimizers()\n",
    "        optim_encoder.zero_grad()\n",
    "        \n",
    "        # RECONSTRUCT DATA\n",
    "        \n",
    "        z_encoded = self.encoder(x)\n",
    "        \n",
    "        # class condition\n",
    "        class_condition = F.one_hot(y, n_classes)\n",
    "        # classifier condition\n",
    "        assert self.classifier_conditioning == 'MSP', 'Only MSP is implemented'\n",
    "        with torch.no_grad():\n",
    "            logits = self.classifier(x)\n",
    "        classifier_condition = get_classifier_MSP(logits)\n",
    "        conditioning = torch.cat((class_condition, classifier_condition), dim=1)\n",
    "        \n",
    "        # reconstruct\n",
    "        x_reconstructed = self.generator(z_encoded, conditioning)\n",
    "\n",
    "        # data reconstruction loss\n",
    "        loss_reconstruction = self._compute_lpips(x_reconstructed, x)\n",
    "        self.log(f'loss_LPIPS', loss_reconstruction)\n",
    "        self.manual_backward(loss_reconstruction)\n",
    "        \n",
    "        optim_encoder.step()\n",
    "        \n",
    "     \n",
    "    def _compute_lpips(self, synth_images, target_images):\n",
    "        synth_images = (synth_images + 1) * (255/2)\n",
    "        assert target_images.max() == 255\n",
    "        \n",
    "        target_features = self.vgg16(target_images, resize_images=False, return_lpips=True)\n",
    "        synth_features = self.vgg16(synth_images, resize_images=False, return_lpips=True)\n",
    "        dist = (target_features - synth_features).square().sum()\n",
    "        \n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PipelineNew(pl.LightningModule):\n",
    "    \n",
    "#     def __init__(self, latent_dim, data_dim, data_n_samples, data_noise, c_dim, hidden_dim,\n",
    "#                  classifier, class_conditioning, classifier_conditioning):\n",
    "#         super().__init__()\n",
    "#         self.save_hyperparameters(ignore=['classifier'])\n",
    "        \n",
    "#         self.automatic_optimization = False\n",
    "        \n",
    "#         if (class_conditioning in ['label', 'prediction', None] and classifier_conditioning == 'TCP'):\n",
    "#             condition_dim = c_dim + 1\n",
    "#         else:\n",
    "#             raise NotImplementedError('Only label or prediction class conditioning and TCP classifier conditioning are implemented')\n",
    "        \n",
    "#         self.class_conditioning = class_conditioning\n",
    "#         self.classifier_conditioning = classifier_conditioning\n",
    "#         self.latent_dim = latent_dim\n",
    "        \n",
    "#         self.classifier = classifier\n",
    "#         self.generator = GeneratorNew(latent_dim, condition_dim, data_dim, hidden_dim)\n",
    "#         self.discriminator = Discriminator(data_dim, condition_dim, hidden_dim)\n",
    "#         self.data_encoder = Encoder(data_dim, latent_dim)\n",
    "#         self.confidence_estimator = Encoder(data_dim, 1)\n",
    "#         self.class_embedder = lambda y: F.one_hot(y, num_classes=2)\n",
    "        \n",
    "#         self.data = MoonsDataset(n_samples=20000, noise=data_noise)        \n",
    "        \n",
    "#     def configure_optimizers(self):\n",
    "#         optim_adv_gen = torch.optim.Adam(self.generator.parameters(), lr=0.001, betas=(0, 0.99))\n",
    "#         optim_adv_disc = torch.optim.Adam(self.discriminator.parameters(), lr=0.001, betas=(0, 0.99))\n",
    "#         optimizers = [optim_adv_gen, optim_adv_disc]\n",
    "        \n",
    "#         params = list(self.data_encoder.parameters()) + list(self.confidence_estimator.parameters()) + list(self.generator.parameters())\n",
    "#         optim_reco = torch.optim.Adam(params, lr=0.001, weight_decay=1e-4)\n",
    "#         optimizers.append(optim_reco)\n",
    "        \n",
    "#         optim_confidence_estimator = torch.optim.Adam(self.confidence_estimator.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "#         optimizers.append(optim_confidence_estimator)\n",
    "        \n",
    "#         params = list(self.data_encoder.parameters()) + list(self.generator.parameters())\n",
    "#         optim_enco = torch.optim.Adam(self.data_encoder.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "#         optimizers.append(optim_enco)\n",
    "        \n",
    "#         return optimizers\n",
    "    \n",
    "    \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x_real, y_real = batch\n",
    "        \n",
    "#         optim_adv_gen, optim_adv_disc, optim_reco, optim_confidence_estimator, optim_enco = self.optimizers()\n",
    "#         optim_adv_gen.zero_grad()\n",
    "#         optim_adv_disc.zero_grad()\n",
    "#         optim_reco.zero_grad()\n",
    "#         optim_confidence_estimator.zero_grad()\n",
    "#         optim_enco.zero_grad()\n",
    "        \n",
    "#         # RECONSTRUCT REAL DATA\n",
    "        \n",
    "#         # encode data\n",
    "#         w_encoded = self.data_encoder(x_real)\n",
    "        \n",
    "#         # class condition\n",
    "#         if self.class_conditioning == 'label':\n",
    "#             class_conditioning = self.class_embedder(y_real.long())\n",
    "#         elif self.class_conditioning == 'prediction':\n",
    "#             with torch.no_grad():\n",
    "#                 logits = self.classifier(x_real)\n",
    "#             probas = torch.sigmoid(logits).squeeze()\n",
    "#             y_pred = probas.round()\n",
    "#             class_conditioning = self.class_embedder(y_pred.long())\n",
    "#         else:\n",
    "#             raise NotImplementedError('Only label or prediction class conditioning are implemented')\n",
    "        \n",
    "#         # classifier condition\n",
    "#         assert self.classifier_conditioning == 'TCP'\n",
    "#         confidence = self._get_classifier_conditioning(x_real, y_real)\n",
    "#         conditioning = torch.cat((class_conditioning, confidence), dim=1)\n",
    "        \n",
    "#         # reconstruct\n",
    "#         synthesis_input = torch.cat((w_encoded, conditioning), dim=1)\n",
    "#         x_reconstructed = self.generator.synthesis(synthesis_input)\n",
    "        \n",
    "#         # data reconstruction loss\n",
    "#         loss_reconstruction_data = F.mse_loss(x_reconstructed, x_real)\n",
    "#         self.log(f'loss_train_reconstruction_data', loss_reconstruction_data)\n",
    "#         loss_reconstruction = loss_reconstruction_data\n",
    "        \n",
    "#         # classif reconstruction loss\n",
    "#         logits_real = self.classifier(x_real)\n",
    "#         logits_rec = self.classifier(x_reconstructed)\n",
    "#         loss_reconstruction_classif = F.mse_loss(logits_rec, logits_real)\n",
    "#         self.log(f'loss_train_reconstruction_classif', loss_reconstruction_classif)\n",
    "#         loss_reconstruction += loss_reconstruction_classif\n",
    "#         self.log(f'loss_train_reconstruction_total', loss_reconstruction)\n",
    "#         self.manual_backward(loss_reconstruction, retain_graph=True)\n",
    "        \n",
    "#         # confidence estimator loss\n",
    "#         confidence_pred = torch.sigmoid(self.confidence_estimator(x_real))\n",
    "#         loss_confid = F.mse_loss(confidence_pred, confidence)\n",
    "#         self.log(f'loss_train_confid', loss_confid)\n",
    "#         self.manual_backward(loss_confid, retain_graph=True)\n",
    "        \n",
    "#         # GENERATE FAKE DATA\n",
    "        \n",
    "#         z = torch.randn(x_real.shape[0], self.latent_dim, device=self.device)\n",
    "#         conditioning_fake = conditioning # conditioning for fake data is the same as for real data for simplicity\n",
    "#         w_fake = self.generator.mapping(z)\n",
    "#         w_c_fake = torch.cat((w_fake, conditioning_fake), dim=1)\n",
    "#         x_fake = self.generator.synthesis(w_c_fake)\n",
    "        \n",
    "#         # generator adversarial loss\n",
    "#         pred_fake = self.discriminator(x_fake, conditioning_fake)\n",
    "#         loss_gen = self.adversarial_loss(pred_fake, torch.ones_like(pred_fake, device=self.device)) # fakes should be predicted as true\n",
    "#         self.log('loss_train_gen', loss_gen)\n",
    "#         self.manual_backward(loss_gen, retain_graph=True)\n",
    "\n",
    "#         # discriminator adversarial loss\n",
    "#         x_fake = x_fake.detach()\n",
    "#         pred_real = self.discriminator(x_real, conditioning)\n",
    "#         pred_fake = self.discriminator(x_fake, conditioning_fake)\n",
    "#         loss_disc = 0.5 * (self.adversarial_loss(pred_fake, torch.zeros_like(pred_fake, device=self.device)) # fakes should be predicted as false\n",
    "#                         + self.adversarial_loss(pred_real, torch.ones_like(pred_real, device=self.device))) # reals should be predicted as true\n",
    "#         self.log('loss_train_disc', loss_disc)\n",
    "#         self.manual_backward(loss_disc, retain_graph=True)\n",
    "        \n",
    "#         # encode fake data loss\n",
    "#         # w_fake = w_fake.detach()\n",
    "#         x_fake = x_fake.detach()\n",
    "#         w_fake_encoded = self.data_encoder(x_fake)\n",
    "#         loss_encoding = F.mse_loss(w_fake_encoded, w_fake)\n",
    "#         self.log(f'loss_train_encoding', loss_encoding)\n",
    "#         self.manual_backward(loss_encoding, retain_graph=True)\n",
    "                \n",
    "#         optim_reco.step()\n",
    "#         optim_confidence_estimator.step()\n",
    "#         optim_adv_gen.step()\n",
    "#         optim_adv_disc.step()\n",
    "#         optim_enco.step()\n",
    " \n",
    "#     def on_train_epoch_end(self):            \n",
    "#         tensorboard_logger = self.logger.experiment\n",
    "        \n",
    "#         # get real samples\n",
    "#         n_samples = 2000\n",
    "#         x_real = self.data[:n_samples][0]\n",
    "#         y_real = self.data[:n_samples][1]\n",
    "        \n",
    "#         # encode data\n",
    "#         with torch.no_grad():\n",
    "#             w_encoded = self.data_encoder(x_real.to(self.device))\n",
    "        \n",
    "#         # class condition\n",
    "#         if self.class_conditioning == 'label':\n",
    "#             class_conditioning = self.class_embedder(y_real.long().to(self.device))\n",
    "#         elif self.class_conditioning == 'prediction':\n",
    "#             with torch.no_grad():\n",
    "#                 logits = self.classifier(x_real.to(self.device))\n",
    "#             probas = torch.sigmoid(logits).squeeze()\n",
    "#             y_pred = probas.round()\n",
    "#             class_conditioning = self.class_embedder(y_pred.long())\n",
    "#         else:\n",
    "#             raise NotImplementedError('Only label or prediction class conditioning are implemented')\n",
    "        \n",
    "#         # classifier condition\n",
    "#         assert self.classifier_conditioning == 'TCP'\n",
    "#         with torch.no_grad():\n",
    "#             confidence_pred = torch.sigmoid(self.confidence_estimator(x_real.to(self.device)))\n",
    "#         conditioning = torch.cat((class_conditioning, confidence_pred), dim=1)\n",
    "        \n",
    "#         # reconstruct\n",
    "#         with torch.no_grad():\n",
    "#             synthesis_input = torch.cat((w_encoded, conditioning), dim=1)\n",
    "#             x_reconstructed = self.generator.synthesis(synthesis_input).cpu().numpy()\n",
    "            \n",
    "#         # adapt variable names\n",
    "#         x_fake = x_reconstructed\n",
    "#         y_fake = y_real\n",
    "#         c_fake = conditioning\n",
    "#         confidence_out = confidence_pred\n",
    "#         confidence_in = confidence_pred\n",
    "            \n",
    "#         # domain using confidence\n",
    "#         u_domain_real = confidence_pred.cpu()       \n",
    "\n",
    "#         # log loss test\n",
    "#         confidence_real = self._get_classifier_conditioning(x_real.to(self.device), y_real.to(self.device))\n",
    "#         loss_confid_real = F.mse_loss(confidence_pred, confidence_real).cpu()\n",
    "#         self.log(f'loss_test_confid_real', loss_confid_real)\n",
    "        \n",
    "#         # log figures\n",
    "#         if self.current_epoch == 0:\n",
    "#             self._log_classifier_analysis(tensorboard_logger, x_real, y_real)\n",
    "#         # if (self.current_epoch == 0) or (self.current_epoch == self.trainer.fit_loop.max_epochs // 2) or (self.current_epoch == self.trainer.fit_loop.max_epochs):\n",
    "#         #     self._log_latent_analysis(tensorboard_logger, n_samples, x_real, y_real, x_fake, class_embed, z_fake, w_fake, confidence_in, confidence_out)\n",
    "#         if (self.current_epoch == 0) or ((self.current_epoch+1)%10 == 0):\n",
    "#             self._log_gan_analysis(tensorboard_logger, x_real, y_real, x_fake, y_fake, c_fake)\n",
    "#             self._log_encoder_analysis(tensorboard_logger, x_real, y_real, x_reconstructed, confidence_real.cpu(), confidence_pred.cpu())\n",
    "#             self._log_domain_analysis(tensorboard_logger, n_samples, x_real, y_real, u_domain_real)\n",
    "                \n",
    "\n",
    "#     def _log_classifier_analysis(self, tensorboard_logger, x_real, y_real):\n",
    "        \n",
    "#         # SHOW DECISION BOUNDARY\n",
    "#         x = np.linspace(-2, 3, 100)\n",
    "#         y = np.linspace(-2, 2, 100)\n",
    "\n",
    "#         grid_data = np.zeros((len(x)*len(y), 2))\n",
    "#         i = 0\n",
    "#         for x_ in x:\n",
    "#             for y_ in y:\n",
    "#                 grid_data[i] = [x_, y_]\n",
    "#                 i += 1\n",
    "#         grid_data = torch.from_numpy(grid_data).float()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             y = self.classifier(grid_data.to(self.device))\n",
    "#         class_pred = torch.sigmoid(y).round().cpu().flatten()#.numpy()\n",
    "\n",
    "#         # SHOW CLASSIF LOSS\n",
    "#         with torch.no_grad():\n",
    "#             logits = self.classifier(x_real.to(self.device)).squeeze().cpu()\n",
    "#             classif_loss = F.binary_cross_entropy_with_logits(logits, y_real, reduction='none')\n",
    "\n",
    "#         fig, ax = plt.subplots()\n",
    "#         ax.set_title('classifier decision boundary')\n",
    "#         ax.scatter(grid_data[class_pred==0, 0], grid_data[class_pred==0, 1], alpha=1, c='C0', label='predicted class 0')\n",
    "#         ax.scatter(grid_data[class_pred!=0, 0], grid_data[class_pred!=0, 1], alpha=1, c='C1', label='predicted class 1')\n",
    "#         ax.scatter(x_real[y_real==0, 0], x_real[y_real==0, 1], alpha=0.3, c=classif_loss[y_real==0], cmap='Reds', marker='o', label='real data - class 0')\n",
    "#         im = ax.scatter(x_real[y_real==1, 0], x_real[y_real==1, 1], alpha=0.3, c=classif_loss[y_real==1], cmap='Reds', marker='+', label='real data - class 1')\n",
    "#         leg = ax.legend(frameon=True)\n",
    "#         for lh in leg.legendHandles: \n",
    "#             lh.set_alpha(1)\n",
    "#         cbar = fig.colorbar(im, ax=ax, label='classifier loss')\n",
    "#         cbar.solids.set(alpha=1)\n",
    "#         tensorboard_logger.add_figure(\"classifier_analysis\", fig, self.current_epoch)\n",
    "        \n",
    "        \n",
    "#     def _log_gan_analysis(self, tensorboard_logger, x_real, y_real, x_fake, y_fake, c):\n",
    "        \n",
    "#         # generated data\n",
    "#         plt.figure()\n",
    "#         plt.scatter(x_real[:, 0], x_real[:, 1], alpha=0.5, c=['C0' if y == 0 else 'C1' for y in y_real])\n",
    "#         color = ['C2' if y == 0 else 'C3' for y in y_fake] if self.class_conditioning is not None else 'k'\n",
    "#         plt.scatter(x_fake[:, 0], x_fake[:, 1], alpha=0.5, c=color)\n",
    "#         tensorboard_logger.add_figure(\"generated_data\", plt.gcf(), self.current_epoch)\n",
    "        \n",
    "\n",
    "#     def _log_latent_analysis(self, tensorboard_logger, n_samples, x_real, y_real, x_fake, class_embed, z, w, confidence_in, confidence_out):\n",
    "        \n",
    "#         # analysis of W\n",
    "#         import time\n",
    "#         start = time.time()\n",
    "#         w_pca = PCA(n_components=2).fit_transform(w.cpu().numpy())\n",
    "#         w_tsne = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(w.cpu().numpy())\n",
    "#         # w_umap = UMAP().fit_transform(w.cpu().numpy())\n",
    "#         fig, axs = plt.subplots(3, 2, figsize=(12, 9), constrained_layout=True)\n",
    "#         for algo, ax_, w_embedded in zip(['PCA', 't-SNE'], axs, [w_pca, w_tsne]):\n",
    "#             if self.class_conditioning is not None:\n",
    "#                 ax_[0].set_title(f'{algo} with input class as color')\n",
    "#                 ax_[0].scatter(w_embedded[y_real==0, 0], w_embedded[y_real==0, 1], c='C0', alpha=0.5, label='class 0')\n",
    "#                 ax_[0].scatter(w_embedded[y_real==1, 0], w_embedded[y_real==1, 1], c='C1', alpha=0.5, label='class 1')\n",
    "#                 ax_[0].legend()\n",
    "#             else:\n",
    "#                 ax_[0].set_title(f'{algo} with input class as color')\n",
    "#                 ax_[0].scatter(w_embedded[:, 0], w_embedded[:, 1], c='k', alpha=0.5, label='no input class given')\n",
    "#                 ax_[0].legend()\n",
    "#             if self.classifier_conditioning is not None:\n",
    "#                 ax_[1].set_title(f'{algo} with Confidence as color')\n",
    "#                 scat = ax_[1].scatter(w_embedded[:, 0], w_embedded[:, 1], c=confidence_in.cpu().numpy(), cmap='viridis', alpha=0.5)\n",
    "#                 cb = fig.colorbar(scat, ax=ax_[1])\n",
    "#                 cb.set_alpha(1)\n",
    "#                 cb.draw_all()\n",
    "#         tensorboard_logger.add_figure('W', plt.gcf(), self.current_epoch)    \n",
    "                \n",
    "#         if self.classifier_conditioning is not None:\n",
    "#             # confidence\n",
    "#             fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6), constrained_layout=True)\n",
    "#             ax1.set_title('Confidence computed from real data')\n",
    "#             ax2.set_title('generated data conditioned by Confidence value')\n",
    "#             scat1 = ax1.scatter(x_real[:, 0].cpu().numpy(), x_real[:, 1].cpu().numpy(), c=confidence_out.cpu().numpy(), cmap='viridis', alpha=0.5)\n",
    "#             ax2.scatter(x_fake[:, 0], x_fake[:, 1], c=confidence_in.cpu().numpy(), cmap='viridis', alpha=0.5)\n",
    "#             cb = fig.colorbar(scat1, ax=ax1)\n",
    "#             cb.set_alpha(1)\n",
    "#             cb.draw_all()\n",
    "#             tensorboard_logger.add_figure(\"Confidence\", fig, self.current_epoch)\n",
    "\n",
    "#             # distances in W\n",
    "#             confidence_one = torch.ones(size=(n_samples, 1)).to(self.device)\n",
    "#             mapping_in = torch.cat((z, confidence_one), dim=1) if self.class_conditioning is None else torch.cat((z, class_embed, confidence_one), dim=1)\n",
    "#             with torch.no_grad():\n",
    "#                 w = self.generator.mapping(mapping_in)\n",
    "#                 w_center_confidence_one = w.mean(0, keepdim=True)\n",
    "#             confidence_rnd = torch.rand(size=(n_samples, 1)).to(self.device)\n",
    "#             mapping_in = torch.cat((z, confidence_rnd), dim=1) if self.class_conditioning is None else torch.cat((z, class_embed, confidence_rnd), dim=1)\n",
    "#             with torch.no_grad():\n",
    "#                 w = self.generator.mapping(mapping_in)\n",
    "#             distances = torch.cdist(w, w_center_confidence_one).cpu().numpy()\n",
    "#             plt.figure()\n",
    "#             plt.title('distances from W center of confidence=1')\n",
    "#             plt.scatter(distances, confidence_rnd.cpu().numpy(), alpha=0.5)\n",
    "#             plt.xlabel('distance from W center of confidence=1')\n",
    "#             plt.ylabel('confidence value')\n",
    "#             tensorboard_logger.add_figure('distances from W center of confidence=1', plt.gcf(), self.current_epoch)\n",
    "            \n",
    "            \n",
    "#     def _log_encoder_analysis(self, tensorboard_logger, x_real, y_real, x_reconstructed, confidence_real, confidence_pred):\n",
    "        \n",
    "#         fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "#         bins = np.histogram(torch.cat((confidence_real.squeeze(), confidence_pred.squeeze())), bins=50)[1]\n",
    "#         ax1.set_title('histogram in u_domain')\n",
    "#         ax1.hist(confidence_real.squeeze(), bins=bins, alpha=0.5, label='ground truth')\n",
    "#         ax1.hist(confidence_pred.squeeze(), bins=bins, alpha=0.5, label='prediction')\n",
    "#         ax1.legend()\n",
    "#         ax2.set_title('reconstructed data')\n",
    "#         ax2.scatter(x_real[y_real==0, 0], x_real[y_real==0, 1], alpha=0.5, c='C0', label='real data - class 0')\n",
    "#         ax2.scatter(x_real[y_real==1, 0], x_real[y_real==1, 1], alpha=0.5, c='C1', label='real data - class 1')\n",
    "#         ax2.scatter(x_reconstructed[y_real==0, 0], x_reconstructed[y_real==0, 1], alpha=0.5, c='C2', label='reconstructed data - class 0')\n",
    "#         ax2.scatter(x_reconstructed[y_real==1, 0], x_reconstructed[y_real==1, 1], alpha=0.5, c='C3', label='reconstructed data - class 1')\n",
    "#         ax2.legend()\n",
    "#         tensorboard_logger.add_figure('reconstructed_data', fig, self.current_epoch)\n",
    "    \n",
    "    \n",
    "#     def _log_domain_analysis(self, tensorboard_logger, n_samples, x_real, y_real, u_domain_real):\n",
    "        \n",
    "#         # Risk coverage curves for REAL test data\n",
    "#         with torch.no_grad(): \n",
    "#             logits = self.classifier(x_real.to(self.device)).cpu()\n",
    "#         probas = torch.sigmoid(logits).squeeze()\n",
    "#         classif_loss = F.binary_cross_entropy_with_logits(logits.squeeze(), y_real.float(), reduction='none')\n",
    "#         classif_correct = (probas.round() == y_real)  \n",
    "#         tcp = torch.zeros_like(probas)\n",
    "#         tcp[y_real==0] = 1 - probas[y_real==0]\n",
    "#         tcp[y_real==1] = probas[y_real==1]\n",
    "\n",
    "#         # baseline: random selection\n",
    "#         domain_cutoff_random = np.linspace(0, 1, 100)\n",
    "#         coverage_random = np.zeros_like(domain_cutoff_random)\n",
    "#         risk_random = np.zeros_like(domain_cutoff_random)\n",
    "#         acc_random = np.zeros_like(domain_cutoff_random)\n",
    "#         for i, cut in enumerate(domain_cutoff_random):\n",
    "#             nb_samples = int((1-cut) * n_samples) # 1-cut to be coherent with other indices below (low value -> high coverage)\n",
    "#             idx_domain = rng.choice(np.arange(n_samples), size=nb_samples, replace=False)\n",
    "#             coverage_random[i] = x_real[idx_domain].shape[0] / n_samples\n",
    "#             risk_random[i] = classif_loss[idx_domain].mean()\n",
    "#             acc_random[i] = classif_correct[idx_domain].float().mean()\n",
    "#             acc_random[i] = classif_correct[idx_domain].float().mean()\n",
    "            \n",
    "#         # baseline: max softmax\n",
    "#         domain_cutoff_baseline = np.linspace(0, 1, 1000)\n",
    "#         coverage_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "#         risk_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "#         acc_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "#         for i, cut in enumerate(domain_cutoff_baseline):\n",
    "#             idx_domain = (probas > cut) | (1-probas > cut)\n",
    "#             coverage_baseline[i] = idx_domain.float().mean()\n",
    "#             risk_baseline[i] = classif_loss[idx_domain].mean()\n",
    "#             acc_baseline[i] = classif_correct[idx_domain].float().mean()\n",
    "            \n",
    "#         # baseline: TCP\n",
    "#         domain_cutoff_baselineTCP = np.linspace(0, 1, 1000)\n",
    "#         coverage_baselineTCP = np.zeros_like(domain_cutoff_baselineTCP)\n",
    "#         risk_baselineTCP = np.zeros_like(domain_cutoff_baselineTCP)\n",
    "#         acc_baselineTCP = np.zeros_like(domain_cutoff_baselineTCP)\n",
    "#         for i, cut in enumerate(domain_cutoff_baselineTCP):\n",
    "#             idx_domain = tcp > cut\n",
    "#             coverage_baselineTCP[i] = idx_domain.float().mean()\n",
    "#             risk_baselineTCP[i] = classif_loss[idx_domain].mean()\n",
    "#             acc_baselineTCP[i] = classif_correct[idx_domain].float().mean()\n",
    "            \n",
    "#         # cut max proba computed in U\n",
    "#         domain_cutoff = np.linspace(0, 1, 1000)\n",
    "#         coverage = np.zeros_like(domain_cutoff)\n",
    "#         risk = np.zeros_like(domain_cutoff)\n",
    "#         acc = np.zeros_like(domain_cutoff)\n",
    "#         for i, cut in enumerate(domain_cutoff):\n",
    "#             idx_domain = self.selection_function(u_domain_real, cut)\n",
    "#             coverage[i] = idx_domain.float().mean()\n",
    "#             risk[i] = classif_loss[idx_domain].mean()\n",
    "#             acc[i] = classif_correct[idx_domain].float().mean()\n",
    "\n",
    "#         fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "#         ax1.set_title('coverage vs. risk\\n(obtained by varying confidence/uncertainty threshold)')\n",
    "#         ax1.plot(coverage, risk, label='cut in U')\n",
    "#         ax1.plot(coverage_baseline, risk_baseline, label='baseline (max softmax)')\n",
    "#         ax1.plot(coverage_random, risk_random, label='baseline (random)')\n",
    "#         ax1.plot(coverage_baselineTCP, risk_baselineTCP, label='baseline (TCP oracle)')\n",
    "#         ax1.legend()\n",
    "#         ax1.set_xlabel('coverage')\n",
    "#         ax1.set_ylabel('risk')\n",
    "\n",
    "#         ax2.set_title('coverage vs. accuracy\\n(obtained by varying confidence/uncertainty threshold)\\n(using pseudo-labels)')\n",
    "#         ax2.plot(coverage, acc, label='cut in U')\n",
    "#         ax2.plot(coverage_baseline, acc_baseline, label='baseline (max softmax)')\n",
    "#         ax2.plot(coverage_random, acc_random, label='baseline (random)')\n",
    "#         ax2.plot(coverage_baselineTCP, acc_baselineTCP, label='baseline (TCP oracle)')\n",
    "#         ax2.legend()\n",
    "#         ax2.set_xlabel('coverage')\n",
    "#         ax2.set_ylabel('accuracy')\n",
    "\n",
    "#         ax3.set_title('coverage vs threshold value')\n",
    "#         ax3.plot((domain_cutoff-domain_cutoff.min())/(domain_cutoff.max()-domain_cutoff.min()), coverage, label='cut in U')\n",
    "#         ax3.plot((domain_cutoff_baseline-domain_cutoff_baseline.min())/(domain_cutoff_baseline.max()-domain_cutoff_baseline.min()), coverage_baseline, label='baseline (max softmax)')\n",
    "#         ax3.plot((domain_cutoff_random-domain_cutoff_random.min())/(domain_cutoff_random.max()-domain_cutoff_random.min()), coverage_random, label='baseline (random)')\n",
    "#         ax3.plot((domain_cutoff_baselineTCP-domain_cutoff_baselineTCP.min())/(domain_cutoff_baselineTCP.max()-domain_cutoff_baselineTCP.min()), coverage_baselineTCP, label='baseline (TCP oracle)')\n",
    "#         ax3.legend()\n",
    "#         ax3.set_xlabel('normalized threshold value')\n",
    "#         ax3.set_ylabel('coverage')\n",
    "\n",
    "#         ax4.set_title('risk vs threshold value')\n",
    "#         ax4.plot((domain_cutoff-domain_cutoff.min())/(domain_cutoff.max()-domain_cutoff.min()), risk, label='cut in U')\n",
    "#         ax4.plot((domain_cutoff_baseline-domain_cutoff_baseline.min())/(domain_cutoff_baseline.max()-domain_cutoff_baseline.min()), risk_baseline, label='baseline (max softmax)')\n",
    "#         ax4.plot((domain_cutoff_random-domain_cutoff_random.min())/(domain_cutoff_random.max()-domain_cutoff_random.min()), risk_random, label='baseline (random)')\n",
    "#         ax4.plot((domain_cutoff_baselineTCP-domain_cutoff_baselineTCP.min())/(domain_cutoff_baselineTCP.max()-domain_cutoff_baselineTCP.min()), risk_baselineTCP, label='baseline (TCP oracle)')\n",
    "#         ax4.legend()\n",
    "#         ax4.set_xlabel('normalized threshold value')\n",
    "#         ax4.set_ylabel('risk')\n",
    "#         tensorboard_logger.add_figure(\"domain_real_data\", fig, self.current_epoch)\n",
    "        \n",
    "#         # Illustrate domain\n",
    "#         x_real = x_real.cpu().numpy()\n",
    "#         fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "#         cut = 0.8\n",
    "#         idx_in_domain = self.selection_function(u_domain_real, cut).cpu()\n",
    "#         idx_out_domain = idx_in_domain.logical_not()\n",
    "#         coverage = idx_in_domain.float().mean()\n",
    "#         risk = classif_loss[idx_in_domain].mean()\n",
    "#         acc = classif_correct[idx_in_domain].float().mean()\n",
    "#         ax1.set_title(f'threshold = {cut}\\ncoverage={coverage:.2f}, risk={risk:.2f}, acc={acc:.2f}')\n",
    "#         ax1.scatter(x_real[idx_out_domain, 0], x_real[idx_out_domain, 1], alpha=0.1, label='out domain', c='r')\n",
    "#         ax1.scatter(x_real[idx_in_domain, 0], x_real[idx_in_domain, 1], alpha=0.1, label='in domain', c='g')\n",
    "#         ax1.legend()\n",
    "\n",
    "#         cut = 0.9\n",
    "#         idx_in_domain = self.selection_function(u_domain_real, cut).cpu()\n",
    "#         idx_out_domain = idx_in_domain.logical_not()\n",
    "#         coverage = idx_in_domain.float().mean()\n",
    "#         risk = classif_loss[idx_in_domain].mean()\n",
    "#         acc = classif_correct[idx_in_domain].float().mean()\n",
    "#         ax2.set_title(f'threshold = {cut}\\ncoverage={coverage:.2f}, risk={risk:.2f}, acc={acc:.2f}')\n",
    "#         ax2.scatter(x_real[idx_out_domain, 0], x_real[idx_out_domain, 1], alpha=0.1, label='out domain', c='r')\n",
    "#         ax2.scatter(x_real[idx_in_domain, 0], x_real[idx_in_domain, 1], alpha=0.1, label='in domain', c='g')\n",
    "#         ax2.legend()\n",
    "\n",
    "#         cut = 0.95\n",
    "#         idx_in_domain = self.selection_function(u_domain_real, cut).cpu()\n",
    "#         idx_out_domain = idx_in_domain.logical_not()\n",
    "#         coverage = idx_in_domain.float().mean()\n",
    "#         risk = classif_loss[idx_in_domain].mean()\n",
    "#         acc = classif_correct[idx_in_domain].float().mean()\n",
    "#         ax3.set_title(f'threshold = {cut}\\ncoverage={coverage:.2f}, risk={risk:.2f}, acc={acc:.2f}')\n",
    "#         ax3.scatter(x_real[idx_out_domain, 0], x_real[idx_out_domain, 1], alpha=0.1, label='out domain', c='r')\n",
    "#         ax3.scatter(x_real[idx_in_domain, 0], x_real[idx_in_domain, 1], alpha=0.1, label='in domain', c='g')\n",
    "#         ax3.legend()\n",
    "#         tensorboard_logger.add_figure(\"domain_real_data_illustration\", fig, self.current_epoch)\n",
    "        \n",
    "#     def selection_function(self, x, cut):\n",
    "#         in_domain = x.squeeze() > cut\n",
    "#         return in_domain\n",
    "    \n",
    "#     def _get_classifier_conditioning(self, x, y=None):\n",
    "#         with torch.no_grad():\n",
    "#             logits = self.classifier(x)\n",
    "#         probas_class1 = torch.sigmoid(logits)\n",
    "#         probas_class0 = 1 - probas_class1\n",
    "#         if self.classifier_conditioning == 'TCP':\n",
    "#             y_as_idx = F.one_hot(y.long(), num_classes=2).bool()\n",
    "#             probas = torch.cat((probas_class0, probas_class1), dim=1)\n",
    "#             confidence = probas[y_as_idx].unsqueeze(1)\n",
    "#         elif self.classifier_conditioning == 'MSP':\n",
    "#             confidence = torch.maximum(probas_class0, probas_class1)\n",
    "#         else:\n",
    "#             raise ValueError('Unknown confidence metric')\n",
    "#         return confidence\n",
    "    \n",
    "#     def adversarial_loss(self, y_hat, y):\n",
    "#         return F.binary_cross_entropy_with_logits(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @click.command()\n",
    "# @click.option('--n_samples', default=20000, help='Number of samples to generate in visualization')\n",
    "# @click.option('--noise', default=0.3, help='Noise to add to the data')\n",
    "# @click.option('--hidden_dim', default=64, help='Hidden dimension of the GAN')\n",
    "# @click.option('--latent_dim', default=8, help='Latent dimension of the GAN')\n",
    "# @click.option('--class_conditioning', default='label', help='class conditioning: \"label\" or \"prediction\" or None (default)')\n",
    "# @click.option('--classifier_conditioning', default='TCP', help='classifier conditioning: \"TCP\" or \"MSP\" or None (default)')\n",
    "# def main(**kwargs):\n",
    "    \n",
    "#     config = EasyDict(kwargs)\n",
    "#     config.c_dim = 2\n",
    "    \n",
    "#     dm = MoonsDataModule(n_samples=config.n_samples, noise=config.noise)\n",
    "    \n",
    "#     if config.noise == 0.1:\n",
    "#         path_classifier = path_results / 'classifier' / '2022-11-29_152904_linear_noise0.1' / 'checkpoints' / 'epoch=99-step=6300.ckpt'\n",
    "#     elif config.noise == 0.3:\n",
    "#         path_classifier = path_results / 'classifier' / '2023-02-03_110311_linear_noise0.3' / 'checkpoints' / 'epoch=99-step=6300.ckpt'\n",
    "#     else:\n",
    "#         raise ValueError('Noise value not supported')\n",
    "#     config.classifier = str(path_classifier)\n",
    "#     classifier = LinearClassifier.load_from_checkpoint(str(path_classifier))\n",
    "    \n",
    "#     model = PipelineNew(config.latent_dim, 2, config.n_samples, config.noise, c_dim=config.c_dim, hidden_dim=config.hidden_dim,\n",
    "#                      classifier=classifier, class_conditioning=config.class_conditioning, classifier_conditioning=config.classifier_conditioning)\n",
    "        \n",
    "#     # create experiment folder to save results and logs\n",
    "#     timestamp = time.strftime('%Y-%m-%d_%H%M%S', time.localtime())\n",
    "#     tag = f'_noise{config.noise}_classCond{config.class_conditioning}_classifCond{config.classifier_conditioning}'\n",
    "#     path_results_exp = path_results / 'pipelineNew' / (timestamp+tag)\n",
    "#     if not path_results_exp.exists(): path_results_exp.mkdir(parents=True)\n",
    "#     logger = TensorBoardLogger(save_dir=path_results_exp, name='', version='')\n",
    "#     with open(os.path.join(path_results_exp, 'training_options.json'), 'wt') as f:\n",
    "#         json.dump(config, f, indent=2)\n",
    "\n",
    "#     # train\n",
    "#     trainer = pl.Trainer(accelerator=\"auto\", devices=1, max_epochs=100, logger=logger, auto_select_gpus=True)\n",
    "#     trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
