{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./stylegan/stylegan2')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms as T\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "\n",
    "from cifar10_models.vgg import vgg11_bn\n",
    "from cifar10_models.mobilenetv2 import mobilenet_v2\n",
    "from cifar10_models.resnet import resnet18, resnet50\n",
    "from temperature_scaling import ModelWithTemperature, _ECELoss\n",
    "from general_calibration_error import gce\n",
    "from dirichletcal.calib.vectorscaling import VectorScaling\n",
    "\n",
    "for p in [\n",
    "    Path('/d/alecoz/projects'), # DeepLab\n",
    "    Path(os.path.expandvars('$WORK')), # Jean Zay\n",
    "    Path('w:/')]: # local\n",
    "    if os.path.exists(p):\n",
    "        path_main = p\n",
    "# path_results = path_main / 'uncertainty-conditioned-gan/results'\n",
    "for p in [\n",
    "    Path('/scratchf/CIFAR'), # DeepLab\n",
    "    Path(os.path.expandvars('$DSDIR'))]: # Jean Zay\n",
    "    if os.path.exists(p):\n",
    "        path_dataset = p\n",
    "path_models = Path.cwd().parent / 'models' / 'CIFAR10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_synthetic_images(images):\n",
    "    assert images.dim() == 4, \"Expected 4D (B x C x H x W) image tensor, got {}D\".format(images.dim())\n",
    "    images = ((images + 1) / 2).clamp(0, 1) # scale\n",
    "    return images\n",
    "\n",
    "def preprocess_images_classifier(images):\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2471, 0.2435, 0.2616)\n",
    "    images = T.Normalize(mean, std)(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "batch_size = 512\n",
    "\n",
    "idx_to_label = {\n",
    "    0: 'airplane',\n",
    "    1: 'car',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer', \n",
    "    5: 'dog', \n",
    "    6: 'frog', \n",
    "    7: 'horse', \n",
    "    8: 'ship',\n",
    "    9: 'truck'}\n",
    "\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2471, 0.2435, 0.2616)\n",
    "transforms = T.Compose(\n",
    "    [T.ToTensor(),\n",
    "    T.Normalize(mean, std)])\n",
    "dataset_train = CIFAR10(root=path_dataset, train=True, transform=transforms)\n",
    "dataset_val = CIFAR10(root=path_dataset, train=False, transform=transforms)\n",
    "dataset_calib, dataset_test = torch.utils.data.random_split(dataset_val, [5000, 5000], generator=torch.Generator().manual_seed(123))\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "dataloader_calib = DataLoader(dataset_calib, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    \n",
    "# GENERATOR\n",
    "with open(path_models/'cifar10.pkl', 'rb') as f:\n",
    "    G = pickle.load(f)['G_ema'].cuda() # torch.nn.Module\n",
    "\n",
    "# CLASSIFIER\n",
    "classifier = resnet50(pretrained=True).eval().requires_grad_(False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generator\n",
    "z = torch.randn([1, G.z_dim]).cuda() # latent codes\n",
    "label_gen = 4\n",
    "c = torch.nn.functional.one_hot(torch.tensor([label_gen]).cuda(), num_classes=G.c_dim) # class labels\n",
    "img = G(z, c, truncation_psi=0.7) # NCHW, float32, dynamic range [-1, +1]\n",
    "img = postprocess_synthetic_images(img)\n",
    "\n",
    "# test classifier\n",
    "logits = classifier(preprocess_images_classifier(img))\n",
    "probas = torch.softmax(logits, dim=1)\n",
    "proba, label_pred = torch.max(probas, 1)\n",
    "\n",
    "# plot\n",
    "plt.figure()\n",
    "plt.imshow(img[0].permute(1,2,0).cpu())\n",
    "plt.title(f'generated {idx_to_label[label_gen]}, predicted {idx_to_label[label_pred.item()]} {100*proba.item():.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_MSP_correct(dataloader, classifier):\n",
    "\n",
    "#     classifier.eval()\n",
    "#     msp = torch.zeros((len(dataloader.dataset)))\n",
    "#     correct = torch.zeros((len(dataloader.dataset)))\n",
    "#     idx = 0\n",
    "#     for X, y in dataloader:\n",
    "#         batch_size = X.shape[0]\n",
    "#         X, y = X.cuda(), y.cuda()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             logits = classifier(X)\n",
    "#             probas, class_pred = torch.max(torch.softmax(logits, axis=1), axis=1)\n",
    "#         msp[idx:idx+batch_size] = probas\n",
    "#         correct[idx:idx+batch_size] = class_pred == y.squeeze()\n",
    "#         idx += batch_size\n",
    "\n",
    "#     return msp, correct\n",
    "\n",
    "\n",
    "# msp_train, correct_train = get_MSP_correct(dataloader_train, classifier)\n",
    "# msp_calib, correct_calib = get_MSP_correct(dataloader_calib, classifier)\n",
    "# msp_test, correct_test = get_MSP_correct(dataloader_test, classifier)\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# ax1.set_xlabel('MSP value')\n",
    "# ax1.hist(msp_test, alpha=0.5, bins=50, log=True, density=True, label='test')\n",
    "# ax1.hist(msp_train, alpha=0.5, bins=50, log=True, density=True, label='train')\n",
    "# ax1.legend()\n",
    "# ax2.set_xlabel('MSP value')\n",
    "# ax2.hist(msp_test, alpha=0.5, bins=50, log=True, density=True, label='test')\n",
    "# ax2.hist(msp_calib, alpha=0.5, bins=50, log=True, density=True, label='calib')\n",
    "# ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'train accuracy: {correct_train.sum() / len(correct_train):.3f}')\n",
    "# print(f'calib accuracy: {correct_calib.sum() / len(correct_calib):.3f}')\n",
    "# print(f'test accuracy: {correct_test.sum() / len(correct_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_from_dataloader(model, dataloader, vector_scale=None):\n",
    "    # First: collect all the logits and labels for the validation set\n",
    "    logits_list = []\n",
    "    labels_list = []\n",
    "    for input, label in dataloader:\n",
    "        input = input.cuda()\n",
    "        with torch.no_grad():\n",
    "            logits = model(input)\n",
    "        logits_list.append(logits)\n",
    "        labels_list.append(label)\n",
    "    logits = torch.cat(logits_list).cpu()\n",
    "    if vector_scale is not None:\n",
    "        probs = torch.from_numpy(vector_scale.predict_proba(logits).copy())\n",
    "    else:\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "    labels = torch.cat(labels_list).cpu()\n",
    "    # Second: compute the metrics\n",
    "    ece = gce(labels, probs, binning_scheme='even', class_conditional=False, max_prob=True, norm='l1', num_bins=15)\n",
    "    sce = gce(labels, probs, binning_scheme='even', class_conditional=False, max_prob=False, norm='l1', num_bins=15)\n",
    "    rmsce = gce(labels, probs, binning_scheme='adaptive', class_conditional=False, max_prob=True, norm='l2', datapoints_per_bin=100)\n",
    "    ace = gce(labels, probs, binning_scheme='adaptive', class_conditional=True, max_prob=False, norm='l1')\n",
    "    tace = gce(labels, probs, binning_scheme='adaptive', class_conditional=True, max_prob=False, norm='l1', threshold=0.01)\n",
    "    \n",
    "    metrics = {'ece': ece, 'sce': sce, 'rmsce': rmsce, 'ace': ace, 'tace': tace}\n",
    "    return metrics\n",
    "\n",
    "def ece_from_dataloader(model, dataloader, unnormalize=False):\n",
    "    # First: collect all the logits and labels for the validation set\n",
    "    logits_list = []\n",
    "    labels_list = []\n",
    "    for input, label in dataloader:\n",
    "        input = input.cuda()\n",
    "        with torch.no_grad():\n",
    "            logits = model(input)\n",
    "        logits_list.append(logits)\n",
    "        labels_list.append(label)\n",
    "    logits = torch.cat(logits_list).cuda()\n",
    "    labels = torch.cat(labels_list).cuda()\n",
    "    # Second: compute the ECE\n",
    "    ece = _ECELoss(unnormalize=unnormalize)(logits, labels)\n",
    "    ece = ece.item()\n",
    "    \n",
    "    return ece\n",
    "\n",
    "def hist_from_dataloader(model, dataloader, vector_scale=None):\n",
    "    n_bins = 15\n",
    "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "        \n",
    "    logits_list = []\n",
    "    labels_list = []\n",
    "    for input, label in dataloader:\n",
    "        input = input.cuda()\n",
    "        with torch.no_grad():\n",
    "            logits = model(input)\n",
    "        logits_list.append(logits)\n",
    "        labels_list.append(label)\n",
    "    logits = torch.cat(logits_list).cuda()\n",
    "    labels = torch.cat(labels_list).cuda()\n",
    "    \n",
    "    if vector_scale is not None:\n",
    "        softmaxes = torch.from_numpy(vector_scale.predict_proba(logits.cpu())).to(logits.device)\n",
    "    else:\n",
    "        softmaxes = F.softmax(logits, dim=1)\n",
    "    confidences, predictions = torch.max(softmaxes, 1)\n",
    "    if labels.dim() > 1:\n",
    "        if labels.shape[1] > 1: # one-hot embedding\n",
    "            labels = labels.argmax(1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "\n",
    "    accuracies_in_bin = []\n",
    "    avg_confidences_in_bin = []\n",
    "    data_in_bin = []\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "        prop_in_bin = in_bin.float().mean()\n",
    "        if prop_in_bin.item() > 0:\n",
    "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "        else:\n",
    "            accuracy_in_bin = float('nan')\n",
    "            avg_confidence_in_bin = float('nan')\n",
    "        accuracies_in_bin.append(accuracy_in_bin)\n",
    "        avg_confidences_in_bin.append(avg_confidence_in_bin)\n",
    "        data_in_bin.append(100*in_bin.cpu().sum()/len(in_bin)) # in %\n",
    "            \n",
    "    avg_confidences_in_bin = torch.as_tensor(avg_confidences_in_bin)\n",
    "    accuracies_in_bin = torch.as_tensor(accuracies_in_bin)\n",
    "    data_in_bin = torch.as_tensor(data_in_bin)\n",
    "            \n",
    "    return avg_confidences_in_bin, accuracies_in_bin, data_in_bin, bin_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From real validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_calib_before_TS = metrics_from_dataloader(classifier, dataloader_calib)\n",
    "metrics_test_before_TS = metrics_from_dataloader(classifier, dataloader_test)\n",
    "confid_calib_before_TS, acc_calib_before_TS, in_bin_calib, bin_boundaries = hist_from_dataloader(classifier, dataloader_calib)\n",
    "confid_test_before_TS, acc_test_before_TS, in_bin_test, bin_boundaries = hist_from_dataloader(classifier, dataloader_test)\n",
    "\n",
    "# Performing temperature scaling\n",
    "model = ModelWithTemperature(classifier).cuda()\n",
    "model.set_temperature(dataloader_calib)\n",
    "\n",
    "metrics_calib_after_TS = metrics_from_dataloader(model, dataloader_calib)\n",
    "metrics_test_after_TS = metrics_from_dataloader(model, dataloader_test)\n",
    "confid_calib_after_TS, acc_calib_after_TS, _, bin_boundaries = hist_from_dataloader(model, dataloader_calib)\n",
    "confid_test_after_TS, acc_test_after_TS, _, bin_boundaries = hist_from_dataloader(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "# histo of data\n",
    "axs[0, 0].stairs(in_bin_calib, bin_boundaries, fill=True)\n",
    "axs[0, 0].set_title('on calib data')\n",
    "axs[0, 1].stairs(in_bin_test, bin_boundaries, fill=True)\n",
    "axs[0, 1].set_title('on test data')\n",
    "for ax in [axs[0, 0], axs[0, 1]]:\n",
    "    ax.set_xlabel('confidence')\n",
    "    ax.set_ylabel('% of samples')\n",
    "\n",
    "# reliability diagrams\n",
    "for ax in axs.flatten()[2:]:\n",
    "    ax.set_xlabel('confidence')\n",
    "    ax.set_ylabel('accuracy')\n",
    "\n",
    "list_names = ['on calib data before TS', 'on test data before TS', 'on calib data after TS', 'on test data after TS']\n",
    "list_avg_confid = [confid_calib_before_TS, confid_test_before_TS, confid_calib_after_TS, confid_test_after_TS]\n",
    "list_acc = [acc_calib_before_TS, acc_test_before_TS, acc_calib_after_TS, acc_test_after_TS]\n",
    "list_metrics = [metrics_calib_before_TS, metrics_test_before_TS, metrics_calib_after_TS, metrics_test_after_TS]\n",
    "for ax, name, avg_confid, acc, metrics in zip(axs.flatten()[2:], list_names, list_avg_confid, list_acc, list_metrics):\n",
    "    ax.stairs(avg_confid, bin_boundaries, fill=True, alpha=0.8, label='perfect')\n",
    "    ax.stairs(acc, bin_boundaries, fill=True, alpha=0.8, label='real')\n",
    "    ax.legend(loc='lower right')\n",
    "    if 'after TS' in name: ax.text(0, 0.1, f'Temp: {model.temperature.item():.3f}', fontsize='large')\n",
    "    ax.text(0, 0.8, f'ECE: {100*metrics[\"ece\"]:.3f}%', fontsize='large')\n",
    "    ax.text(0, 0.7, f'SCE: {100*metrics[\"sce\"]:.3f}%', fontsize='large')\n",
    "    ax.text(0, 0.6, f'RMSCE: {100*metrics[\"rmsce\"]:.3f}%', fontsize='large')\n",
    "    ax.text(0, 0.5, f'ACE: {100*metrics[\"ace\"]:.3f}%', fontsize='large')\n",
    "    ax.set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From real train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_calib_before_TS = metrics_from_dataloader(classifier, dataloader_train)\n",
    "# metrics_test_before_TS = metrics_from_dataloader(classifier, dataloader_test)\n",
    "# confid_calib_before_TS, acc_calib_before_TS, in_bin_calib, bin_boundaries = hist_from_dataloader(classifier, dataloader_train)\n",
    "# confid_test_before_TS, acc_test_before_TS, in_bin_test, bin_boundaries = hist_from_dataloader(classifier, dataloader_test)\n",
    "\n",
    "# # Performing temperature scaling\n",
    "# model = ModelWithTemperature(classifier).cuda()\n",
    "# model.set_temperature(dataloader_train)\n",
    "\n",
    "# metrics_calib_after_TS = metrics_from_dataloader(model, dataloader_train)\n",
    "# metrics_test_after_TS = metrics_from_dataloader(model, dataloader_test)\n",
    "# confid_calib_after_TS, acc_calib_after_TS, _, bin_boundaries = hist_from_dataloader(model, dataloader_train)\n",
    "# confid_test_after_TS, acc_test_after_TS, _, bin_boundaries = hist_from_dataloader(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "# # histo of data\n",
    "# axs[0, 0].stairs(in_bin_calib, bin_boundaries, fill=True)\n",
    "# axs[0, 0].set_title('on calib data')\n",
    "# axs[0, 1].stairs(in_bin_test, bin_boundaries, fill=True)\n",
    "# axs[0, 1].set_title('on test data')\n",
    "# for ax in [axs[0, 0], axs[0, 1]]:\n",
    "#     ax.set_xlabel('confidence')\n",
    "#     ax.set_ylabel('% of samples')\n",
    "\n",
    "# # reliability diagrams\n",
    "# for ax in axs.flatten()[2:]:\n",
    "#     ax.set_xlabel('confidence')\n",
    "#     ax.set_ylabel('accuracy')\n",
    "\n",
    "# list_names = ['on calib data before TS', 'on test data before TS', 'on calib data after TS', 'on test data after TS']\n",
    "# list_avg_confid = [confid_calib_before_TS, confid_test_before_TS, confid_calib_after_TS, confid_test_after_TS]\n",
    "# list_acc = [acc_calib_before_TS, acc_test_before_TS, acc_calib_after_TS, acc_test_after_TS]\n",
    "# list_metrics = [metrics_calib_before_TS, metrics_test_before_TS, metrics_calib_after_TS, metrics_test_after_TS]\n",
    "# for ax, name, avg_confid, acc, metrics in zip(axs.flatten()[2:], list_names, list_avg_confid, list_acc, list_metrics):\n",
    "#     ax.stairs(avg_confid, bin_boundaries, fill=True, alpha=0.8, label='perfect')\n",
    "#     ax.stairs(acc, bin_boundaries, fill=True, alpha=0.8, label='real')\n",
    "#     ax.legend(loc='lower right')\n",
    "#     if 'after TS' in name: ax.text(0, 0.1, f'Temp: {model.temperature.item():.3f}', fontsize='large')\n",
    "#     ax.text(0, 0.8, f'ECE: {100*metrics[\"ece\"]:.3f}%', fontsize='large')\n",
    "#     ax.text(0, 0.7, f'SCE: {100*metrics[\"sce\"]:.3f}%', fontsize='large')\n",
    "#     ax.text(0, 0.6, f'RMSCE: {100*metrics[\"rmsce\"]:.3f}%', fontsize='large')\n",
    "#     ax.text(0, 0.5, f'ACE: {100*metrics[\"ace\"]:.3f}%', fontsize='large')\n",
    "#     ax.set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticImageDataset(Dataset):\n",
    "    def __init__(self, generator, max_len):\n",
    "        self.G = generator\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        z = torch.randn([1, G.z_dim]).cuda() # latent codes\n",
    "        label = torch.randint(G.c_dim, (1,)).cuda()\n",
    "        c = torch.nn.functional.one_hot(label, num_classes=G.c_dim) # class labels\n",
    "        img = G(z, c, truncation_psi=1) # NCHW, float32, dynamic range [-1, +1]\n",
    "        img = postprocess_synthetic_images(img)\n",
    "        img = preprocess_images_classifier(img).squeeze()\n",
    "        return img, label.squeeze()\n",
    "\n",
    "class FilteredSyntheticImageDataset(Dataset):\n",
    "    def __init__(self, generator, max_len, classifier):\n",
    "        self.G = generator\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        z = torch.randn([1, G.z_dim]).cuda() # latent codes\n",
    "        label = torch.randint(G.c_dim, (1,)).cuda()\n",
    "        c = torch.nn.functional.one_hot(label, num_classes=G.c_dim) # class labels\n",
    "        img = G(z, c, truncation_psi=1) # NCHW, float32, dynamic range [-1, +1]\n",
    "        img = postprocess_synthetic_images(img)\n",
    "        img = preprocess_images_classifier(img).squeeze()\n",
    "        return img, label.squeeze()\n",
    "    \n",
    "dataset_synthetic = SyntheticImageDataset(G, 10000)\n",
    "dataloader_calib_synthetic = DataLoader(dataset_synthetic, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_MSP_correct(dataloader, classifier):\n",
    "\n",
    "#     classifier.eval()\n",
    "#     msp = torch.zeros((len(dataloader.dataset)))\n",
    "#     correct = torch.zeros((len(dataloader.dataset)))\n",
    "#     idx = 0\n",
    "#     for X, y in dataloader:\n",
    "#         batch_size = X.shape[0]\n",
    "#         X, y = X.cuda(), y.cuda()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             logits = classifier(X)\n",
    "#             probas, class_pred = torch.max(torch.softmax(logits, axis=1), axis=1)\n",
    "#         msp[idx:idx+batch_size] = probas\n",
    "#         correct[idx:idx+batch_size] = class_pred == y.squeeze()\n",
    "#         idx += batch_size\n",
    "\n",
    "#     return msp, correct\n",
    "\n",
    "# msp_synthetic, correct_synthetic = get_MSP_correct(dataloader_calib_synthetic, classifier)\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "# ax.set_xlabel('MSP value')\n",
    "# ax.hist(msp_test, alpha=0.5, bins=50, log=True, density=True, label='test')\n",
    "# ax.hist(msp_synthetic, alpha=0.5, bins=50, log=True, density=True, label='synthetic')\n",
    "# ax.legend()\n",
    "\n",
    "# print(f'generator - classifier agreement: {correct_synthetic.sum() / len(correct_synthetic):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_calib_before_TS = metrics_from_dataloader(classifier, dataloader_calib_synthetic)\n",
    "metrics_test_before_TS = metrics_from_dataloader(classifier, dataloader_test)\n",
    "confid_calib_before_TS, acc_calib_before_TS, in_bin_calib, bin_boundaries = hist_from_dataloader(classifier, dataloader_calib_synthetic)\n",
    "confid_test_before_TS, acc_test_before_TS, in_bin_test, bin_boundaries = hist_from_dataloader(classifier, dataloader_test)\n",
    "\n",
    "# Performing temperature scaling\n",
    "model = ModelWithTemperature(classifier).cuda()\n",
    "model.set_temperature(dataloader_calib_synthetic)\n",
    "\n",
    "metrics_calib_after_TS = metrics_from_dataloader(model, dataloader_calib_synthetic)\n",
    "metrics_test_after_TS = metrics_from_dataloader(model, dataloader_test)\n",
    "confid_calib_after_TS, acc_calib_after_TS, _, bin_boundaries = hist_from_dataloader(model, dataloader_calib_synthetic)\n",
    "confid_test_after_TS, acc_test_after_TS, _, bin_boundaries = hist_from_dataloader(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "# histo of data\n",
    "axs[0, 0].stairs(in_bin_calib, bin_boundaries, fill=True)\n",
    "axs[0, 0].set_title('on calib data')\n",
    "axs[0, 1].stairs(in_bin_test, bin_boundaries, fill=True)\n",
    "axs[0, 1].set_title('on test data')\n",
    "for ax in [axs[0, 0], axs[0, 1]]:\n",
    "    ax.set_xlabel('confidence')\n",
    "    ax.set_ylabel('% of samples')\n",
    "\n",
    "# reliability diagrams\n",
    "for ax in axs.flatten()[2:]:\n",
    "    ax.set_xlabel('confidence')\n",
    "    ax.set_ylabel('accuracy')\n",
    "\n",
    "list_names = ['on calib data before TS', 'on test data before TS', 'on calib data after TS', 'on test data after TS']\n",
    "list_avg_confid = [confid_calib_before_TS, confid_test_before_TS, confid_calib_after_TS, confid_test_after_TS]\n",
    "list_acc = [acc_calib_before_TS, acc_test_before_TS, acc_calib_after_TS, acc_test_after_TS]\n",
    "list_metrics = [metrics_calib_before_TS, metrics_test_before_TS, metrics_calib_after_TS, metrics_test_after_TS]\n",
    "for ax, name, avg_confid, acc, metrics in zip(axs.flatten()[2:], list_names, list_avg_confid, list_acc, list_metrics):\n",
    "    ax.stairs(avg_confid, bin_boundaries, fill=True, alpha=0.8, label='perfect')\n",
    "    ax.stairs(acc, bin_boundaries, fill=True, alpha=0.8, label='real')\n",
    "    ax.legend(loc='lower right')\n",
    "    if 'after TS' in name: ax.text(0, 0.1, f'Temp: {model.temperature.item():.3f}', fontsize='large')\n",
    "    ax.text(0, 0.8, f'ECE: {100*metrics[\"ece\"]:.3f}%', fontsize='large')\n",
    "    ax.text(0, 0.7, f'SCE: {100*metrics[\"sce\"]:.3f}%', fontsize='large')\n",
    "    ax.text(0, 0.6, f'RMSCE: {100*metrics[\"rmsce\"]:.3f}%', fontsize='large')\n",
    "    ax.text(0, 0.5, f'ACE: {100*metrics[\"ace\"]:.3f}%', fontsize='large')\n",
    "    ax.set_title(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10000 samples\n",
    "# ece 2.658\n",
    "# sce 0.468\n",
    "# rmsce 4.701\n",
    "# ace 0.797\n",
    "# temp 1.113\n",
    "\n",
    "# 20000 samples\n",
    "# almost same (temp 1.114)\n",
    "\n",
    "# 5000 samples\n",
    "# a bit worse (temp 1.120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector scaling from real validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_calib_before_TS = metrics_from_dataloader(classifier, dataloader_calib)\n",
    "metrics_test_before_TS = metrics_from_dataloader(classifier, dataloader_test)\n",
    "confid_calib_before_TS, acc_calib_before_TS, in_bin_calib, bin_boundaries = hist_from_dataloader(classifier, dataloader_calib)\n",
    "confid_test_before_TS, acc_test_before_TS, in_bin_test, bin_boundaries = hist_from_dataloader(classifier, dataloader_test)\n",
    "\n",
    "# Fit vector scaling\n",
    "vs = VectorScaling(logit_input=True, logit_constant=0.0)\n",
    "logits_list = []\n",
    "labels_list = []\n",
    "for input, label in dataloader_calib:\n",
    "    input = input.cuda()\n",
    "    with torch.no_grad():\n",
    "        logits = classifier(input)\n",
    "    logits_list.append(logits)\n",
    "    labels_list.append(label)\n",
    "logits = torch.cat(logits_list).cpu()\n",
    "labels = torch.cat(labels_list).cpu()\n",
    "vs.fit(logits.numpy(), labels.numpy())\n",
    "\n",
    "metrics_calib_after_TS = metrics_from_dataloader(classifier, dataloader_calib, vs)\n",
    "metrics_test_after_TS = metrics_from_dataloader(classifier, dataloader_test, vs)\n",
    "confid_calib_after_TS, acc_calib_after_TS, _, bin_boundaries = hist_from_dataloader(classifier, dataloader_calib, vs)\n",
    "confid_test_after_TS, acc_test_after_TS, _, bin_boundaries = hist_from_dataloader(classifier, dataloader_test, vs)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "# histo of data\n",
    "axs[0, 0].stairs(in_bin_calib, bin_boundaries, fill=True)\n",
    "axs[0, 0].set_title('on calib data')\n",
    "axs[0, 1].stairs(in_bin_test, bin_boundaries, fill=True)\n",
    "axs[0, 1].set_title('on test data')\n",
    "for ax in [axs[0, 0], axs[0, 1]]:\n",
    "    ax.set_xlabel('confidence')\n",
    "    ax.set_ylabel('% of samples')\n",
    "\n",
    "# reliability diagrams\n",
    "for ax in axs.flatten()[2:]:\n",
    "    ax.set_xlabel('confidence')\n",
    "    ax.set_ylabel('accuracy')\n",
    "\n",
    "list_names = ['on calib data before calib', 'on test data before calib', 'on calib data after calib', 'on test data after calib']\n",
    "list_avg_confid = [confid_calib_before_TS, confid_test_before_TS, confid_calib_after_TS, confid_test_after_TS]\n",
    "list_acc = [acc_calib_before_TS, acc_test_before_TS, acc_calib_after_TS, acc_test_after_TS]\n",
    "list_metrics = [metrics_calib_before_TS, metrics_test_before_TS, metrics_calib_after_TS, metrics_test_after_TS]\n",
    "for ax, name, avg_confid, acc, metrics in zip(axs.flatten()[2:], list_names, list_avg_confid, list_acc, list_metrics):\n",
    "    ax.stairs(avg_confid, bin_boundaries, fill=True, alpha=0.8, label='perfect')\n",
    "    ax.stairs(acc, bin_boundaries, fill=True, alpha=0.8, label='real')\n",
    "    ax.legend(loc='lower right')\n",
    "    # if 'after TS' in name: ax.text(0, 0.1, f'Temp: {model.temperature.item():.3f}', fontsize='large')\n",
    "    ax.text(0, 0.8, f'ECE: {100*metrics[\"ece\"]:.3f}%', fontsize='large')\n",
    "    ax.text(0, 0.7, f'SCE: {100*metrics[\"sce\"]:.3f}%', fontsize='large')\n",
    "    ax.text(0, 0.6, f'RMSCE: {100*metrics[\"rmsce\"]:.3f}%', fontsize='large')\n",
    "    ax.text(0, 0.5, f'ACE: {100*metrics[\"ace\"]:.3f}%', fontsize='large')\n",
    "    ax.set_title(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import (train_test_split,\n",
    "                                     StratifiedKFold,\n",
    "                                     GridSearchCV,\n",
    "                                     cross_val_score)\n",
    "from dirichletcal.calib.fulldirichlet import FullDirichletCalibrator\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "reg = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "# Full Dirichlet\n",
    "calibrator = FullDirichletCalibrator(reg_lambda=reg, reg_mu=None)\n",
    "# ODIR Dirichlet\n",
    "#calibrator = FullDirichletCalibrator(reg_lambda=reg, reg_mu=reg)\n",
    "gscv = GridSearchCV(calibrator, param_grid={'reg_lambda':  reg,\n",
    "                                            'reg_mu': [None]},\n",
    "                    cv=skf, scoring='neg_log_loss')\n",
    "gscv.fit(logits.numpy(), labels.numpy())\n",
    "\n",
    "print('Grid of parameters cross-validated')\n",
    "print(gscv.param_grid)\n",
    "print('Best parameters: {}'.format(gscv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
