{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "from transformers import CLIPTokenizer, CLIPTextModel, CLIPVisionModel, CLIPFeatureExtractor\n",
    "\n",
    "path_results = '../results/loop_clip_sd/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "class FrozenCLIPEmbedder(AbstractEncoder):\n",
    "    \"\"\"Uses the CLIP transformer encoder for text (from Hugging Face)\"\"\"\n",
    "    def __init__(self, version=\"openai/clip-vit-large-patch14\", device=\"cuda\", max_length=77):\n",
    "        super().__init__()\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(version)\n",
    "        self.transformer = CLIPTextModel.from_pretrained(version)\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.freeze()\n",
    "\n",
    "    def freeze(self):\n",
    "        self.transformer = self.transformer.eval()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, text):\n",
    "        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n",
    "                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        tokens = batch_encoding[\"input_ids\"].to(self.device)\n",
    "        outputs = self.transformer(input_ids=tokens)\n",
    "\n",
    "        z = outputs.last_hidden_state\n",
    "        return z\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Loading model from /gpfswork/rech/dcf/ulb98yg/DATA/sd-v1-4.ckpt\n",
      "Global Step: 470000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\n",
      "Sampling:   0%|                                           | 0/2 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Conditioning: torch.Size([3, 77, 768])\n",
      "\n",
      "Data shape for PLMS sampling is (3, 4, 64, 64)\n",
      "Running PLMS Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "PLMS Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   2%|▌                             | 1/50 [00:00<00:31,  1.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   4%|█▏                            | 2/50 [00:00<00:19,  2.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   6%|█▊                            | 3/50 [00:01<00:15,  3.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   8%|██▍                           | 4/50 [00:01<00:13,  3.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  10%|███                           | 5/50 [00:01<00:12,  3.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  12%|███▌                          | 6/50 [00:01<00:11,  3.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  14%|████▏                         | 7/50 [00:02<00:10,  3.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  16%|████▊                         | 8/50 [00:02<00:10,  3.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  18%|█████▍                        | 9/50 [00:02<00:10,  4.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  20%|█████▊                       | 10/50 [00:02<00:09,  4.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  22%|██████▍                      | 11/50 [00:03<00:09,  4.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  24%|██████▉                      | 12/50 [00:03<00:09,  4.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  26%|███████▌                     | 13/50 [00:03<00:08,  4.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  28%|████████                     | 14/50 [00:03<00:08,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  30%|████████▋                    | 15/50 [00:04<00:08,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  32%|█████████▎                   | 16/50 [00:04<00:08,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  34%|█████████▊                   | 17/50 [00:04<00:07,  4.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  36%|██████████▍                  | 18/50 [00:04<00:07,  4.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  38%|███████████                  | 19/50 [00:04<00:07,  4.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  40%|███████████▌                 | 20/50 [00:05<00:07,  4.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  42%|████████████▏                | 21/50 [00:05<00:06,  4.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  44%|████████████▊                | 22/50 [00:05<00:06,  4.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  46%|█████████████▎               | 23/50 [00:05<00:06,  4.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  48%|█████████████▉               | 24/50 [00:06<00:06,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  50%|██████████████▌              | 25/50 [00:06<00:06,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  52%|███████████████              | 26/50 [00:06<00:05,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  54%|███████████████▋             | 27/50 [00:06<00:05,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  56%|████████████████▏            | 28/50 [00:07<00:05,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  58%|████████████████▊            | 29/50 [00:07<00:05,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  60%|█████████████████▍           | 30/50 [00:07<00:04,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  62%|█████████████████▉           | 31/50 [00:07<00:04,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  64%|██████████████████▌          | 32/50 [00:08<00:04,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  66%|███████████████████▏         | 33/50 [00:08<00:04,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  68%|███████████████████▋         | 34/50 [00:08<00:03,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  70%|████████████████████▎        | 35/50 [00:08<00:03,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  72%|████████████████████▉        | 36/50 [00:09<00:03,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  74%|█████████████████████▍       | 37/50 [00:09<00:03,  4.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  76%|██████████████████████       | 38/50 [00:09<00:02,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  78%|██████████████████████▌      | 39/50 [00:09<00:02,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  80%|███████████████████████▏     | 40/50 [00:10<00:02,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  82%|███████████████████████▊     | 41/50 [00:10<00:02,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  84%|████████████████████████▎    | 42/50 [00:10<00:01,  4.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  86%|████████████████████████▉    | 43/50 [00:10<00:01,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  88%|█████████████████████████▌   | 44/50 [00:11<00:01,  4.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  90%|██████████████████████████   | 45/50 [00:11<00:01,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  92%|██████████████████████████▋  | 46/50 [00:11<00:00,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  94%|███████████████████████████▎ | 47/50 [00:11<00:00,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  96%|███████████████████████████▊ | 48/50 [00:11<00:00,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  98%|████████████████████████████▍| 49/50 [00:12<00:00,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler: 100%|█████████████████████████████| 50/50 [00:12<00:00,  4.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:14<00:00, 14.45s/it]\u001b[A\n",
      "Sampling:  50%|█████████████████▌                 | 1/2 [00:14<00:14, 14.45s/it]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Conditioning: torch.Size([3, 77, 768])\n",
      "\n",
      "Data shape for PLMS sampling is (3, 4, 64, 64)\n",
      "Running PLMS Sampling with 50 timesteps\n",
      "\n",
      "\n",
      "PLMS Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   2%|▌                             | 1/50 [00:00<00:23,  2.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   4%|█▏                            | 2/50 [00:00<00:16,  2.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   6%|█▊                            | 3/50 [00:00<00:13,  3.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   8%|██▍                           | 4/50 [00:01<00:12,  3.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  10%|███                           | 5/50 [00:01<00:11,  3.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  12%|███▌                          | 6/50 [00:01<00:11,  3.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  14%|████▏                         | 7/50 [00:01<00:10,  4.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  16%|████▊                         | 8/50 [00:02<00:10,  4.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  18%|█████▍                        | 9/50 [00:02<00:10,  4.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  20%|█████▊                       | 10/50 [00:02<00:09,  4.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  22%|██████▍                      | 11/50 [00:02<00:09,  4.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  24%|██████▉                      | 12/50 [00:03<00:09,  4.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  26%|███████▌                     | 13/50 [00:03<00:08,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  28%|████████                     | 14/50 [00:03<00:08,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  30%|████████▋                    | 15/50 [00:03<00:08,  4.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  32%|█████████▎                   | 16/50 [00:04<00:08,  4.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  34%|█████████▊                   | 17/50 [00:04<00:07,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  36%|██████████▍                  | 18/50 [00:04<00:07,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  38%|███████████                  | 19/50 [00:04<00:07,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  40%|███████████▌                 | 20/50 [00:05<00:07,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  42%|████████████▏                | 21/50 [00:05<00:06,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  44%|████████████▊                | 22/50 [00:05<00:06,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  46%|█████████████▎               | 23/50 [00:05<00:06,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  48%|█████████████▉               | 24/50 [00:06<00:06,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  50%|██████████████▌              | 25/50 [00:06<00:06,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  52%|███████████████              | 26/50 [00:06<00:05,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  54%|███████████████▋             | 27/50 [00:06<00:05,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  56%|████████████████▏            | 28/50 [00:06<00:05,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  58%|████████████████▊            | 29/50 [00:07<00:05,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  60%|█████████████████▍           | 30/50 [00:07<00:04,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  62%|█████████████████▉           | 31/50 [00:07<00:04,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  64%|██████████████████▌          | 32/50 [00:07<00:04,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  66%|███████████████████▏         | 33/50 [00:08<00:04,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  68%|███████████████████▋         | 34/50 [00:08<00:03,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  70%|████████████████████▎        | 35/50 [00:08<00:03,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  72%|████████████████████▉        | 36/50 [00:08<00:03,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  74%|█████████████████████▍       | 37/50 [00:09<00:03,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  76%|██████████████████████       | 38/50 [00:09<00:02,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  78%|██████████████████████▌      | 39/50 [00:09<00:02,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  80%|███████████████████████▏     | 40/50 [00:09<00:02,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  82%|███████████████████████▊     | 41/50 [00:10<00:02,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  84%|████████████████████████▎    | 42/50 [00:10<00:01,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  86%|████████████████████████▉    | 43/50 [00:10<00:01,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  88%|█████████████████████████▌   | 44/50 [00:10<00:01,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  90%|██████████████████████████   | 45/50 [00:11<00:01,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  92%|██████████████████████████▋  | 46/50 [00:11<00:00,  4.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  94%|███████████████████████████▎ | 47/50 [00:11<00:00,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  96%|███████████████████████████▊ | 48/50 [00:11<00:00,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  98%|████████████████████████████▍| 49/50 [00:12<00:00,  4.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler: 100%|█████████████████████████████| 50/50 [00:12<00:00,  4.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:13<00:00, 13.82s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 2/2 [00:28<00:00, 14.13s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "/gpfswork/rech/dcf/ulb98yg/dream-domain/results/loop_clip_sd/iter0 \n",
      " \n",
      "Enjoy.\n"
     ]
    }
   ],
   "source": [
    "text = ['santa is coming tonight', 'a picture of a dog', 'a picture of a cat']\n",
    "with open(f'{path_results}/conditioning_text.txt', 'w') as f:\n",
    "    for item in text:\n",
    "        f.write(f'{item}\\n')\n",
    "text_encoder = FrozenCLIPEmbedder().cuda()\n",
    "\n",
    "\n",
    "for iter in range(1):\n",
    "\n",
    "    # Create and save conditioning\n",
    "    cond = text_encoder(text).cpu().numpy()\n",
    "    cond_filename = f'{path_results}/conditioning_embedding_iter{iter}.npy'\n",
    "    np.save(cond_filename, cond)\n",
    "\n",
    "    # Generate\n",
    "    !PYTHONPATH=\"$PYTHONPATH:./stable-diffusion-main\" \\\n",
    "        python stable-diffusion-main/scripts/txt2img.py --plms \\\n",
    "        --config './stable-diffusion-main/configs/stable-diffusion/v1-inference.yaml' \\\n",
    "        --ckpt /gpfswork/rech/dcf/ulb98yg/DATA/sd-v1-4.ckpt \\\n",
    "        --outdir '/gpfswork/rech/dcf/ulb98yg/dream-domain/results/loop_clip_sd/iter{iter}' \\\n",
    "        --cond-from-file {cond_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip_text_model to instantiate a model of type clip_vision_model. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CLIPVisionModel were not initialized from the model checkpoint at openai/clip-vit-large-patch14 and are newly initialized: ['vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "img_encoder = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "img = processor(images=torch.randn(3, 64, 64))['pixel_values'][0]\n",
    "img = torch.tensor(img).unsqueeze(0)\n",
    "img_encoder(img)['last_hidden_state'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size embedding text and image not the same! Stable diffusion conditioned on full sequence text embedding, not on multimodal vector. Impossible to reproject image in CLIP and use embedding as condition..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.0.1+py3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
