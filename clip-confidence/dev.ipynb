{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import CLIPModel, CLIPTokenizer, CLIPProcessor\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "path_huggingface = os.path.expandvars('$DSDIR/HuggingFace_Models/') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_model():\n",
    "\n",
    "    # Load the pre-trained CLIP model\n",
    "    model = CLIPModel.from_pretrained(path_huggingface+'openai/clip-vit-large-patch14')\n",
    "    model = model.cuda()\n",
    "\n",
    "    # Load the corresponding tokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(path_huggingface+'openai/clip-vit-large-patch14')\n",
    "    processor = CLIPProcessor.from_pretrained(path_huggingface+\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    return model, tokenizer, processor\n",
    "\n",
    "def load_bert_model():\n",
    "\n",
    "    bert_model = SentenceTransformer(path_huggingface+'sentence-transformers/paraphrase-distilroberta-base-v1')\n",
    "\n",
    "    return bert_model\n",
    "\n",
    "def compute_text_embeddings(premises):\n",
    "\n",
    "    num_premises = len(premises)\n",
    "    batch_size = 512\n",
    "\n",
    "    clip_model, tokenizer, processor = load_clip_model()\n",
    "    bert_model = load_bert_model()\n",
    "\n",
    "    # Compute the embeddings for each batch of premises\n",
    "    bert_text_embeds_prompts = []\n",
    "    for i in tqdm(range(0, len(premises), batch_size)):\n",
    "        premises_batch = premises[i:i+batch_size]\n",
    "        with torch.no_grad():\n",
    "            text_embeds_prompts_batch = bert_model.encode(premises_batch)\n",
    "\n",
    "        text_embeds_prompts_batch = torch.from_numpy(text_embeds_prompts_batch)\n",
    "        text_embeds_prompts_batch = F.normalize(text_embeds_prompts_batch, dim=1)\n",
    "\n",
    "        bert_text_embeds_prompts.append(text_embeds_prompts_batch)\n",
    "\n",
    "    # Concatenate the embeddings for all batches\n",
    "    bert_text_embeds_prompts = torch.cat(bert_text_embeds_prompts, dim=0)\n",
    "\n",
    "\n",
    "    # split the premises into batches\n",
    "    premises_batches = [premises[i:i+batch_size] for i in range(0, num_premises, batch_size)]\n",
    "\n",
    "    # compute the embeddings for each batch of premises\n",
    "    clip_text_embeds_prompts = torch.zeros(num_premises, 768)\n",
    "    for i, premises_batch in enumerate(tqdm(premises_batches)):\n",
    "        tok = tokenizer(premises_batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        for key in tok.keys():\n",
    "            tok[key] = tok[key].cuda()\n",
    "        with torch.no_grad():\n",
    "            text_outputs = clip_model.text_model(**tok)\n",
    "        text_embeds = text_outputs[1]\n",
    "        text_embeds = clip_model.text_projection(text_embeds)\n",
    "        text_embeds_prompt = F.normalize(text_embeds, dim=1)\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, num_premises)\n",
    "        clip_text_embeds_prompts[start_idx:end_idx, :] = text_embeds_prompt\n",
    "\n",
    "    return bert_text_embeds_prompts, clip_text_embeds_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('similar_from_MS-COCO_top0_do_steerFalse_steer.csv', header=0, names=['sample_1', 'sample_2', 'CLIP_similarity', 'BERT_similarity', 'diff'])\n",
    "for col in ['sample_1', 'sample_2']:\n",
    "    df[col] = df[col].str.replace('  ', ' ')\n",
    "    df[col] = df[col].str.replace(' .', '.')\n",
    "    df[col] = df[col].str.lstrip()\n",
    "    df[col] = df[col].str.rstrip()\n",
    "\n",
    "df = df[df['sample_1'] != df['sample_2']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot CLIP similarity histogram\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(df['CLIP_similarity'], bins=20, color='blue')\n",
    "plt.xlabel('CLIP Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of CLIP Similarity')\n",
    "\n",
    "# Plot BERT similarity histogram\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(df['BERT_similarity'], bins=20, color='blue')\n",
    "plt.xlabel('BERT Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of BERT Similarity')\n",
    "\n",
    "# Plot diff histogram\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(df['diff'], bins=20, color='green')\n",
    "plt.xlabel('Difference')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Difference')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_bad = (df['CLIP_similarity'] > 0.9) & (df['diff'].abs() > 0.2)\n",
    "df[mask_bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_good = (df['CLIP_similarity'] > 0.9) & (df['diff'].abs() < 0.05)\n",
    "df[mask_good]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build negative data (text not correctly embedded by CLIP)\n",
    "text_bad = pd.concat((df.loc[mask_bad, 'sample_1'], df.loc[mask_bad, 'sample_2'])).to_list()\n",
    "y_bad = np.zeros(len(text_bad))\n",
    "\n",
    "# build positive data\n",
    "text_good = pd.concat((df.loc[mask_good, 'sample_1'], df.loc[mask_good, 'sample_2'])).to_list()\n",
    "y_good = np.ones(len(text_good))\n",
    "\n",
    "# build train/test sets\n",
    "test_set_fraction = 0.1\n",
    "text_train, text_test, y_train, y_test = train_test_split(text_bad+text_good, np.concatenate((y_bad, y_good)), test_size=test_set_fraction, random_state=123)\n",
    "\n",
    "X_train_bert, X_train_clip = compute_text_embeddings(text_train)\n",
    "X_test_bert, X_test_clip = compute_text_embeddings(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_test_clip)\n",
    "\n",
    "# Plot the t-SNE visualization\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.title('t-SNE Visualization of Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_test_bert)\n",
    "\n",
    "# Plot the t-SNE visualization\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.title('t-SNE Visualization of Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train_clip, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(768, 64)\n",
    "        self.layer2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.sigmoid(self.layer2(x))\n",
    "        return x\n",
    "model = BinaryClassifier()\n",
    "\n",
    "train_data = TensorDataset(torch.FloatTensor(X_train_clip.detach().numpy()), torch.LongTensor(y_train))\n",
    "test_data = TensorDataset(torch.FloatTensor(X_test_clip.detach().numpy()), torch.LongTensor(y_test))\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):  # Number of epochs\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(X_batch).squeeze()  # Forward pass\n",
    "        loss = criterion(outputs, y_batch.float())  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "    if (epoch+1) % 10 == 0:  # Print loss every 10 epochs\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_clip.detach().numpy())\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_pred, y_test, alpha=0.5)\n",
    "plt.xlabel('pred')\n",
    "plt.ylabel('true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
