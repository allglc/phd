{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import assign_free_gpus\n",
    "assign_free_gpus()\n",
    "\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from generate_data import MoonsDataModule, MoonsDataset\n",
    "from models import LinearClassifier, Classifier, GAN, ContrastiveMapping\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "rng = np.random.default_rng(0)\n",
    "path_results = Path.cwd().parent / 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = 0.3\n",
    "# path_classifier = path_results / 'classifier' / '2022-12-06_163615_linear_noise0.3' / 'checkpoints' / 'epoch=99-step=6300.ckpt'\n",
    "# path_gan = path_results / 'GAN' / '2022-11-29_152439_noise0.3' / 'checkpoints' / 'epoch=99-step=12600.ckpt'\n",
    "# # path_model = path_results / 'contrastiveMapping' / '2023-01-17_170210_linear_noise0.3' / 'checkpoints' / 'epoch=59-step=15120.ckpt'\n",
    "# path_model = path_results / 'contrastiveMapping' / '2023-01-17_171054_linear_noise0.3_learndistrib' / 'checkpoints' / 'epoch=59-step=15120.ckpt'\n",
    "\n",
    "\n",
    "\n",
    "noise = 0.1\n",
    "path_classifier = path_results / 'classifier' / '2022-11-29_152904_linear_noise0.1' / 'checkpoints' / 'epoch=99-step=6300.ckpt'\n",
    "path_gan = path_results / 'GAN' / '2022-11-29_152457_noise0.1' / 'checkpoints' / 'epoch=99-step=12600.ckpt'\n",
    "# path_model = path_results / 'contrastiveMapping' / '2023-01-17_162629_linear_noise0.1' / 'checkpoints' / 'epoch=59-step=15120.ckpt'\n",
    "# path_model = path_results / 'contrastiveMapping' / '2023-01-17_163649_linear_noise0.1_learndistrib' / 'checkpoints' / 'epoch=59-step=15120.ckpt'\n",
    "# path_model = path_results / 'contrastiveMapping' / '2023-01-17_174727_linear_noise0.1_classifLossMSE' / 'checkpoints' / 'epoch=59-step=15120.ckpt'\n",
    "# path_model = path_results / 'contrastiveMapping' / '2023-01-17_175117_linear_noise0.1_classifLossKLDiv' / 'checkpoints' / 'epoch=59-step=15120.ckpt'\n",
    "path_model = path_results / 'contrastiveMapping' / '2023-01-17_175828_linear_noise0.1_learndistrib_classifLossMSE' / 'checkpoints' / 'epoch=59-step=15120.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LinearClassifier.load_from_checkpoint(str(path_classifier))\n",
    "gan = GAN.load_from_checkpoint(str(path_gan))\n",
    "\n",
    "data_test = MoonsDataset(n_samples=10000, noise=noise, random_state=2)\n",
    "x_test = data_test.x\n",
    "y_test = data_test.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(accelerator='auto', devices=1)\n",
    "trainer.validate(classifier, datamodule=MoonsDataModule(n_samples=20000, noise=noise, random_state=2))\n",
    "\n",
    "# SHOW DESCISION BOUNDARY\n",
    "x = np.linspace(-2, 3, 100)\n",
    "y = np.linspace(-2, 2, 100)\n",
    "\n",
    "grid_data = np.zeros((len(x)*len(y), 2))\n",
    "i = 0\n",
    "for x_ in x:\n",
    "    for y_ in y:\n",
    "        grid_data[i] = [x_, y_]\n",
    "        i += 1\n",
    "grid_data = torch.from_numpy(grid_data).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = classifier(grid_data)\n",
    "class_pred = torch.sigmoid(y).round().cpu().flatten()#.numpy()\n",
    "\n",
    "# SHOW CLASSIF LOSS\n",
    "with torch.no_grad():\n",
    "    logits = classifier(x_test).squeeze()\n",
    "    classif_loss = F.binary_cross_entropy_with_logits(logits, y_test, reduction='none')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('classifier decision boundary')\n",
    "ax.scatter(grid_data[class_pred==0, 0], grid_data[class_pred==0, 1], alpha=1, c='C0', label='predicted class 0')\n",
    "ax.scatter(grid_data[class_pred!=0, 0], grid_data[class_pred!=0, 1], alpha=1, c='C1', label='predicted class 1')\n",
    "ax.scatter(x_test[y_test==0, 0], x_test[y_test==0, 1], alpha=0.1, c=classif_loss[y_test==0], cmap='Reds', marker='o', label='real data - class 0')\n",
    "im = ax.scatter(x_test[y_test==1, 0], x_test[y_test==1, 1], alpha=0.1, c=classif_loss[y_test==1], cmap='Reds', marker='+', label='real data - class 1')\n",
    "leg = ax.legend(frameon=True)\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "cbar = fig.colorbar(im, ax=ax, label='classifier loss')\n",
    "cbar.solids.set(alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(x_test)\n",
    "z = torch.randn(n_samples, gan.latent_dim, device=gan.device)\n",
    "if gan.c_dim > 0:\n",
    "    rnd_label = torch.randint(gan.c_dim, size=(z.shape[0],), device=gan.device)\n",
    "    c = F.one_hot(rnd_label, num_classes=gan.c_dim)\n",
    "    z = torch.cat([z, c], dim=1)\n",
    "with torch.no_grad():\n",
    "    w = gan.generator.mapping(z)\n",
    "    x_fake = gan.generator.synthesis(w).detach().cpu().numpy()\n",
    "    rnd_label = rnd_label.cpu().numpy()\n",
    "    w = w.detach().cpu().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('fake vs. real data')\n",
    "plt.scatter(x_test[y_test==0, 0], x_test[y_test==0, 1], alpha=0.5, c='C0', label='real data - class 0')\n",
    "plt.scatter(x_test[y_test==1, 0], x_test[y_test==1, 1], alpha=0.5, c='C1', label='real data - class 1')\n",
    "plt.scatter(x_fake[rnd_label==0, 0], x_fake[rnd_label==0, 1], alpha=0.5, c='C2', label='fake data - class 0')\n",
    "plt.scatter(x_fake[rnd_label==1, 0], x_fake[rnd_label==1, 1], alpha=0.5, c='C3', label='fake data - class 1')\n",
    "plt.legend()\n",
    "\n",
    "# w_embedded = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(w)\n",
    "# plt.figure()\n",
    "# plt.title('t-SNE in W')\n",
    "# plt.scatter(w_embedded[rnd_label==0, 0], w_embedded[rnd_label==0, 1], alpha=0.5, c='C0', label='class 0')\n",
    "# plt.scatter(w_embedded[rnd_label==1, 0], w_embedded[rnd_label==1, 1], alpha=0.5, c='C1', label='class 1')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContrastiveMapping.load_from_checkpoint(str(path_model), gan=gan, classifier=classifier)\n",
    "\n",
    "def selection_function(x):\n",
    "    p_x_y0 = Normal(model.mapping.mean0, model.mapping.std0).log_prob(x).exp().detach()\n",
    "    p_x_y1 = Normal(model.mapping.mean1, model.mapping.std1).log_prob(x).exp().detach()\n",
    "    p_y0 = 0.5\n",
    "    p_y1 = 0.5\n",
    "    p_y0_x = p_x_y0 * p_y0 / (p_x_y0 * p_y0 + p_x_y1 * p_y1)\n",
    "    p_y1_x = p_x_y1 * p_y1 / (p_x_y0 * p_y0 + p_x_y1 * p_y1)\n",
    "    max_p_y_x = torch.maximum(p_y0_x, p_y1_x)\n",
    "    return max_p_y_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define domain for fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk coverage curves\n",
    "u, c = model.mapping.sample_u(x_test.shape[0])\n",
    "u_domain = u[:, -1]\n",
    "with torch.no_grad(): \n",
    "    logits = model(u)\n",
    "y = torch.sigmoid(logits).squeeze()\n",
    "classif_loss = F.binary_cross_entropy_with_logits(logits.squeeze(), c.float(), reduction='none')\n",
    "classif_correct = (y.round() == c)\n",
    "\n",
    "# baseline: random selection\n",
    "domain_cutoff_random = np.linspace(0, 1, 100)\n",
    "coverage_random = np.zeros_like(domain_cutoff_random)\n",
    "risk_random = np.zeros_like(domain_cutoff_random)\n",
    "acc_random = np.zeros_like(domain_cutoff_random)\n",
    "for i, cut in enumerate(domain_cutoff_random):\n",
    "    nb_samples = int((1-cut) * x_test.shape[0]) # 1-cut to be coherent with other indices below (low value -> high coverage)\n",
    "    idx_domain = rng.choice(np.arange(x_test.shape[0]), size=nb_samples, replace=False)\n",
    "    coverage_random[i] = x_test[idx_domain].shape[0] / x_test.shape[0]\n",
    "    risk_random[i] = classif_loss[idx_domain].mean()\n",
    "    acc_random[i] = classif_correct[idx_domain].float().mean()\n",
    "\n",
    "# baseline: max softmax\n",
    "domain_cutoff_baseline = np.linspace(0, 1, 1000)\n",
    "coverage_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "risk_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "acc_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "for i, cut in enumerate(domain_cutoff_baseline):\n",
    "    idx_domain = (y > cut) | (1-y > cut)\n",
    "    coverage_baseline[i] = idx_domain.float().mean()\n",
    "    risk_baseline[i] = classif_loss[idx_domain].mean()\n",
    "    acc_baseline[i] = classif_correct[idx_domain].float().mean()\n",
    "\n",
    "# cut max proba computed in U\n",
    "domain_cutoff = np.linspace(0.5, 1, 1000)\n",
    "coverage = np.zeros_like(domain_cutoff)\n",
    "risk = np.zeros_like(domain_cutoff)\n",
    "acc = np.zeros_like(domain_cutoff)\n",
    "for i, cut in enumerate(domain_cutoff):\n",
    "    idx_domain = selection_function(u_domain) > cut\n",
    "    coverage[i] = idx_domain.float().mean()\n",
    "    risk[i] = classif_loss[idx_domain].mean()\n",
    "    acc[i] = classif_correct[idx_domain].float().mean()\n",
    "\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "ax1.set_title('coverage vs. risk\\n(obtained by varying confidence/uncertainty threshold)\\n(using pseudo-labels)')\n",
    "ax1.plot(coverage, risk, label='cut in U')\n",
    "ax1.plot(coverage_baseline, risk_baseline, label='baseline (max softmax)')\n",
    "ax1.plot(coverage_random, risk_random, label='baseline (random)')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('coverage')\n",
    "ax1.set_ylabel('risk')\n",
    "\n",
    "ax2.set_title('coverage vs. accuracy\\n(obtained by varying confidence/uncertainty threshold)\\n(using pseudo-labels)')\n",
    "ax2.plot(coverage, acc, label='cut in U')\n",
    "ax2.plot(coverage_baseline, acc_baseline, label='baseline (max softmax)')\n",
    "ax2.plot(coverage_random, acc_random, label='baseline (random)')\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('coverage')\n",
    "ax2.set_ylabel('accuracy')\n",
    "\n",
    "ax3.set_title('coverage vs threshold value')\n",
    "ax3.plot((domain_cutoff-domain_cutoff.min())/(domain_cutoff.max()-domain_cutoff.min()), coverage, label='cut in U')\n",
    "ax3.plot((domain_cutoff_baseline-domain_cutoff_baseline.min())/(domain_cutoff_baseline.max()-domain_cutoff_baseline.min()), coverage_baseline, label='baseline (max softmax)')\n",
    "ax3.plot((domain_cutoff_random-domain_cutoff_random.min())/(domain_cutoff_random.max()-domain_cutoff_random.min()), coverage_random, label='baseline (random)')\n",
    "ax3.legend()\n",
    "ax3.set_xlabel('normalized threshold value')\n",
    "ax3.set_ylabel('coverage')\n",
    "\n",
    "ax4.set_title('risk vs threshold value')\n",
    "ax4.plot((domain_cutoff-domain_cutoff.min())/(domain_cutoff.max()-domain_cutoff.min()), risk, label='cut in U')\n",
    "ax4.plot((domain_cutoff_baseline-domain_cutoff_baseline.min())/(domain_cutoff_baseline.max()-domain_cutoff_baseline.min()), risk_baseline, label='baseline (max softmax)')\n",
    "ax4.plot((domain_cutoff_random-domain_cutoff_random.min())/(domain_cutoff_random.max()-domain_cutoff_random.min()), risk_random, label='baseline (random)')\n",
    "ax4.legend()\n",
    "ax4.set_xlabel('normalized threshold value')\n",
    "ax4.set_ylabel('risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define domain for real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk coverage curves for real test data\n",
    "u = model.encoder(x_test.to(model.device))\n",
    "u_domain = u[:, -1]\n",
    "\n",
    "with torch.no_grad(): \n",
    "    logits = model.classifier(x_test.to(model.device))\n",
    "y = torch.sigmoid(logits).squeeze()\n",
    "classif_loss = F.binary_cross_entropy_with_logits(logits.squeeze(), y_test, reduction='none')\n",
    "classif_correct = (y.round() == y_test)\n",
    "tcp = torch.zeros_like(y)\n",
    "tcp[y_test==0] = 1 - y[y_test==0]\n",
    "tcp[y_test==1] = y[y_test==1]\n",
    "\n",
    "# baseline: random selection\n",
    "domain_cutoff_random = np.linspace(0, 1, 100)\n",
    "coverage_random = np.zeros_like(domain_cutoff_random)\n",
    "risk_random = np.zeros_like(domain_cutoff_random)\n",
    "acc_random = np.zeros_like(domain_cutoff_random)\n",
    "for i, cut in enumerate(domain_cutoff_random):\n",
    "    nb_samples = int((1-cut) * x_test.shape[0]) # 1-cut to be coherent with other indices below (low value -> high coverage)\n",
    "    idx_domain = rng.choice(np.arange(x_test.shape[0]), size=nb_samples, replace=False)\n",
    "    coverage_random[i] = x_test[idx_domain].shape[0] / x_test.shape[0]\n",
    "    risk_random[i] = classif_loss[idx_domain].mean()\n",
    "    acc_random[i] = classif_correct[idx_domain].float().mean()\n",
    "    acc_random[i] = classif_correct[idx_domain].float().mean()\n",
    "    \n",
    "# baseline: max softmax\n",
    "domain_cutoff_baseline = np.linspace(0, 1, 1000)\n",
    "coverage_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "risk_baseline = np.zeros_like(domain_cutoff_baseline)\n",
    "acc_baseline = np.zeros_like(domain_cutoff)\n",
    "for i, cut in enumerate(domain_cutoff_baseline):\n",
    "    idx_domain = (y > cut) | (1-y > cut)\n",
    "    coverage_baseline[i] = idx_domain.float().mean()\n",
    "    risk_baseline[i] = classif_loss[idx_domain].mean()\n",
    "    acc_baseline[i] = classif_correct[idx_domain].float().mean()\n",
    "    \n",
    "# baseline: TCP\n",
    "domain_cutoff_baselineTCP = np.linspace(0, 1, 1000)\n",
    "coverage_baselineTCP = np.zeros_like(domain_cutoff_baselineTCP)\n",
    "risk_baselineTCP = np.zeros_like(domain_cutoff_baselineTCP)\n",
    "acc_baselineTCP = np.zeros_like(domain_cutoff)\n",
    "for i, cut in enumerate(domain_cutoff_baselineTCP):\n",
    "    idx_domain = tcp > cut\n",
    "    coverage_baselineTCP[i] = idx_domain.float().mean()\n",
    "    risk_baselineTCP[i] = classif_loss[idx_domain].mean()\n",
    "    acc_baselineTCP[i] = classif_correct[idx_domain].float().mean()\n",
    "    \n",
    "# cut max proba computed in U\n",
    "domain_cutoff = np.linspace(0.5, 1, 1000)\n",
    "coverage = np.zeros_like(domain_cutoff)\n",
    "risk = np.zeros_like(domain_cutoff)\n",
    "acc = np.zeros_like(domain_cutoff)\n",
    "for i, cut in enumerate(domain_cutoff):\n",
    "    idx_domain = selection_function(u_domain) > cut\n",
    "    coverage[i] = idx_domain.float().mean()\n",
    "    risk[i] = classif_loss[idx_domain].mean()\n",
    "    acc[i] = classif_correct[idx_domain].float().mean()\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "ax1.set_title('coverage vs. risk\\n(obtained by varying confidence/uncertainty threshold)')\n",
    "ax1.plot(coverage, risk, label='cut in U')\n",
    "ax1.plot(coverage_baseline, risk_baseline, label='baseline (max softmax)')\n",
    "ax1.plot(coverage_random, risk_random, label='baseline (random)')\n",
    "ax1.plot(coverage_baselineTCP, risk_baselineTCP, label='baseline (TCP oracle)')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('coverage')\n",
    "ax1.set_ylabel('risk')\n",
    "\n",
    "ax2.set_title('coverage vs. accuracy\\n(obtained by varying confidence/uncertainty threshold)\\n(using pseudo-labels)')\n",
    "ax2.plot(coverage, acc, label='cut in U')\n",
    "ax2.plot(coverage_baseline, acc_baseline, label='baseline (max softmax)')\n",
    "ax2.plot(coverage_random, acc_random, label='baseline (random)')\n",
    "ax2.plot(coverage_baselineTCP, acc_baselineTCP, label='baseline (TCP oracle)')\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('coverage')\n",
    "ax2.set_ylabel('accuracy')\n",
    "\n",
    "ax3.set_title('coverage vs threshold value')\n",
    "ax3.plot((domain_cutoff-domain_cutoff.min())/(domain_cutoff.max()-domain_cutoff.min()), coverage, label='cut in U')\n",
    "ax3.plot((domain_cutoff_baseline-domain_cutoff_baseline.min())/(domain_cutoff_baseline.max()-domain_cutoff_baseline.min()), coverage_baseline, label='baseline (max softmax)')\n",
    "ax3.plot((domain_cutoff_random-domain_cutoff_random.min())/(domain_cutoff_random.max()-domain_cutoff_random.min()), coverage_random, label='baseline (random)')\n",
    "ax3.plot((domain_cutoff_baselineTCP-domain_cutoff_baselineTCP.min())/(domain_cutoff_baselineTCP.max()-domain_cutoff_baselineTCP.min()), coverage_baselineTCP, label='baseline (TCP oracle)')\n",
    "ax3.legend()\n",
    "ax3.set_xlabel('normalized threshold value')\n",
    "ax3.set_ylabel('coverage')\n",
    "\n",
    "ax4.set_title('risk vs threshold value')\n",
    "ax4.plot((domain_cutoff-domain_cutoff.min())/(domain_cutoff.max()-domain_cutoff.min()), risk, label='cut in U')\n",
    "ax4.plot((domain_cutoff_baseline-domain_cutoff_baseline.min())/(domain_cutoff_baseline.max()-domain_cutoff_baseline.min()), risk_baseline, label='baseline (max softmax)')\n",
    "ax4.plot((domain_cutoff_random-domain_cutoff_random.min())/(domain_cutoff_random.max()-domain_cutoff_random.min()), risk_random, label='baseline (random)')\n",
    "ax4.plot((domain_cutoff_baselineTCP-domain_cutoff_baselineTCP.min())/(domain_cutoff_baselineTCP.max()-domain_cutoff_baselineTCP.min()), risk_baselineTCP, label='baseline (TCP oracle)')\n",
    "ax4.legend()\n",
    "ax4.set_xlabel('normalized threshold value')\n",
    "ax4.set_ylabel('risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    u = model.encoder(x_test.to(model.device))\n",
    "    u_domain = u[:, -1]\n",
    "    w = model.mapping(u)\n",
    "    x_recon = model.gan.generator.synthesis(w)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.hist(u_domain.cpu().numpy(), bins=50)\n",
    "ax1.set_title('histogram in U_domain')\n",
    "\n",
    "ax2.scatter(x_test[:, 0], x_test[:, 1], alpha=0.1, label='real data')\n",
    "ax2.scatter(x_recon[:, 0], x_recon[:, 1], alpha=0.1, label='reconstructed data')\n",
    "ax2.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out-of-domain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "cut = 0.8\n",
    "idx_in_domain = selection_function(u_domain) > cut\n",
    "idx_out_domain = idx_in_domain.logical_not()\n",
    "coverage = idx_in_domain.float().mean()\n",
    "risk = classif_loss[idx_in_domain].mean()\n",
    "acc = classif_correct[idx_in_domain].float().mean()\n",
    "ax1.set_title(f'threshold = {cut}\\ncoverage={coverage:.2f}, risk={risk:.2f}, acc={acc:.2f}')\n",
    "ax1.scatter(x_test[idx_out_domain, 0], x_test[idx_out_domain, 1], alpha=0.1, label='out domain', c='r')\n",
    "ax1.scatter(x_test[idx_in_domain, 0], x_test[idx_in_domain, 1], alpha=0.1, label='in domain', c='g')\n",
    "ax1.legend()\n",
    "\n",
    "cut = 0.9\n",
    "idx_in_domain = selection_function(u_domain) > cut\n",
    "idx_out_domain = idx_in_domain.logical_not()\n",
    "coverage = idx_in_domain.float().mean()\n",
    "risk = classif_loss[idx_in_domain].mean()\n",
    "acc = classif_correct[idx_in_domain].float().mean()\n",
    "ax2.set_title(f'threshold = {cut}\\ncoverage={coverage:.2f}, risk={risk:.2f}, acc={acc:.2f}')\n",
    "ax2.scatter(x_test[idx_out_domain, 0], x_test[idx_out_domain, 1], alpha=0.1, label='out domain', c='r')\n",
    "ax2.scatter(x_test[idx_in_domain, 0], x_test[idx_in_domain, 1], alpha=0.1, label='in domain', c='g')\n",
    "ax2.legend()\n",
    "\n",
    "cut = 0.95\n",
    "idx_in_domain = selection_function(u_domain) > cut\n",
    "idx_out_domain = idx_in_domain.logical_not()\n",
    "coverage = idx_in_domain.float().mean()\n",
    "risk = classif_loss[idx_in_domain].mean()\n",
    "acc = classif_correct[idx_in_domain].float().mean()\n",
    "ax3.set_title(f'threshold = {cut}\\ncoverage={coverage:.2f}, risk={risk:.2f}, acc={acc:.2f}')\n",
    "ax3.scatter(x_test[idx_out_domain, 0], x_test[idx_out_domain, 1], alpha=0.1, label='out domain', c='r')\n",
    "ax3.scatter(x_test[idx_in_domain, 0], x_test[idx_in_domain, 1], alpha=0.1, label='in domain', c='g')\n",
    "ax3.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-3, 3, 0.1)\n",
    "p_y0 = 0.5\n",
    "p_y1 = 0.5\n",
    "p_x_y0 = Normal(model.mapping.mean0, model.mapping.std0).log_prob(x).exp().detach()\n",
    "p_x_y1 = Normal(model.mapping.mean1, model.mapping.std1).log_prob(x).exp().detach()\n",
    "p_y0_x = p_x_y0 * p_y0 / (p_x_y0 * p_y0 + p_x_y1 * p_y1)\n",
    "p_y1_x = p_x_y1 * p_y1 / (p_x_y0 * p_y0 + p_x_y1 * p_y1)\n",
    "max_p_y_x = torch.maximum(p_y0_x, p_y1_x)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('proba density function')\n",
    "plt.plot(x.numpy(), p_x_y0.numpy(), label='class 0')\n",
    "plt.plot(x.numpy(), p_x_y1.numpy(), label='class 1')\n",
    "plt.plot(x.numpy(), (p_x_y0+p_x_y1).numpy(), label='sum')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x.numpy(), p_y0_x.numpy(), label='p(y=0|x)')\n",
    "plt.plot(x.numpy(), p_y1_x.numpy(), label='p(y=1|x)')\n",
    "plt.plot(x.numpy(), max_p_y_x.numpy(), label='max p(y|x)', alpha=0.5)\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d98911e8e1829f7f6c5e31f61dda5e143049f52824dab13c60371001bf774251"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
